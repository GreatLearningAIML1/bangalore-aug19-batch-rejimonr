{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical NLP Project Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Description    Classification is probably the most popular task that you would deal with in real life. Text in the form of blogs, posts, articles, etc. is written every second. It is a challenge to predict the information about the writer without knowing about him/her.  \n",
    " \n",
    "We are going to create a classifier that predicts multiple features of the author of a given text. We have designed it as a Multilabel classification problem.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import basic libraries needed for data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data into pandas dataframe\n",
    "\n",
    "#Lets load only a sample no of rows now considering the space on the machine\n",
    "\n",
    "blog_df = pd.read_csv(\"blog-authorship-corpus.zip\",nrows=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backup copy in case needed without having to read the data from disk again\n",
    "bkup = blog_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use when needed and comment back\n",
    "#blog_df = bkup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16907</th>\n",
       "      <td>3618432</td>\n",
       "      <td>female</td>\n",
       "      <td>25</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>18,June,2004</td>\n",
       "      <td>I dreamt last night that I drove up to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14899</th>\n",
       "      <td>727002</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Leo</td>\n",
       "      <td>10,October,2003</td>\n",
       "      <td>on an unrelated note, i feel ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>3359005</td>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>Student</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>05,July,2004</td>\n",
       "      <td>urlLink    Dennis Frentsos and E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25359</th>\n",
       "      <td>3635310</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>05,May,2003</td>\n",
       "      <td>Hello Everyone - I went to the podiatr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30169</th>\n",
       "      <td>3951537</td>\n",
       "      <td>female</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>16,July,2004</td>\n",
       "      <td>i have much to speak of on this night. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  gender  age          topic     sign             date  \\\n",
       "16907  3618432  female   25         indUnk   Cancer     18,June,2004   \n",
       "14899   727002    male   23       Internet      Leo  10,October,2003   \n",
       "954    3359005    male   14        Student  Scorpio     05,July,2004   \n",
       "25359  3635310  female   27  Manufacturing   Gemini      05,May,2003   \n",
       "30169  3951537  female   16        Student    Virgo     16,July,2004   \n",
       "\n",
       "                                                    text  \n",
       "16907         I dreamt last night that I drove up to ...  \n",
       "14899               on an unrelated note, i feel ever...  \n",
       "954                  urlLink    Dennis Frentsos and E...  \n",
       "25359          Hello Everyone - I went to the podiatr...  \n",
       "30169         i have much to speak of on this night. ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity\n",
    "blog_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert date to proper date string for manipulation if needed later\n",
    "blog_df.date = pd.to_datetime(blog_df.date,errors=\"coerce\",infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of the data set\n",
    "blog_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 7 columns):\n",
      "id        50000 non-null int64\n",
      "gender    50000 non-null object\n",
      "age       50000 non-null int64\n",
      "topic     50000 non-null object\n",
      "sign      50000 non-null object\n",
      "date      49535 non-null datetime64[ns]\n",
      "text      50000 non-null object\n",
      "dtypes: datetime64[ns](1), int64(2), object(4)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#Check the basic info of the dataset now\n",
    "blog_df.info()\n",
    "\n",
    "#Age does not have any null.\n",
    "#Need to check the values of all the others as part of exploration down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male      25815\n",
       "female    24185\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the distribution of the gender of this sample\n",
    "blog_df.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e61d9fc2b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the gender distribution\n",
    "sns.countplot(blog_df.gender)\n",
    "\n",
    "#almost equal representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17    6859\n",
       "24    5746\n",
       "23    5518\n",
       "16    4156\n",
       "27    4094\n",
       "15    3508\n",
       "35    3365\n",
       "26    2869\n",
       "25    2837\n",
       "14    2043\n",
       "36    1985\n",
       "34    1886\n",
       "33    1654\n",
       "13     745\n",
       "39     412\n",
       "41     394\n",
       "46     330\n",
       "48     318\n",
       "37     310\n",
       "47     206\n",
       "38     196\n",
       "40     192\n",
       "43     150\n",
       "42      96\n",
       "45      93\n",
       "44      38\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check the age distribution\n",
    "blog_df.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e61e3060f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets visualize the distribution\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.countplot(blog_df.age)\n",
    "\n",
    "#We can see that the distribution in this sample is heavily skewed towards younger population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indUnk                     17560\n",
       "Student                    10660\n",
       "Technology                  4379\n",
       "Education                   2646\n",
       "Arts                        1817\n",
       "Fashion                     1805\n",
       "Communications-Media        1603\n",
       "Internet                    1420\n",
       "Engineering                 1402\n",
       "Science                      705\n",
       "Government                   599\n",
       "Non-Profit                   491\n",
       "Manufacturing                441\n",
       "BusinessServices             416\n",
       "Marketing                    414\n",
       "Accounting                   364\n",
       "Law                          308\n",
       "Museums-Libraries            285\n",
       "Banking                      283\n",
       "Advertising                  273\n",
       "Religion                     258\n",
       "Consulting                   243\n",
       "Publishing                   207\n",
       "Transportation               196\n",
       "Military                     194\n",
       "LawEnforcement-Security      125\n",
       "Sports-Recreation            120\n",
       "Automotive                   116\n",
       "Biotech                      101\n",
       "InvestmentBanking             85\n",
       "HumanResources                79\n",
       "Agriculture                   78\n",
       "Chemicals                     75\n",
       "Architecture                  70\n",
       "Tourism                       65\n",
       "Maritime                      54\n",
       "Construction                  28\n",
       "RealEstate                    17\n",
       "Telecommunications            12\n",
       "Environment                    6\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check the topic distribution\n",
    "blog_df.topic.value_counts()\n",
    "\n",
    "#We can see that indUnk (which is basicaly unknown) dominates in this particular sample\n",
    "#..not sure on the overall dataset\n",
    "#In this sample Student topic is also seen more. Age distribution correlates with this info\n",
    "#We can also see that some of the topics are underrepresented like Environment and Technology. \n",
    "#This will be a challenge for final classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42263</th>\n",
       "      <td>888601</td>\n",
       "      <td>female</td>\n",
       "      <td>24</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Libra</td>\n",
       "      <td>2004-05-23</td>\n",
       "      <td>I'm sweating.  I hate to sweat, especia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13525</th>\n",
       "      <td>1976124</td>\n",
       "      <td>male</td>\n",
       "      <td>25</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Libra</td>\n",
       "      <td>2003-11-26</td>\n",
       "      <td>I'm back again.  An odd thing hap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13885</th>\n",
       "      <td>480727</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>2004-06-02</td>\n",
       "      <td>arghhh,..had a bump on my right e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32189</th>\n",
       "      <td>1538911</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Libra</td>\n",
       "      <td>2004-07-14</td>\n",
       "      <td>Had to call the cops again tonight.  Ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44754</th>\n",
       "      <td>1098541</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>2004-03-05</td>\n",
       "      <td>WE INTERUPT THIS REGULAR BLOG ENTRY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  gender  age   topic       sign       date  \\\n",
       "42263   888601  female   24  indUnk      Libra 2004-05-23   \n",
       "13525  1976124    male   25  indUnk      Libra 2003-11-26   \n",
       "13885   480727    male   23  indUnk     Pisces 2004-06-02   \n",
       "32189  1538911  female   35  indUnk      Libra 2004-07-14   \n",
       "44754  1098541    male   15  indUnk  Capricorn 2004-03-05   \n",
       "\n",
       "                                                    text  \n",
       "42263         I'm sweating.  I hate to sweat, especia...  \n",
       "13525               I'm back again.  An odd thing hap...  \n",
       "13885               arghhh,..had a bump on my right e...  \n",
       "32189         Had to call the cops again tonight.  Ou...  \n",
       "44754             WE INTERUPT THIS REGULAR BLOG ENTRY...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check some samples where the topic is shown as unknown which is the majority\n",
    "blog_df[blog_df.topic == \"indUnk\"].sample(5)\n",
    "\n",
    "#Key decision is if we want to keep these records.\n",
    "#Lets keep them now and see if we can predict the other characteristics of the person like sign, age and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Aries          7795\n",
       "Aquarius       4784\n",
       "Cancer         4589\n",
       "Sagittarius    4571\n",
       "Libra          4378\n",
       "Pisces         4142\n",
       "Leo            3904\n",
       "Capricorn      3819\n",
       "Taurus         3390\n",
       "Scorpio        3243\n",
       "Virgo          2827\n",
       "Gemini         2558\n",
       "Name: sign, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check the sign\n",
    "blog_df.sign.value_counts()\n",
    "\n",
    "#Seems to be dominated by Aries in this sample heavily..this could cause some prediction challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "1999.0        5\n",
       "2000.0       48\n",
       "2001.0      215\n",
       "2002.0     2083\n",
       "2003.0     7912\n",
       "2004.0    39272\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the year of blogs..most of the info in this sample is from 2004\n",
    "blog_df.groupby(blog_df.date.dt.year).id.aggregate(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8877          4\n",
       "48254         4\n",
       "19259         4\n",
       "45986         7\n",
       "45952         7\n",
       "35369         8\n",
       "35367         8\n",
       "35365         8\n",
       "35366         8\n",
       "45958         8\n",
       "46006         8\n",
       "31554         9\n",
       "17463         9\n",
       "31591         9\n",
       "31624        10\n",
       "25263        10\n",
       "38395        11\n",
       "38457        11\n",
       "38483        11\n",
       "18208        11\n",
       "38412        11\n",
       "38469        11\n",
       "45189        11\n",
       "38447        11\n",
       "18189        11\n",
       "5239         12\n",
       "5030         12\n",
       "5073         12\n",
       "4940         12\n",
       "27794        12\n",
       "          ...  \n",
       "9805      22495\n",
       "28297     22624\n",
       "27773     23094\n",
       "30638     23887\n",
       "21914     24051\n",
       "48191     24887\n",
       "17907     25315\n",
       "17908     25315\n",
       "2         25467\n",
       "30508     25505\n",
       "32127     26012\n",
       "6107      26328\n",
       "28264     26597\n",
       "12500     26837\n",
       "27301     27654\n",
       "26623     27667\n",
       "13310     28055\n",
       "3629      28995\n",
       "12410     30246\n",
       "1848      30737\n",
       "37144     31617\n",
       "9983      44747\n",
       "6106      48828\n",
       "11684     48977\n",
       "42927     49057\n",
       "18513     61013\n",
       "18512     61922\n",
       "16760     68545\n",
       "16759    119813\n",
       "31751    321278\n",
       "Name: text, Length: 50000, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check length of the text values which is the key feature for this problem\n",
    "blog_df.text.str.len().sort_values()\n",
    "\n",
    "#Length of the blog posts is very varied with very low # of words to very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     50000.000000\n",
       "mean       1130.585300\n",
       "std        2216.412948\n",
       "min           4.000000\n",
       "25%         237.000000\n",
       "50%         662.000000\n",
       "75%        1460.000000\n",
       "max      321278.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.text.str.len().describe()\n",
    "\n",
    "#We can see there are some very huge ones and some very low ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1969"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see how many of them have less than 50 chars\n",
    "blog_df[blog_df.text.str.len() <= 50].id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5048</th>\n",
       "      <td>1103575</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>2003-09-24</td>\n",
       "      <td>I blogged out on xanga... silly Rachel....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>2004-08-05</td>\n",
       "      <td>it is time for me to move on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47683</th>\n",
       "      <td>988941</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>2004-02-21</td>\n",
       "      <td>Oh and Matt: BLOG.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>2004-08-05</td>\n",
       "      <td>i love madonna!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47852</th>\n",
       "      <td>988941</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>2004-04-03</td>\n",
       "      <td>I tidied my room</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  gender  age       topic       sign       date  \\\n",
       "5048   1103575  female   17      indUnk    Scorpio 2003-09-24   \n",
       "3501    589736    male   35  Technology      Aries 2004-08-05   \n",
       "47683   988941  female   17     Student  Capricorn 2004-02-21   \n",
       "2237    589736    male   35  Technology      Aries 2004-08-05   \n",
       "47852   988941  female   17     Student  Capricorn 2004-04-03   \n",
       "\n",
       "                                                    text  \n",
       "5048          I blogged out on xanga... silly Rachel....  \n",
       "3501               it is time for me to move on           \n",
       "47683                        Oh and Matt: BLOG.           \n",
       "2237                            i love madonna!           \n",
       "47852                          I tidied my room           "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample records with less than 50 chars\n",
    "blog_df[blog_df.text.str.len() <= 50].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5697"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see how many of them have less than 100 chars\n",
    "blog_df[blog_df.text.str.len() <= 100].id.count()\n",
    "\n",
    "#That is almost 10% of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48560</th>\n",
       "      <td>3415693</td>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Leo</td>\n",
       "      <td>2004-08-06</td>\n",
       "      <td>HEAVENS TO BETSEY, iTUNES WO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14909</th>\n",
       "      <td>727002</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Leo</td>\n",
       "      <td>2003-11-15</td>\n",
       "      <td>urlLink CONSPIRACY!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28510</th>\n",
       "      <td>1103016</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>2004-01-30</td>\n",
       "      <td>I turned into a Blue Bruce.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14904</th>\n",
       "      <td>727002</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Leo</td>\n",
       "      <td>2003-11-23</td>\n",
       "      <td>My life is rated R.  What is y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42496</th>\n",
       "      <td>3968157</td>\n",
       "      <td>male</td>\n",
       "      <td>26</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Leo</td>\n",
       "      <td>2004-07-29</td>\n",
       "      <td>urlLink    raza  raza</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id gender  age     topic    sign       date  \\\n",
       "48560  3415693   male   14    indUnk     Leo 2004-08-06   \n",
       "14909   727002   male   23  Internet     Leo 2003-11-15   \n",
       "28510  1103016   male   16   Student  Gemini 2004-01-30   \n",
       "14904   727002   male   23  Internet     Leo 2003-11-23   \n",
       "42496  3968157   male   26    indUnk     Leo 2004-07-29   \n",
       "\n",
       "                                                    text  \n",
       "48560                    HEAVENS TO BETSEY, iTUNES WO...  \n",
       "14909                      urlLink CONSPIRACY!            \n",
       "28510               I turned into a Blue Bruce.           \n",
       "14904                  My life is rated R.  What is y...  \n",
       "42496                   urlLink    raza  raza             "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample records with less text\n",
    "blog_df[blog_df.text.str.len() <= 100].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets clean up the text and then check the length. We will drop rows which have zero or very less words to be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess rows of the text column\n",
    "\n",
    "a. Remove unwanted characters \n",
    "b. Convert text to lowercase \n",
    "c. Remove unwanted spaces \n",
    "d. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"             Age: 32 Unit: Army School of Ammunition, Royal Logistic Corps Hometown: Romsey, England Details: Killed during an explosive ordnance disposal operation in southern Iraq on March 31, 2003  Articles:  urlLink #1 ,  urlLink #2 ,  urlLink #3 ,  urlLink #4 ,  urlLink #5   There may be more, but that's the first page of the google search.          \",\n",
       " \"       He drives me crazy sometimes.&nbsp; I wake up and his hands are on me and his back is moving above me and he smells like wet dirt and musk.&nbsp; I can't help but fall into him because he is still the sexiest man I've ever seen.&nbsp; I'm still far too in love with him.&nbsp; I'm still always waiting for him.&nbsp; I still need the sound of his voice and I need to hear that I'm beautiful and I need to hear him laugh.&nbsp; So when he doesn't I immediately think it is me, it is us.&nbsp; And that is my own ridiculous insecurities.&nbsp;      \",\n",
       " \"        Timing is everything  I have my share of good days and bad days.  But for 2 days in a row, I've managed to be in the right place at the right time.    Right around 11am, they pull the breakfast buffet from the restaurant. Warm chafing dishes half-filled with crispy, savory hour-old bacon and grease-filled crispy-on-the-outside, juicy-on-the-inside fat little sausages.  Left overs from what has been picked over since 6:30 this morning by hotel guests with more time and money on their hands than they know what to do with.    My cholesterol has probably spiked high from my recent good fortune; but, man! how a little bacon and sausage can make the day go oh-so good.  Diva out.           \",\n",
       " \"                  Afraid Of Emotions     Why are so many people afraid to be nice, or show that they care at all? Not caring... My friend once told me the way to maintain your relationships with people was not to care, so then are they really relationships at all then? 'Cause if you don't care about what happens to those people or those relationships then its not really a relationship to begin with. I will admit that sometimes I care to much what is going on in my friendships and I let myself get carried away in it but I think that it can be better than not trying/caring at all. Think about it...               \",\n",
       " \"       I kind of rearranged my room.  Actually all I did was move my bed and bring my laptop upstairs off of the dining room table.  I need to find a way to move the air conditioners without giving myself a hernia (can women get hernias?).     I was bored so I've added an 'Ask A Question' feature to both my blog and my webpage.  Feel free to ask whatever you want; I don't embarass, or offend, easily.  I'll answer the question, or questions, as soon as I can.   OK, I'm off to throw my back out completely by trying to move two air conditioners by myself.  Won't this be fun?         \"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check some sample text values\n",
    "list(blog_df.text.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##We can see the references to urlLink which should be removed\n",
    "##We can see additional spaces, names, mixed case, special characters etc\n",
    "##We can also see lot of spelling issues which could be a problem - for future how to correct this\n",
    "##Do we need numbers? since we are only looking for classification..\n",
    "#Should we attempt to do NER and remove/keep them? - For future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get nltk for stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the list of stopwords in English from NLTK corpus\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "#Add custom stopwords to the list\n",
    "mystop_words = [\"urllink\"]\n",
    "\n",
    "stoplist.extend(mystop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility method to tokenize a sentence and remove stop words\n",
    "def remove_stop_words(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    clean_tokens = [tokens for tokens in tokens if tokens not in stoplist]\n",
    "    clean_sentenance = \" \".join(clean_tokens)\n",
    "    return clean_sentenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"       September 24, 2002   Bummed around until 10:30 this morning.  Then I had to get up and take the biddy to the cardiologist for a day full of tests.  I missed my spine scan and endocrinologist appointments; I'm never going to feel better if I don't stop pissing around!    Per usual, I didn't get anything in the mail today.  I did sell a pair of DM Chukka boots, though; that rocks.  I had an inquiry about the Marc Jacobs jeans I have on eBay; hopefully I'll sell another pair of those too.  I don't think $40 is too bad for a brand new pair of $140 jeans.   The landlords are still driving me crazy with their damn kids and the husband's temper tantrums.  It'd be nice if I could get a full night sleep.  Alas that's not going to happen until we move or a mysterious accident takes out the bottom half of the house.  I have a special place in hell reserved just for me...   Yet again, people piss me off.  Nothing new there...   I did receive a really cool DM poster yesterday; 3 weeks after I paid for it.  Luckily it's cool and I only paid $13 for it.  The 18 eye snakeskin Docs auction I was planning on winning was ended early.  Why list something if you're not sure you want to sell it?   I did win a cool pair of rose pattern boots for $25, so that kinda makes up for it.  The seller also agreed to ship via my UPS account so that saves me about $6 in shipping!    Playlist Metallica 'Outlaw Torn' Europe 'The Final Countdown' Dream Theater 'Space Dye Vest' Depeche Mode 'Stripped'    Off to clean Fluffy's room, and I guess I should go to the laundromat as well.   urlLink            \""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "s = list(blog_df.sample().text)[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"September 24 , 2002 Bummed around 10:30 morning . Then I get take biddy cardiologist day full tests . I missed spine scan endocrinologist appointments ; I 'm never going feel better I n't stop pissing around ! Per usual , I n't get anything mail today . I sell pair DM Chukka boots , though ; rocks . I inquiry Marc Jacobs jeans I eBay ; hopefully I 'll sell another pair . I n't think $ 40 bad brand new pair $ 140 jeans . The landlords still driving crazy damn kids husband 's temper tantrums . It 'd nice I could get full night sleep . Alas 's going happen move mysterious accident takes bottom half house . I special place hell reserved ... Yet , people piss . Nothing new ... I receive really cool DM poster yesterday ; 3 weeks I paid . Luckily 's cool I paid $ 13 . The 18 eye snakeskin Docs auction I planning winning ended early . Why list something 're sure want sell ? I win cool pair rose pattern boots $ 25 , kinda makes . The seller also agreed ship via UPS account saves $ 6 shipping ! Playlist Metallica 'Outlaw Torn ' Europe 'The Final Countdown ' Dream Theater 'Space Dye Vest ' Depeche Mode 'Stripped ' Off clean Fluffy 's room , I guess I go laundromat well . urlLink\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity\n",
    "remove_stop_words(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to convert items to lower case\n",
    "def lowercase(sentence):\n",
    "    return sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"       september 24, 2002   bummed around until 10:30 this morning.  then i had to get up and take the biddy to the cardiologist for a day full of tests.  i missed my spine scan and endocrinologist appointments; i'm never going to feel better if i don't stop pissing around!    per usual, i didn't get anything in the mail today.  i did sell a pair of dm chukka boots, though; that rocks.  i had an inquiry about the marc jacobs jeans i have on ebay; hopefully i'll sell another pair of those too.  i don't think $40 is too bad for a brand new pair of $140 jeans.   the landlords are still driving me crazy with their damn kids and the husband's temper tantrums.  it'd be nice if i could get a full night sleep.  alas that's not going to happen until we move or a mysterious accident takes out the bottom half of the house.  i have a special place in hell reserved just for me...   yet again, people piss me off.  nothing new there...   i did receive a really cool dm poster yesterday; 3 weeks after i paid for it.  luckily it's cool and i only paid $13 for it.  the 18 eye snakeskin docs auction i was planning on winning was ended early.  why list something if you're not sure you want to sell it?   i did win a cool pair of rose pattern boots for $25, so that kinda makes up for it.  the seller also agreed to ship via my ups account so that saves me about $6 in shipping!    playlist metallica 'outlaw torn' europe 'the final countdown' dream theater 'space dye vest' depeche mode 'stripped'    off to clean fluffy's room, and i guess i should go to the laundromat as well.   urllink            \""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "lowercase(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re for patten matching\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function to remove special characters\n",
    "#Adding a toggle for keeping or removing numbers\n",
    "#Should we replace special chars with space or blanks? Both create problem..lets keep blank for now since probably\n",
    "#more better\n",
    "def remove_special_chars(sentence,nos=False):\n",
    "    if nos:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z\\s]'\n",
    "    clean_sentence = re.sub(pattern,'',sentence)\n",
    "    return clean_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"       September 24, 2002   Bummed around until 10:30 this morning.  Then I had to get up and take the biddy to the cardiologist for a day full of tests.  I missed my spine scan and endocrinologist appointments; I'm never going to feel better if I don't stop pissing around!    Per usual, I didn't get anything in the mail today.  I did sell a pair of DM Chukka boots, though; that rocks.  I had an inquiry about the Marc Jacobs jeans I have on eBay; hopefully I'll sell another pair of those too.  I don't think $40 is too bad for a brand new pair of $140 jeans.   The landlords are still driving me crazy with their damn kids and the husband's temper tantrums.  It'd be nice if I could get a full night sleep.  Alas that's not going to happen until we move or a mysterious accident takes out the bottom half of the house.  I have a special place in hell reserved just for me...   Yet again, people piss me off.  Nothing new there...   I did receive a really cool DM poster yesterday; 3 weeks after I paid for it.  Luckily it's cool and I only paid $13 for it.  The 18 eye snakeskin Docs auction I was planning on winning was ended early.  Why list something if you're not sure you want to sell it?   I did win a cool pair of rose pattern boots for $25, so that kinda makes up for it.  The seller also agreed to ship via my UPS account so that saves me about $6 in shipping!    Playlist Metallica 'Outlaw Torn' Europe 'The Final Countdown' Dream Theater 'Space Dye Vest' Depeche Mode 'Stripped'    Off to clean Fluffy's room, and I guess I should go to the laundromat as well.   urlLink            \""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'       September     Bummed around until  this morning  Then I had to get up and take the biddy to the cardiologist for a day full of tests  I missed my spine scan and endocrinologist appointments Im never going to feel better if I dont stop pissing around    Per usual I didnt get anything in the mail today  I did sell a pair of DM Chukka boots though that rocks  I had an inquiry about the Marc Jacobs jeans I have on eBay hopefully Ill sell another pair of those too  I dont think  is too bad for a brand new pair of  jeans   The landlords are still driving me crazy with their damn kids and the husbands temper tantrums  Itd be nice if I could get a full night sleep  Alas thats not going to happen until we move or a mysterious accident takes out the bottom half of the house  I have a special place in hell reserved just for me   Yet again people piss me off  Nothing new there   I did receive a really cool DM poster yesterday  weeks after I paid for it  Luckily its cool and I only paid  for it  The  eye snakeskin Docs auction I was planning on winning was ended early  Why list something if youre not sure you want to sell it   I did win a cool pair of rose pattern boots for  so that kinda makes up for it  The seller also agreed to ship via my UPS account so that saves me about  in shipping    Playlist Metallica Outlaw Torn Europe The Final Countdown Dream Theater Space Dye Vest Depeche Mode Stripped    Off to clean Fluffys room and I guess I should go to the laundromat as well   urlLink            '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "remove_special_chars(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function to remove additional space\n",
    "def remove_extra_spaces(sentence):\n",
    "    pattern = r'\\s+'\n",
    "    clean_sentence = re.sub(pattern,' ',sentence)\n",
    "    return clean_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" September 24, 2002 Bummed around until 10:30 this morning. Then I had to get up and take the biddy to the cardiologist for a day full of tests. I missed my spine scan and endocrinologist appointments; I'm never going to feel better if I don't stop pissing around! Per usual, I didn't get anything in the mail today. I did sell a pair of DM Chukka boots, though; that rocks. I had an inquiry about the Marc Jacobs jeans I have on eBay; hopefully I'll sell another pair of those too. I don't think $40 is too bad for a brand new pair of $140 jeans. The landlords are still driving me crazy with their damn kids and the husband's temper tantrums. It'd be nice if I could get a full night sleep. Alas that's not going to happen until we move or a mysterious accident takes out the bottom half of the house. I have a special place in hell reserved just for me... Yet again, people piss me off. Nothing new there... I did receive a really cool DM poster yesterday; 3 weeks after I paid for it. Luckily it's cool and I only paid $13 for it. The 18 eye snakeskin Docs auction I was planning on winning was ended early. Why list something if you're not sure you want to sell it? I did win a cool pair of rose pattern boots for $25, so that kinda makes up for it. The seller also agreed to ship via my UPS account so that saves me about $6 in shipping! Playlist Metallica 'Outlaw Torn' Europe 'The Final Countdown' Dream Theater 'Space Dye Vest' Depeche Mode 'Stripped' Off to clean Fluffy's room, and I guess I should go to the laundromat as well. urlLink \""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "remove_extra_spaces(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rejimonr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Lets try out Lemmatization as well\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to lemmatize sentences\n",
    "def word_lemmatize(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    clean_sentence =  \" \".join([wnl.lemmatize(token) for token in tokens])\n",
    "    return clean_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"       September 24, 2002   Bummed around until 10:30 this morning.  Then I had to get up and take the biddy to the cardiologist for a day full of tests.  I missed my spine scan and endocrinologist appointments; I'm never going to feel better if I don't stop pissing around!    Per usual, I didn't get anything in the mail today.  I did sell a pair of DM Chukka boots, though; that rocks.  I had an inquiry about the Marc Jacobs jeans I have on eBay; hopefully I'll sell another pair of those too.  I don't think $40 is too bad for a brand new pair of $140 jeans.   The landlords are still driving me crazy with their damn kids and the husband's temper tantrums.  It'd be nice if I could get a full night sleep.  Alas that's not going to happen until we move or a mysterious accident takes out the bottom half of the house.  I have a special place in hell reserved just for me...   Yet again, people piss me off.  Nothing new there...   I did receive a really cool DM poster yesterday; 3 weeks after I paid for it.  Luckily it's cool and I only paid $13 for it.  The 18 eye snakeskin Docs auction I was planning on winning was ended early.  Why list something if you're not sure you want to sell it?   I did win a cool pair of rose pattern boots for $25, so that kinda makes up for it.  The seller also agreed to ship via my UPS account so that saves me about $6 in shipping!    Playlist Metallica 'Outlaw Torn' Europe 'The Final Countdown' Dream Theater 'Space Dye Vest' Depeche Mode 'Stripped'    Off to clean Fluffy's room, and I guess I should go to the laundromat as well.   urlLink            \""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"September 24 , 2002 Bummed around until 10:30 this morning . Then I had to get up and take the biddy to the cardiologist for a day full of test . I missed my spine scan and endocrinologist appointment ; I 'm never going to feel better if I do n't stop pissing around ! Per usual , I did n't get anything in the mail today . I did sell a pair of DM Chukka boot , though ; that rock . I had an inquiry about the Marc Jacobs jean I have on eBay ; hopefully I 'll sell another pair of those too . I do n't think $ 40 is too bad for a brand new pair of $ 140 jean . The landlord are still driving me crazy with their damn kid and the husband 's temper tantrum . It 'd be nice if I could get a full night sleep . Alas that 's not going to happen until we move or a mysterious accident take out the bottom half of the house . I have a special place in hell reserved just for me ... Yet again , people piss me off . Nothing new there ... I did receive a really cool DM poster yesterday ; 3 week after I paid for it . Luckily it 's cool and I only paid $ 13 for it . The 18 eye snakeskin Docs auction I wa planning on winning wa ended early . Why list something if you 're not sure you want to sell it ? I did win a cool pair of rose pattern boot for $ 25 , so that kinda make up for it . The seller also agreed to ship via my UPS account so that save me about $ 6 in shipping ! Playlist Metallica 'Outlaw Torn ' Europe 'The Final Countdown ' Dream Theater 'Space Dye Vest ' Depeche Mode 'Stripped ' Off to clean Fluffy 's room , and I guess I should go to the laundromat a well . urlLink\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lemmatize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to consolidate all clean up actions\n",
    "def text_clean(sentence,lower=False,stopwords_removal=False,extra_spaces=True,special_chars=False,lemmatize=False):\n",
    "    clean_sentence = sentence\n",
    "    if lower:\n",
    "        clean_sentence = lowercase(clean_sentence)\n",
    "    if special_chars:\n",
    "        clean_sentence = remove_special_chars(clean_sentence)\n",
    "    if lemmatize:\n",
    "        clean_sentence = word_lemmatize(clean_sentence)\n",
    "    if stopwords_removal:\n",
    "        clean_sentence = remove_stop_words(clean_sentence)\n",
    "    if extra_spaces:\n",
    "        clean_sentence = remove_extra_spaces(clean_sentence)\n",
    "        \n",
    "    return clean_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'       Before Jaelyn was born our lawyers advised us not to bring her home until after the forty-eight hours of maternal consent had passed.  Too many custody cases that they had seen, too many broken hearted adoptive parents, too much risk  they said  to take her home after the 12 hour wait that the hospital mandated.   In _The Great Divorce_, the narrator talks about all the reasons that humans choose to turn back to the small dark dingy city that they came from instead of growing into and up-in and through live in the dangerously beautiful new world.  One of the stories he tells is of a mother who is so wrapped up in the lives of her children that she cannot bear to let go of them so that she may enter the Great New World.  Parental love, like all the greatest parts of being human seems like one of the most dangerous and difficult endeavors.  How do we love without strings attached?  How can we possibly invest every hope and dream into these people and then love them just as fully when they need to choose dreams that we cant understand?  I see this question being struggled through by college students who are on the receiving end of constricting love.   So we decided to bring her home anyway.  Those 36 hours felt like one of the top 5 most courageous moments in my life.  After a two month wait for her, after a miscarriage and all the brokenness that that brings, after two years of infertility.    I read  urlLink this blog  that inspired me. The question:   What is the bravest thing that you have ever done?  Seems like one of those questions that push us to tell some of our best stories  look for what we admire mostin each other  and become something more courageous than we are  peace~      '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity check\n",
    "s = list(blog_df.sample().text)[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jaelyn wa born lawyer advised u bring home fortyeight hour maternal consent passed many custody case seen many broken hearted adoptive parent much risk said take home hour wait hospital mandated great divorce narrator talk reason human choose turn back small dark dingy city came instead growing upin live dangerously beautiful new world one story tell mother wrapped life child bear let go may enter great new world parental love like greatest part human seems like one dangerous difficult endeavor love without string attached possibly invest every hope dream people love fully need choose dream cant understand see question struggled college student receiving end constricting love decided bring home anyway hour felt like one top courageous moment life two month wait miscarriage brokenness brings two year infertility read blog inspired question bravest thing ever done seems like one question push u tell best story look admire mostin become something courageous peace'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean(s,lower=True,stopwords_removal=True,extra_spaces=True,special_chars=True,lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the clean up method on the data set\n",
    "\n",
    "blog_df.text = blog_df.text.apply(lambda x: text_clean(x,\n",
    "                                                       lower=True,\n",
    "                                                       stopwords_removal=True,\n",
    "                                                       extra_spaces=True,\n",
    "                                                       special_chars=True,\n",
    "                                                       lemmatize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     50000.000000\n",
       "mean        675.762600\n",
       "std        1325.822541\n",
       "min           0.000000\n",
       "25%         130.000000\n",
       "50%         388.000000\n",
       "75%         880.000000\n",
       "max      184415.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Lets check the text lengths now after the clean up activities\n",
    "blog_df.text.str.len().describe()\n",
    "\n",
    "#We can see overall text lengths have reduced from the cleaning\n",
    "#Interestingly some of the entries have became zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check the records where no text is available after cleaning\n",
    "blog_df[blog_df.text.str.len() == 0].id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36024</th>\n",
       "      <td>4060663</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>2004-08-08</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32726</th>\n",
       "      <td>1543875</td>\n",
       "      <td>female</td>\n",
       "      <td>25</td>\n",
       "      <td>Education</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>2004-07-26</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31624</th>\n",
       "      <td>2161822</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>2004-08-04</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21273</th>\n",
       "      <td>3922848</td>\n",
       "      <td>male</td>\n",
       "      <td>17</td>\n",
       "      <td>Arts</td>\n",
       "      <td>Leo</td>\n",
       "      <td>2004-07-14</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36220</th>\n",
       "      <td>3660006</td>\n",
       "      <td>female</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>2004-07-27</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  gender  age      topic         sign       date text\n",
       "36024  4060663  female   27     indUnk  Sagittarius 2004-08-08     \n",
       "32726  1543875  female   25  Education       Cancer 2004-07-26     \n",
       "31624  2161822    male   15    Student          Leo 2004-08-04     \n",
       "21273  3922848    male   17       Arts          Leo 2004-07-14     \n",
       "36220  3660006  female   16    Student    Capricorn 2004-07-27     "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the sample records\n",
    "blog_df[blog_df.text.str.len() == 0].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will drop these records for now\n",
    "blog_df = blog_df[blog_df.text.str.len() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity check\n",
    "blog_df[blog_df.text.str.len() == 0].id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2290"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the number of records which have very less text\n",
    "text_length = 20\n",
    "blog_df[blog_df.text.str.len() <= text_length].id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43147</th>\n",
       "      <td>3474281</td>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>Museums-Libraries</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>2004-06-08</td>\n",
       "      <td>come photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46968</th>\n",
       "      <td>988941</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>2003-05-08</td>\n",
       "      <td>yep especially</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14848</th>\n",
       "      <td>727002</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Leo</td>\n",
       "      <td>2003-06-09</td>\n",
       "      <td>oo oo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>2004-08-05</td>\n",
       "      <td>liked cookie story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46891</th>\n",
       "      <td>988941</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>2003-04-01</td>\n",
       "      <td>jo hereit boring bye</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  gender  age              topic       sign       date  \\\n",
       "43147  3474281    male   14  Museums-Libraries    Scorpio 2004-06-08   \n",
       "46968   988941  female   17            Student  Capricorn 2003-05-08   \n",
       "14848   727002    male   23           Internet        Leo 2003-06-09   \n",
       "3076    589736    male   35         Technology      Aries 2004-08-05   \n",
       "46891   988941  female   17            Student  Capricorn 2003-04-01   \n",
       "\n",
       "                       text  \n",
       "43147            come photo  \n",
       "46968        yep especially  \n",
       "14848                 oo oo  \n",
       "3076     liked cookie story  \n",
       "46891  jo hereit boring bye  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample records where text is very less\n",
    "blog_df[blog_df.text.str.len() <= text_length].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2290.000000\n",
       "mean       12.375983\n",
       "std         5.071756\n",
       "min         1.000000\n",
       "25%         9.000000\n",
       "50%        13.000000\n",
       "75%        17.000000\n",
       "max        20.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check distribution\n",
    "blog_df[blog_df.text.str.len() <= text_length].text.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try removing these records for now and check\n",
    "blog_df = blog_df[blog_df.text.str.len() > text_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity check\n",
    "blog_df[blog_df.text.str.len() <= text_length].id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     47306.000000\n",
       "mean        713.647085\n",
       "std        1353.244682\n",
       "min          21.000000\n",
       "25%         163.000000\n",
       "50%         427.000000\n",
       "75%         919.000000\n",
       "max      184415.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check distribution\n",
    "blog_df.text.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep dataset\n",
    "As we want to make this into a multi-label classification problem, you are required to merge all the label columns together, so that we have all the labels together for a particular sentence (7.5 points) a. Label columns to merge: gender, age, topic, sign b. After completing the previous step, there should be only two columns in your data frame i.e. text and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a copy to proceed further\n",
    "clean_blog_df = blog_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 47306 entries, 0 to 49999\n",
      "Data columns (total 7 columns):\n",
      "id        47306 non-null int64\n",
      "gender    47306 non-null object\n",
      "age       47306 non-null int64\n",
      "topic     47306 non-null object\n",
      "sign      47306 non-null object\n",
      "date      46857 non-null datetime64[ns]\n",
      "text      47306 non-null object\n",
      "dtypes: datetime64[ns](1), int64(2), object(4)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "clean_blog_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge gender,age,topic,sign to form the labels\n",
    "clean_blog_df[\"labels\"] = clean_blog_df[[\"gender\",\"age\",\"topic\",\"sign\"]].apply(lambda row: list(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20423</th>\n",
       "      <td>3460671</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>Banking</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>2004-05-29</td>\n",
       "      <td>finding website google search naughty little m...</td>\n",
       "      <td>[male, 27, Banking, Sagittarius]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id gender  age    topic         sign       date  \\\n",
       "20423  3460671   male   27  Banking  Sagittarius 2004-05-29   \n",
       "\n",
       "                                                    text  \\\n",
       "20423  finding website google search naughty little m...   \n",
       "\n",
       "                                 labels  \n",
       "20423  [male, 27, Banking, Sagittarius]  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity\n",
    "clean_blog_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the other columns and retain only text and labels\n",
    "clean_blog_df.drop([\"id\",\"gender\",\"age\",\"topic\",\"sign\",\"date\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20530</th>\n",
       "      <td>reality tv tomorrow final rose ceremony two ba...</td>\n",
       "      <td>[female, 27, indUnk, Aries]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "20530  reality tv tomorrow final rose ceremony two ba...   \n",
       "\n",
       "                            labels  \n",
       "20530  [female, 27, indUnk, Aries]  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity\n",
    "clean_blog_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate features and labels, and split the data into training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets split the dataframe into features and labels\n",
    "features = clean_blog_df[\"text\"]\n",
    "labels = clean_blog_df[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32336    wonderful hitchens piece value gay marriage be...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "features.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18105    [male, 16, Student, Pisces]\n",
       "Name: labels, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "labels.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into training and test\n",
    "#import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will split to have 20% data\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features,labels,test_size=0.2,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features (37844,)\n",
      "Test Features (9462,)\n",
      "Train Labels (37844,)\n",
      "Test Labels (9462,)\n"
     ]
    }
   ],
   "source": [
    "#Sanity check\n",
    "print(\"Train Features\",features_train.shape)\n",
    "print(\"Test Features\",features_test.shape)\n",
    "print(\"Train Labels\",labels_train.shape)\n",
    "print(\"Test Labels\",labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the features \n",
    "\n",
    "a. Create a Bag of Words using count vectorizer\n",
    "    i. Use ngram_range=(1, 2) \n",
    "    ii. Vectorize training and testing features b. Print the term-document matrix\n",
    "\n",
    "b. Print the term-document matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import countvectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize with ngram as 1,2\n",
    "cvect = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit on the training set\n",
    "features_train_dtm = cvect.fit_transform(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hey': 951863,\n",
       " 'everyone': 662121,\n",
       " 'new': 1423842,\n",
       " 'picture': 1573564,\n",
       " 'officially': 1470394,\n",
       " 'tonight': 2165711,\n",
       " 'cell': 318435,\n",
       " 'group': 882459,\n",
       " 'christmas': 350540,\n",
       " 'party': 1533611,\n",
       " 'well': 2326294,\n",
       " 'many': 1283348,\n",
       " 'others': 1503886,\n",
       " 'go': 840251,\n",
       " 'wwwpbasecomtimkang': 2397956,\n",
       " 'check': 334297,\n",
       " 'saw': 1806824,\n",
       " 'return': 1754223,\n",
       " 'king': 1124537,\n",
       " 'wednesday': 2318592,\n",
       " 'night': 1435625,\n",
       " 'wa': 2272345,\n",
       " 'expected': 678403,\n",
       " 'thoroughly': 2126904,\n",
       " 'satisfied': 1803938,\n",
       " 'im': 1013895,\n",
       " 'still': 1999724,\n",
       " 'scifi': 1822990,\n",
       " 'guy': 892526,\n",
       " 'series': 1855870,\n",
       " 'definitely': 504943,\n",
       " 'take': 2064826,\n",
       " 'cake': 282756,\n",
       " 'best': 195936,\n",
       " 'movie': 1375050,\n",
       " 'trilogy': 2191039,\n",
       " 'hey everyone': 952092,\n",
       " 'everyone new': 662907,\n",
       " 'new picture': 1425730,\n",
       " 'picture officially': 1574477,\n",
       " 'officially picture': 1470542,\n",
       " 'picture tonight': 1574902,\n",
       " 'tonight cell': 2165848,\n",
       " 'cell group': 318509,\n",
       " 'group christmas': 882651,\n",
       " 'christmas party': 350752,\n",
       " 'party well': 1534883,\n",
       " 'well many': 2328352,\n",
       " 'many others': 1284539,\n",
       " 'others go': 1504288,\n",
       " 'go wwwpbasecomtimkang': 845209,\n",
       " 'wwwpbasecomtimkang check': 2397957,\n",
       " 'check saw': 335207,\n",
       " 'saw return': 1808049,\n",
       " 'return king': 1754512,\n",
       " 'king wednesday': 1125003,\n",
       " 'wednesday night': 2318843,\n",
       " 'night wa': 1438260,\n",
       " 'wa expected': 2275673,\n",
       " 'expected wa': 678771,\n",
       " 'wa thoroughly': 2282480,\n",
       " 'thoroughly satisfied': 2126966,\n",
       " 'satisfied im': 1803988,\n",
       " 'im still': 1018078,\n",
       " 'still scifi': 2001897,\n",
       " 'scifi guy': 1823012,\n",
       " 'guy series': 894369,\n",
       " 'series definitely': 1855973,\n",
       " 'definitely take': 505526,\n",
       " 'take cake': 2065104,\n",
       " 'cake best': 282784,\n",
       " 'best movie': 196900,\n",
       " 'movie trilogy': 1376767,\n",
       " 'help': 946272,\n",
       " 'get': 819054,\n",
       " 'sure': 2047524,\n",
       " 'mood': 1363446,\n",
       " 'black': 214798,\n",
       " 'cant': 296463,\n",
       " 'stand': 1981910,\n",
       " 'waiting': 2286655,\n",
       " 'hatchet': 925567,\n",
       " 'drop': 580913,\n",
       " 'absolutely': 4546,\n",
       " 'motivation': 1371233,\n",
       " 'page': 1516683,\n",
       " 'mandatory': 1281081,\n",
       " 'otso': 1505665,\n",
       " 'make': 1270713,\n",
       " 'call': 284335,\n",
       " 'goal': 845289,\n",
       " 'burn': 272252,\n",
       " 'little': 1217696,\n",
       " 'people': 1549391,\n",
       " 'faster': 706802,\n",
       " 'sense': 1851810,\n",
       " 'gon': 854220,\n",
       " 'na': 1394225,\n",
       " 'laid': 1147371,\n",
       " 'work': 2371577,\n",
       " 'ot': 1503653,\n",
       " 'life': 1190684,\n",
       " 'figure': 725890,\n",
       " 'point': 1601641,\n",
       " 'havent': 928404,\n",
       " 'heard': 935715,\n",
       " 'anything': 89944,\n",
       " 'job': 1088374,\n",
       " 'positive': 1613759,\n",
       " 'negativewhich': 1417401,\n",
       " 'good': 855514,\n",
       " 'real': 1701774,\n",
       " 'belgian': 188451,\n",
       " 'jobpray': 1090089,\n",
       " 'general': 814297,\n",
       " 'public': 1661687,\n",
       " 'exercising': 675391,\n",
       " 'writing': 2392884,\n",
       " 'muscle': 1387494,\n",
       " 'trying': 2199784,\n",
       " 'daily': 473210,\n",
       " 'thing': 2114058,\n",
       " 'help get': 946799,\n",
       " 'get sure': 823806,\n",
       " 'sure take': 2048939,\n",
       " 'take mood': 2066168,\n",
       " 'mood black': 1363492,\n",
       " 'black mood': 215256,\n",
       " 'mood cant': 1363519,\n",
       " 'cant stand': 297498,\n",
       " 'stand waiting': 1982660,\n",
       " 'waiting hatchet': 2286969,\n",
       " 'hatchet drop': 925568,\n",
       " 'drop absolutely': 580914,\n",
       " 'absolutely motivation': 4758,\n",
       " 'motivation page': 1371302,\n",
       " 'page check': 1516787,\n",
       " 'check still': 335306,\n",
       " 'still mandatory': 2001267,\n",
       " 'mandatory otso': 1281106,\n",
       " 'otso make': 1505666,\n",
       " 'make call': 1271074,\n",
       " 'call goal': 285057,\n",
       " 'goal goal': 845461,\n",
       " 'goal burn': 845347,\n",
       " 'burn little': 272417,\n",
       " 'little people': 1219757,\n",
       " 'people faster': 1550626,\n",
       " 'faster make': 706903,\n",
       " 'make sense': 1273045,\n",
       " 'sense gon': 1852067,\n",
       " 'gon na': 854233,\n",
       " 'na get': 1394703,\n",
       " 'get laid': 821714,\n",
       " 'laid work': 1147530,\n",
       " 'work ot': 2373636,\n",
       " 'ot life': 1503680,\n",
       " 'life cant': 1191085,\n",
       " 'cant figure': 296894,\n",
       " 'figure point': 726369,\n",
       " 'point still': 1603075,\n",
       " 'still havent': 2000874,\n",
       " 'havent heard': 928664,\n",
       " 'heard anything': 935759,\n",
       " 'anything job': 90699,\n",
       " 'job positive': 1089404,\n",
       " 'positive negativewhich': 1613898,\n",
       " 'negativewhich good': 1417402,\n",
       " 'good real': 857951,\n",
       " 'real positive': 1702605,\n",
       " 'positive belgian': 1613774,\n",
       " 'belgian jobpray': 188465,\n",
       " 'jobpray general': 1090090,\n",
       " 'general public': 814708,\n",
       " 'public exercising': 1661839,\n",
       " 'exercising writing': 675434,\n",
       " 'writing muscle': 2393414,\n",
       " 'muscle trying': 1387650,\n",
       " 'trying daily': 2199982,\n",
       " 'daily writing': 473556,\n",
       " 'writing positive': 2393508,\n",
       " 'positive thing': 1613976,\n",
       " 'short': 1881496,\n",
       " 'essay': 647279,\n",
       " 'state': 1989876,\n",
       " 'bribe': 254581,\n",
       " 'circa': 354475,\n",
       " 'jennifers': 1083752,\n",
       " 'city': 355781,\n",
       " 'short essay': 1881720,\n",
       " 'essay state': 647496,\n",
       " 'state bribe': 1990030,\n",
       " 'bribe circa': 254583,\n",
       " 'circa jennifers': 354491,\n",
       " 'jennifers city': 1083755,\n",
       " 'agree': 36848,\n",
       " 'disagree': 541450,\n",
       " 'thanks': 2102170,\n",
       " 'dialogue': 527283,\n",
       " 'agree point': 37144,\n",
       " 'point disagree': 1602057,\n",
       " 'disagree others': 541517,\n",
       " 'others thanks': 1504897,\n",
       " 'thanks dialogue': 2102398,\n",
       " 'going': 849272,\n",
       " 'sleeping': 1920222,\n",
       " 'bus': 273540,\n",
       " 'stop': 2005500,\n",
       " 'must': 1390565,\n",
       " 'say': 1808597,\n",
       " 'getting': 824948,\n",
       " 'bit': 210707,\n",
       " 'worried': 2380702,\n",
       " 'future': 801462,\n",
       " 'living': 1223270,\n",
       " 'situation': 1912012,\n",
       " 'talk': 2069841,\n",
       " 'mike': 1334375,\n",
       " 'amanda': 65236,\n",
       " 'everyday': 661641,\n",
       " 'seems': 1842509,\n",
       " 'discouraging': 543825,\n",
       " 'place': 1582229,\n",
       " 'sleep': 1919011,\n",
       " 'fantastic': 702902,\n",
       " 'cousin': 443201,\n",
       " 'crystal': 460804,\n",
       " 'need': 1413279,\n",
       " 'give': 832929,\n",
       " 'rental': 1738024,\n",
       " 'truck': 2194629,\n",
       " 'back': 148080,\n",
       " 'eventually': 657488,\n",
       " 'interview': 1056738,\n",
       " 'transfer': 2183633,\n",
       " 'next': 1429096,\n",
       " 'friday': 781737,\n",
       " 'noho': 1442347,\n",
       " 'target': 2076877,\n",
       " 'cross': 457021,\n",
       " 'finger': 736148,\n",
       " 'lateral': 1158331,\n",
       " 'move': 1373192,\n",
       " 'otherwise': 1505150,\n",
       " 'probably': 1641924,\n",
       " 'demote': 511159,\n",
       " 'stay': 1993224,\n",
       " 'company': 393374,\n",
       " 'last': 1153805,\n",
       " 'want': 2292864,\n",
       " 'somewhere': 1947513,\n",
       " 'else': 614929,\n",
       " 'especially': 645974,\n",
       " 'filling': 728623,\n",
       " 'application': 100355,\n",
       " 'apartment': 95737,\n",
       " 'speaking': 1964419,\n",
       " 'ask': 119523,\n",
       " 'possible': 1614667,\n",
       " 'someone': 1940894,\n",
       " 'raise': 1686684,\n",
       " 'rent': 1737776,\n",
       " 'finished': 737316,\n",
       " 'looking': 1237188,\n",
       " 'find': 732422,\n",
       " 'funnier': 799039,\n",
       " 'thought': 2129435,\n",
       " 'easier': 595627,\n",
       " 'closer': 368377,\n",
       " 'leaving': 1174106,\n",
       " 'theyre': 2111996,\n",
       " 'working': 2376061,\n",
       " 'harder': 922187,\n",
       " 'week': 2319369,\n",
       " 'month': 1361629,\n",
       " 'right': 1762406,\n",
       " 'bed': 180951,\n",
       " 'already': 56215,\n",
       " 'freakin': 776492,\n",
       " 'got': 862038,\n",
       " 'ago': 35522,\n",
       " 'wired': 2356256,\n",
       " 'sorry': 1954346,\n",
       " 'dont': 563314,\n",
       " 'camera': 292029,\n",
       " 'scanner': 1814834,\n",
       " 'ill': 1011384,\n",
       " 'rely': 1732103,\n",
       " 'chuck': 351579,\n",
       " 'eagerness': 592499,\n",
       " 'part': 1529405,\n",
       " 'done': 561164,\n",
       " 'packing': 1515899,\n",
       " 'another': 81947,\n",
       " 'day': 485152,\n",
       " 'ive': 1072713,\n",
       " 'worked': 2375030,\n",
       " 'odd': 1466339,\n",
       " 'shift': 1874309,\n",
       " 'lot': 1243415,\n",
       " 'huge': 993523,\n",
       " 'block': 220451,\n",
       " 'except': 670967,\n",
       " 'lazy': 1164297,\n",
       " 'anyway': 91896,\n",
       " 'five': 744315,\n",
       " 'road': 1769588,\n",
       " 'punch': 1665193,\n",
       " 'face': 689521,\n",
       " 'eye': 687050,\n",
       " 'soon': 1951254,\n",
       " 'ahead': 38470,\n",
       " 'exchange': 672375,\n",
       " 'michigan': 1330147,\n",
       " 'currency': 464764,\n",
       " 'california': 283907,\n",
       " 'hear': 934615,\n",
       " 'lower': 1253699,\n",
       " 'rate': 1692201,\n",
       " 'game': 804701,\n",
       " 'try': 2198308,\n",
       " 'morning': 1366326,\n",
       " 'come': 382727,\n",
       " 'care': 302521,\n",
       " 'dream': 573979,\n",
       " 'remember': 1733326,\n",
       " 'whatever': 2338078,\n",
       " 'doesnt': 557097,\n",
       " 'count': 436765,\n",
       " 'time': 2142182,\n",
       " 'going sleeping': 852252,\n",
       " 'sleeping bus': 1920273,\n",
       " 'bus stop': 273913,\n",
       " 'stop must': 2006254,\n",
       " 'must say': 1391523,\n",
       " 'say im': 1810126,\n",
       " 'im getting': 1015702,\n",
       " 'getting bit': 825113,\n",
       " 'bit worried': 212802,\n",
       " 'worried future': 2380803,\n",
       " 'future living': 801833,\n",
       " 'living situation': 1223896,\n",
       " 'situation talk': 1912575,\n",
       " 'talk mike': 2070830,\n",
       " 'mike amanda': 1334391,\n",
       " 'amanda everyday': 65313,\n",
       " 'everyday seems': 661917,\n",
       " 'seems bit': 1842617,\n",
       " 'bit discouraging': 211180,\n",
       " 'discouraging im': 543831,\n",
       " 'im worried': 1018825,\n",
       " 'worried place': 2380928,\n",
       " 'place sleep': 1583970,\n",
       " 'sleep fantastic': 1919327,\n",
       " 'fantastic cousin': 702947,\n",
       " 'cousin crystal': 443320,\n",
       " 'crystal need': 460858,\n",
       " 'need give': 1414262,\n",
       " 'give rental': 834292,\n",
       " 'rental truck': 1738090,\n",
       " 'truck back': 2194644,\n",
       " 'back eventually': 149262,\n",
       " 'eventually interview': 657702,\n",
       " 'interview transfer': 1057170,\n",
       " 'transfer next': 2183714,\n",
       " 'next friday': 1429493,\n",
       " 'friday noho': 782218,\n",
       " 'noho target': 1442354,\n",
       " 'target cross': 2076936,\n",
       " 'cross finger': 457101,\n",
       " 'finger get': 736316,\n",
       " 'get lateral': 821735,\n",
       " 'lateral move': 1158333,\n",
       " 'move otherwise': 1373823,\n",
       " 'otherwise probably': 1505395,\n",
       " 'probably demote': 1642213,\n",
       " 'demote stay': 511163,\n",
       " 'stay company': 1993384,\n",
       " 'company last': 393789,\n",
       " 'last thing': 1154915,\n",
       " 'thing want': 2117872,\n",
       " 'want go': 2293949,\n",
       " 'go somewhere': 844304,\n",
       " 'somewhere else': 1947662,\n",
       " 'else especially': 615279,\n",
       " 'especially filling': 646304,\n",
       " 'filling application': 728629,\n",
       " 'application apartment': 100367,\n",
       " 'apartment speaking': 96155,\n",
       " 'speaking apartment': 1964442,\n",
       " 'apartment everyone': 95880,\n",
       " 'everyone ask': 662189,\n",
       " 'ask mike': 120150,\n",
       " 'mike possible': 1334669,\n",
       " 'possible get': 1614913,\n",
       " 'get someone': 823535,\n",
       " 'someone raise': 1941977,\n",
       " 'raise rent': 1686858,\n",
       " 'rent apartment': 1737787,\n",
       " 'apartment finished': 95896,\n",
       " 'finished looking': 737625,\n",
       " 'looking find': 1237605,\n",
       " 'find funnier': 733184,\n",
       " 'funnier anything': 799042,\n",
       " 'anything else': 90375,\n",
       " 'else thought': 616049,\n",
       " 'thought work': 2131523,\n",
       " 'work wa': 2374752,\n",
       " 'wa going': 2276445,\n",
       " 'going easier': 850324,\n",
       " 'easier closer': 595675,\n",
       " 'closer get': 368491,\n",
       " 'get leaving': 821768,\n",
       " 'leaving theyre': 1174694,\n",
       " 'theyre working': 2113056,\n",
       " 'working harder': 2376553,\n",
       " 'harder week': 922427,\n",
       " 'week last': 2320505,\n",
       " 'last month': 1154443,\n",
       " 'month right': 1362584,\n",
       " 'right probably': 1764409,\n",
       " 'probably bed': 1642028,\n",
       " 'bed work': 181937,\n",
       " 'work already': 2371674,\n",
       " 'already freakin': 56645,\n",
       " 'freakin got': 776560,\n",
       " 'got work': 866187,\n",
       " 'work little': 2373262,\n",
       " 'little bit': 1217958,\n",
       " 'bit ago': 210740,\n",
       " 'ago im': 35993,\n",
       " 'im wired': 1018777,\n",
       " 'wired sleep': 2356288,\n",
       " 'sleep want': 1920033,\n",
       " 'want picture': 2294779,\n",
       " 'picture im': 1574195,\n",
       " 'im sorry': 1017967,\n",
       " 'sorry dont': 1954576,\n",
       " 'dont camera': 563550,\n",
       " 'camera scanner': 292341,\n",
       " 'scanner ill': 1814847,\n",
       " 'ill rely': 1012455,\n",
       " 'rely chuck': 1732108,\n",
       " 'chuck eagerness': 351625,\n",
       " 'eagerness part': 592501,\n",
       " 'part done': 1529908,\n",
       " 'done packing': 562173,\n",
       " 'packing another': 1515901,\n",
       " 'another day': 82418,\n",
       " 'day ive': 487189,\n",
       " 'ive worked': 1074553,\n",
       " 'worked odd': 2375403,\n",
       " 'odd shift': 1466630,\n",
       " 'shift havent': 1874399,\n",
       " 'havent lot': 928737,\n",
       " 'lot huge': 1244436,\n",
       " 'huge block': 993589,\n",
       " 'block anything': 220466,\n",
       " 'anything except': 90418,\n",
       " 'except im': 671319,\n",
       " 'im lazy': 1016378,\n",
       " 'lazy anyway': 1164312,\n",
       " 'anyway five': 92341,\n",
       " 'five day': 744415,\n",
       " 'day road': 488491,\n",
       " 'road punch': 1770080,\n",
       " 'punch face': 1665222,\n",
       " 'face black': 689646,\n",
       " 'black eye': 215023,\n",
       " 'eye sure': 688207,\n",
       " 'sure soon': 2048847,\n",
       " 'soon probably': 1951997,\n",
       " 'probably go': 1642429,\n",
       " 'go ahead': 840325,\n",
       " 'ahead exchange': 38571,\n",
       " 'exchange michigan': 672470,\n",
       " 'michigan currency': 1330180,\n",
       " 'currency california': 464765,\n",
       " 'california currency': 283966,\n",
       " 'currency get': 464780,\n",
       " 'get hear': 821192,\n",
       " 'hear lower': 935189,\n",
       " 'lower exchange': 1253756,\n",
       " 'exchange rate': 672500,\n",
       " 'rate good': 1692307,\n",
       " 'good stay': 858381,\n",
       " 'stay ahead': 1993238,\n",
       " 'ahead game': 38585,\n",
       " 'game well': 806440,\n",
       " 'well im': 2327931,\n",
       " 'im going': 1015750,\n",
       " 'going try': 852713,\n",
       " 'try get': 2198795,\n",
       " 'get sleep': 823445,\n",
       " 'sleep morning': 1919603,\n",
       " 'morning come': 1366584,\n",
       " 'come everyone': 383460,\n",
       " 'everyone take': 663273,\n",
       " 'take care': 2065121,\n",
       " 'care dream': 302825,\n",
       " 'dream remember': 574650,\n",
       " 'remember whatever': 1734629,\n",
       " 'whatever dream': 2338237,\n",
       " 'dream doesnt': 574192,\n",
       " 'doesnt count': 557249,\n",
       " 'count real': 437030,\n",
       " 'real life': 1702383,\n",
       " 'life next': 1192620,\n",
       " 'next time': 1430247,\n",
       " 'song': 1948882,\n",
       " 'episode': 640650,\n",
       " 'simpson': 1901810,\n",
       " 'went': 2331973,\n",
       " 'musical': 1389648,\n",
       " 'broadway': 259143,\n",
       " 'betty': 201041,\n",
       " 'ford': 762164,\n",
       " 'clinic': 365850,\n",
       " 'welcome': 2325765,\n",
       " 'singing': 1905749,\n",
       " 'theme': 2107666,\n",
       " 'kotter': 1139547,\n",
       " 'youre': 2419354,\n",
       " 'ticket': 2138749,\n",
       " 'enough': 633470,\n",
       " 'show': 1885295,\n",
       " 'hot': 982169,\n",
       " 'john': 1091020,\n",
       " 'travolta': 2186509,\n",
       " 'oh': 1472463,\n",
       " 'mama': 1277171,\n",
       " 'calm': 289438,\n",
       " 'kiddingif': 1118886,\n",
       " 'werent': 2335771,\n",
       " 'id': 1004449,\n",
       " 'lying': 1259277,\n",
       " 'stoach': 2003625,\n",
       " 'full': 793933,\n",
       " 'pill': 1577354,\n",
       " 'alcohol': 46414,\n",
       " 'allie': 50015,\n",
       " 'wasnt': 2301311,\n",
       " 'hoped': 977561,\n",
       " 'one': 1482312,\n",
       " 'list': 1213183,\n",
       " 'hollywoodthen': 968694,\n",
       " 'thats': 2104115,\n",
       " 'disillusioning': 546966,\n",
       " 'settle': 1862039,\n",
       " 'kansaslike': 1105287,\n",
       " 'dorothy': 567337,\n",
       " 'marianne': 1287100,\n",
       " 'gilligans': 829518,\n",
       " 'islandnice': 1066244,\n",
       " 'peaceful': 1545643,\n",
       " 'moral': 1365223,\n",
       " 'goodhearted': 859411,\n",
       " 'wellestablished': 2330539,\n",
       " 'kansasill': 1105285,\n",
       " 'let': 1182718,\n",
       " 'pick': 1571498,\n",
       " 'though': 2127184,\n",
       " 'seem': 1840955,\n",
       " 'like': 1196826,\n",
       " 'telling': 2090362,\n",
       " 'big': 203577,\n",
       " 'captial': 299232,\n",
       " 'topeka': 2170707,\n",
       " 'popacatepotal': 1609995,\n",
       " 'canada': 294163,\n",
       " 'rather': 1692633,\n",
       " 'mexico': 1328150,\n",
       " 'mexicosorry': 1328303,\n",
       " 'randomness': 1689540,\n",
       " 'couldnt': 435724,\n",
       " 'resist': 1746133,\n",
       " 'really': 1705831,\n",
       " 'abilene': 1834,\n",
       " 'center': 319337,\n",
       " 'geographic': 817449,\n",
       " 'united': 2230388,\n",
       " 'albany': 45455,\n",
       " 'kansa': 1105233,\n",
       " 'small': 1925344,\n",
       " 'quaint': 1672996,\n",
       " 'ka': 1103567,\n",
       " 'mo': 1351625,\n",
       " 'kidding': 1118637,\n",
       " 'everybody': 661270,\n",
       " 'seen': 1843694,\n",
       " 'wwwdailyheraldcom': 2397671,\n",
       " 'look': 1233970,\n",
       " 'community': 392427,\n",
       " 'news': 1427356,\n",
       " 'geneva': 815995,\n",
       " 'story': 2009120,\n",
       " 'astronomy': 127071,\n",
       " 'dayits': 490043,\n",
       " 'meah': 1305159,\n",
       " 'wooo': 2368772,\n",
       " 'exciting': 673536,\n",
       " 'talkingim': 2074118,\n",
       " 'miss': 1346123,\n",
       " 'youits': 2417816,\n",
       " 'almost': 52190,\n",
       " 'augustyoure': 135229,\n",
       " 'leavingexcept': 1174786,\n",
       " 'underclassman': 2223270,\n",
       " 'case': 308116,\n",
       " 'leavingill': 1174790,\n",
       " 'allsome': 51724,\n",
       " 'know': 1130604,\n",
       " 'wonder': 2364553,\n",
       " 'bascially': 166739,\n",
       " 'read': 1696926,\n",
       " 'blogand': 223047,\n",
       " 'blog': 221064,\n",
       " 'pretty': 1635195,\n",
       " 'safe': 1792373,\n",
       " 'bet': 197834,\n",
       " 'joei': 1090595,\n",
       " 'hate': 925578,\n",
       " 'christy': 350985,\n",
       " 'gagan': 803038,\n",
       " 'dan': 476075,\n",
       " 'shane': 1867969,\n",
       " 'jennifer': 1083562,\n",
       " 'tim': 2141913,\n",
       " 'renee': 1737348,\n",
       " 'mambo': 1277383,\n",
       " 'anonymous': 81755,\n",
       " 'anyone': 88881,\n",
       " 'evrybody': 667230,\n",
       " 'cub': 461378,\n",
       " 'run': 1785133,\n",
       " 'homerspitifulsosa': 972224,\n",
       " 'ramirez': 1688022,\n",
       " 'lee': 1175670,\n",
       " 'todaythats': 2157229,\n",
       " 'folksphillies': 755646,\n",
       " 'win': 2351885,\n",
       " 'random': 1688909,\n",
       " 'factoid': 693121,\n",
       " 'font': 757621,\n",
       " 'herald': 949674,\n",
       " 'us': 2241189,\n",
       " 'utopiap': 2249300,\n",
       " 'pip': 1579451,\n",
       " 'song episode': 1949322,\n",
       " 'episode simpson': 640969,\n",
       " 'simpson went': 1901954,\n",
       " 'went musical': 2334105,\n",
       " 'musical broadway': 1389681,\n",
       " 'broadway betty': 259147,\n",
       " 'betty ford': 201052,\n",
       " 'ford clinic': 762177,\n",
       " 'clinic speaking': 365890,\n",
       " 'speaking welcome': 1964995,\n",
       " 'welcome back': 2325790,\n",
       " 'back singing': 151406,\n",
       " 'singing theme': 1906024,\n",
       " 'theme welcome': 2107984,\n",
       " 'back kotter': 150068,\n",
       " 'kotter youre': 1139549,\n",
       " 'youre dream': 2419708,\n",
       " 'dream ticket': 574828,\n",
       " 'ticket welcome': 2139196,\n",
       " 'back enough': 149232,\n",
       " 'enough good': 634111,\n",
       " 'good thing': 858578,\n",
       " 'thing show': 2117211,\n",
       " 'show wa': 1887286,\n",
       " 'wa hot': 2277054,\n",
       " 'hot john': 982520,\n",
       " 'john travolta': 1091622,\n",
       " 'travolta oh': 2186516,\n",
       " 'oh mama': 1473154,\n",
       " 'mama everyone': 1277209,\n",
       " 'everyone calm': 662292,\n",
       " 'calm im': 289540,\n",
       " 'im kiddingif': 1016300,\n",
       " 'kiddingif werent': 1118887,\n",
       " 'werent id': 2335941,\n",
       " 'id lying': 1004938,\n",
       " 'lying stoach': 1259471,\n",
       " 'stoach full': 2003626,\n",
       " 'full pill': 794582,\n",
       " 'pill alcohol': 1577358,\n",
       " 'alcohol anyway': 46429,\n",
       " 'anyway welcome': 93331,\n",
       " 'back allie': 148166,\n",
       " 'allie im': 50021,\n",
       " 'sorry broadway': 1954458,\n",
       " 'broadway wasnt': 259190,\n",
       " 'wasnt hoped': 2301810,\n",
       " 'hoped check': 977573,\n",
       " 'check one': 335035,\n",
       " 'one list': 1485444,\n",
       " 'list next': 1213682,\n",
       " 'next stop': 1430167,\n",
       " 'stop hollywoodthen': 2006020,\n",
       " 'hollywoodthen thats': 968695,\n",
       " 'thats disillusioning': 2104664,\n",
       " 'disillusioning settle': 546968,\n",
       " 'settle kansaslike': 1862107,\n",
       " 'kansaslike dorothy': 1105288,\n",
       " 'dorothy marianne': 567352,\n",
       " 'marianne gilligans': 1287106,\n",
       " 'gilligans islandnice': 829520,\n",
       " 'islandnice peaceful': 1066245,\n",
       " 'peaceful moral': 1545704,\n",
       " 'moral goodhearted': 1365295,\n",
       " 'goodhearted wellestablished': 859413,\n",
       " 'wellestablished kansasill': 2330540,\n",
       " 'kansasill let': 1105286,\n",
       " 'let pick': 1183659,\n",
       " 'pick city': 1571627,\n",
       " 'city though': 356674,\n",
       " 'though doesnt': 2127676,\n",
       " 'doesnt seem': 557695,\n",
       " 'seem like': 1841365,\n",
       " 'like im': 1200466,\n",
       " 'im telling': 1018297,\n",
       " 'telling big': 2090401,\n",
       " 'big captial': 203807,\n",
       " 'captial topeka': 299234,\n",
       " 'topeka popacatepotal': 2170711,\n",
       " 'popacatepotal canada': 1609996,\n",
       " 'canada rather': 294339,\n",
       " 'rather mexico': 1693318,\n",
       " 'mexico mexico': 1328229,\n",
       " 'mexico mexicosorry': 1328230,\n",
       " 'mexicosorry randomness': 1328304,\n",
       " 'randomness couldnt': 1689545,\n",
       " 'couldnt resist': 436127,\n",
       " 'resist really': 1746217,\n",
       " 'really big': 1706106,\n",
       " 'big abilene': 203578,\n",
       " 'abilene center': 1837,\n",
       " 'center geographic': 319508,\n",
       " 'geographic united': 817482,\n",
       " 'united state': 2230451,\n",
       " 'state albany': 1989907,\n",
       " 'albany kansa': 45462,\n",
       " 'kansa really': 1105269,\n",
       " 'really small': 1708523,\n",
       " 'small quaint': 1926018,\n",
       " 'quaint kansa': 1673002,\n",
       " 'kansa city': 1105241,\n",
       " 'city ka': 356233,\n",
       " 'ka mo': 1103622,\n",
       " 'mo im': 1351701,\n",
       " 'im kidding': 1016298,\n",
       " 'kidding im': 1118730,\n",
       " 'im get': 1015698,\n",
       " 'get everybody': 820630,\n",
       " 'everybody havent': 661385,\n",
       " 'havent seen': 928893,\n",
       " 'seen go': 1844131,\n",
       " 'go wwwdailyheraldcom': 845197,\n",
       " 'wwwdailyheraldcom look': 2397672,\n",
       " 'look community': 1234345,\n",
       " 'community news': 392727,\n",
       " 'news geneva': 1427707,\n",
       " 'geneva find': 816001,\n",
       " 'find story': 734308,\n",
       " 'story astronomy': 2009190,\n",
       " 'astronomy dayits': 127077,\n",
       " 'dayits meah': 490046,\n",
       " 'meah wooo': 1305160,\n",
       " 'wooo exciting': 2368779,\n",
       " 'exciting anyway': 673562,\n",
       " 'anyway im': 92501,\n",
       " 'im done': 1015120,\n",
       " 'done talkingim': 562581,\n",
       " 'talkingim going': 2074119,\n",
       " 'going miss': 851369,\n",
       " 'miss youits': 1347255,\n",
       " 'youits almost': 2417817,\n",
       " 'almost augustyoure': 52242,\n",
       " 'augustyoure almost': 135230,\n",
       " 'almost leavingexcept': 52708,\n",
       " 'leavingexcept underclassman': 1174787,\n",
       " 'underclassman case': 2223271,\n",
       " 'case im': 308572,\n",
       " 'im almost': 1014009,\n",
       " 'almost leavingill': 52709,\n",
       " 'leavingill miss': 1174791,\n",
       " 'miss allsome': 1346149,\n",
       " 'allsome others': 51725,\n",
       " 'others know': 1504436,\n",
       " 'know doesnt': 1131667,\n",
       " 'doesnt make': 557513,\n",
       " 'make wonder': 1273694,\n",
       " 'wonder except': 2364780,\n",
       " 'except know': 671374,\n",
       " 'know bascially': 1130906,\n",
       " 'bascially read': 166741,\n",
       " 'read blogand': 1697087,\n",
       " 'blogand many': 223052,\n",
       " 'many people': 1284578,\n",
       " 'people read': 1552101,\n",
       " 'read blog': 1697086,\n",
       " 'blog id': 221868,\n",
       " 'id say': 1005173,\n",
       " 'say pretty': 1811072,\n",
       " 'pretty safe': 1636025,\n",
       " 'safe bet': 1792400,\n",
       " 'bet youre': 198182,\n",
       " 'youre one': 2420274,\n",
       " 'one except': 1484144,\n",
       " 'except joei': 671354,\n",
       " 'joei hate': 1090596,\n",
       " 'hate christy': 925795,\n",
       " 'christy gagan': 350999,\n",
       " 'gagan dan': 803039,\n",
       " 'dan shane': 476437,\n",
       " 'shane jennifer': 1867989,\n",
       " 'jennifer tim': 1083721,\n",
       " 'tim renee': 2142028,\n",
       " 'renee mike': 1737370,\n",
       " 'mike mambo': 1334610,\n",
       " 'mambo anonymous': 1277384,\n",
       " 'anonymous anyone': 81760,\n",
       " 'anyone else': 89109,\n",
       " 'else read': 615791,\n",
       " 'blog ill': 221878,\n",
       " 'ill really': 1012425,\n",
       " 'really miss': 1707753,\n",
       " 'miss good': 1346491,\n",
       " 'good night': 857545,\n",
       " 'night evrybody': 1436435,\n",
       " 'evrybody cub': 667231,\n",
       " 'cub run': 461465,\n",
       " 'run come': 1785323,\n",
       " 'come homerspitifulsosa': 383801,\n",
       " 'homerspitifulsosa ramirez': 972225,\n",
       " 'ramirez lee': 1688027,\n",
       " 'lee todaythats': 1175837,\n",
       " 'todaythats folksphillies': 2157230,\n",
       " 'folksphillies win': 755647,\n",
       " 'win random': 2352300,\n",
       " 'random factoid': 1689042,\n",
       " 'factoid font': 693130,\n",
       " 'font daily': 757637,\n",
       " 'daily herald': 473345,\n",
       " 'herald us': 949699,\n",
       " 'us utopiap': 2241402,\n",
       " 'utopiap pip': 2249301,\n",
       " 'pip pip': 1579467,\n",
       " 'guess': 887132,\n",
       " 'town': 2177864,\n",
       " 'brazil': 250438,\n",
       " 'whole': 2345123,\n",
       " 'country': 437979,\n",
       " 'silly': 1898858,\n",
       " 'national': 1404389,\n",
       " 'football': 760407,\n",
       " 'team': 2082845,\n",
       " 'mean': 1305493,\n",
       " 'see': 1835183,\n",
       " 'play': 1589067,\n",
       " 'catalunya': 311458,\n",
       " 'wicked': 2347693,\n",
       " 'atmosphere': 128749,\n",
       " 'crazy': 449953,\n",
       " 'brazilian': 250513,\n",
       " 'playing': 1592622,\n",
       " 'drum': 583130,\n",
       " 'fan': 701512,\n",
       " 'actually': 17088,\n",
       " 'making': 1274279,\n",
       " 'noise': 1442411,\n",
       " 'swear': 2054837,\n",
       " 'chanting': 328722,\n",
       " 'etc': 649161,\n",
       " 'put': 1669269,\n",
       " 'together': 2157990,\n",
       " 'mexican': 1328015,\n",
       " 'wave': 2308571,\n",
       " 'name': 1398216,\n",
       " 'ronaldo': 1775926,\n",
       " 'scored': 1823936,\n",
       " 'two': 2210755,\n",
       " 'cracking': 447114,\n",
       " 'half': 906776,\n",
       " 'even': 652502,\n",
       " 'overhead': 1510913,\n",
       " 'kick': 1116085,\n",
       " 'top': 2169712,\n",
       " 'special': 1965320,\n",
       " 'catwalk': 313385,\n",
       " 'saturday': 1804347,\n",
       " 'smacked': 1925282,\n",
       " 'faceagain': 690902,\n",
       " 'ended': 626356,\n",
       " 'passeig': 1536739,\n",
       " 'de': 491418,\n",
       " 'gracia': 869191,\n",
       " 'metro': 1327653,\n",
       " 'walking': 2290255,\n",
       " 'yellow': 2409921,\n",
       " 'green': 877990,\n",
       " 'line': 1209056,\n",
       " 'long': 1230797,\n",
       " 'interminable': 1054629,\n",
       " 'walk': 2288664,\n",
       " 'pretending': 1634964,\n",
       " 'matrix': 1296598,\n",
       " 'sound': 1957831,\n",
       " 'effect': 605188,\n",
       " 'everything': 663725,\n",
       " 'quite': 1679538,\n",
       " 'amusing': 72139,\n",
       " 'goodbye': 859027,\n",
       " 'girl': 830087,\n",
       " 'came': 290270,\n",
       " 'january': 1078996,\n",
       " 'sayin': 1812533,\n",
       " 'third': 2124871,\n",
       " 'year': 2405589,\n",
       " 'became': 178187,\n",
       " 'surreal': 2051375,\n",
       " 'experience': 679672,\n",
       " 'edgar': 601762,\n",
       " 'ha': 895854,\n",
       " 'gone': 854335,\n",
       " 'holiday': 967635,\n",
       " 'left': 1176028,\n",
       " 'alone': 53361,\n",
       " 'preparty': 1630801,\n",
       " 'forsnizzel': 768320,\n",
       " 'op': 1492701,\n",
       " 'anywho': 94636,\n",
       " 'something': 1942719,\n",
       " 'peace': 1545204,\n",
       " 'love': 1247580,\n",
       " 'guess wa': 888489,\n",
       " 'wa town': 2282678,\n",
       " 'town last': 2178201,\n",
       " 'last night': 1154484,\n",
       " 'night brazil': 1435907,\n",
       " 'brazil whole': 250510,\n",
       " 'whole country': 2345363,\n",
       " 'country dont': 438235,\n",
       " 'dont silly': 564875,\n",
       " 'silly national': 1899041,\n",
       " 'national football': 1404519,\n",
       " 'football team': 760678,\n",
       " 'team mean': 2083401,\n",
       " 'mean guess': 1306334,\n",
       " 'guess went': 888522,\n",
       " 'went see': 2334747,\n",
       " 'see play': 1838019,\n",
       " 'play catalunya': 1589272,\n",
       " 'catalunya wa': 311461,\n",
       " 'wa wicked': 2283462,\n",
       " 'wicked brazil': 2347708,\n",
       " 'brazil atmosphere': 250443,\n",
       " 'atmosphere wa': 128870,\n",
       " 'wa crazy': 2274525,\n",
       " 'crazy brazilian': 450042,\n",
       " 'brazilian playing': 250530,\n",
       " 'playing drum': 1592899,\n",
       " 'drum fan': 583181,\n",
       " 'fan actually': 701517,\n",
       " 'actually making': 18089,\n",
       " 'making noise': 1274983,\n",
       " 'noise swear': 1442618,\n",
       " 'swear wa': 2055064,\n",
       " 'wa chanting': 2273895,\n",
       " 'chanting etc': 328733,\n",
       " 'etc one': 649645,\n",
       " 'one game': 1484494,\n",
       " 'game others': 805782,\n",
       " 'others see': 1504747,\n",
       " 'see put': 1838169,\n",
       " 'put together': 1671005,\n",
       " 'together mexican': 2158678,\n",
       " 'mexican wave': 1328127,\n",
       " 'wave name': 2308736,\n",
       " 'name ronaldo': 1399675,\n",
       " 'ronaldo scored': 1775938,\n",
       " 'scored two': 1824036,\n",
       " 'two cracking': 2211225,\n",
       " 'cracking goal': 447138,\n",
       " 'goal going': 845463,\n",
       " 'going half': 850745,\n",
       " 'half time': 907617,\n",
       " 'time even': 2143729,\n",
       " 'even overhead': 654524,\n",
       " 'overhead kick': 1510937,\n",
       " 'kick top': 1116475,\n",
       " 'top special': 2170480,\n",
       " 'special went': 1965920,\n",
       " 'went catwalk': 2332481,\n",
       " 'catwalk saturday': 313389,\n",
       " 'saturday night': 1804760,\n",
       " 'night got': 1436655,\n",
       " 'got smacked': 865387,\n",
       " ...}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the vocab and size\n",
    "print(len(cvect.vocabulary_))\n",
    "cvect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37844, 2426702)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the shape of the training dtm now\n",
    "features_train_dtm.shape\n",
    "\n",
    "#Large number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the test set as well\n",
    "features_test_dtm = cvect.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9462, 2426702)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check shape\n",
    "features_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 951863)\t1\n",
      "  (0, 662121)\t1\n",
      "  (0, 1423842)\t1\n",
      "  (0, 1573564)\t2\n",
      "  (0, 1470394)\t1\n",
      "  (0, 2165711)\t1\n",
      "  (0, 318435)\t1\n",
      "  (0, 882459)\t1\n",
      "  (0, 350540)\t1\n",
      "  (0, 1533611)\t1\n",
      "  (0, 2326294)\t1\n",
      "  (0, 1283348)\t1\n",
      "  (0, 1503886)\t1\n",
      "  (0, 840251)\t1\n",
      "  (0, 2397956)\t1\n",
      "  (0, 334297)\t1\n",
      "  (0, 1806824)\t1\n",
      "  (0, 1754223)\t1\n",
      "  (0, 1124537)\t1\n",
      "  (0, 2318592)\t1\n",
      "  (0, 1435625)\t1\n",
      "  (0, 2272345)\t2\n",
      "  (0, 678403)\t1\n",
      "  (0, 2126904)\t1\n",
      "  (0, 1803938)\t1\n",
      "  :\t:\n",
      "  (37843, 1564133)\t1\n",
      "  (37843, 1090600)\t1\n",
      "  (37843, 78458)\t1\n",
      "  (37843, 1090280)\t1\n",
      "  (37843, 914779)\t1\n",
      "  (37843, 178178)\t1\n",
      "  (37843, 639452)\t1\n",
      "  (37843, 970012)\t1\n",
      "  (37843, 914817)\t1\n",
      "  (37843, 14734)\t1\n",
      "  (37843, 2174001)\t1\n",
      "  (37843, 2127893)\t1\n",
      "  (37843, 813978)\t1\n",
      "  (37843, 907649)\t1\n",
      "  (37843, 2210635)\t1\n",
      "  (37843, 2385847)\t1\n",
      "  (37843, 928260)\t1\n",
      "  (37843, 598303)\t1\n",
      "  (37843, 1256610)\t1\n",
      "  (37843, 627936)\t1\n",
      "  (37843, 985688)\t1\n",
      "  (37843, 959372)\t1\n",
      "  (37843, 145441)\t1\n",
      "  (37843, 2383509)\t1\n",
      "  (37843, 914822)\t1\n"
     ]
    }
   ],
   "source": [
    "#Print the DTM. This will be a sparse matrix\n",
    "print(features_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary to get the count of every label i.e. the key will be label name and value will be the total count of the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use defaultdict for this\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize with int type as we want to capture count\n",
    "label_counts = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate over the labels to generate the dict object\n",
    "#We will use the labels before split\n",
    "for row in labels:\n",
    "    for element in row:\n",
    "        label_counts[element] += 1 #increment if found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'male': 24209,\n",
       " 15: 3339,\n",
       " 'Student': 10064,\n",
       " 'Leo': 3574,\n",
       " 33: 1605,\n",
       " 'InvestmentBanking': 84,\n",
       " 'Aquarius': 4668,\n",
       " 'female': 23097,\n",
       " 14: 1962,\n",
       " 'indUnk': 16836,\n",
       " 'Aries': 7132,\n",
       " 25: 2730,\n",
       " 'Capricorn': 3577,\n",
       " 17: 6398,\n",
       " 'Gemini': 2342,\n",
       " 23: 5219,\n",
       " 'Non-Profit': 480,\n",
       " 'Cancer': 4404,\n",
       " 'Banking': 280,\n",
       " 37: 297,\n",
       " 'Sagittarius': 4389,\n",
       " 26: 2703,\n",
       " 24: 5539,\n",
       " 'Scorpio': 3065,\n",
       " 27: 3950,\n",
       " 'Education': 2564,\n",
       " 45: 91,\n",
       " 'Engineering': 1342,\n",
       " 'Libra': 4223,\n",
       " 'Science': 654,\n",
       " 34: 1859,\n",
       " 41: 392,\n",
       " 'Communications-Media': 1395,\n",
       " 'BusinessServices': 397,\n",
       " 'Sports-Recreation': 118,\n",
       " 'Virgo': 2746,\n",
       " 'Taurus': 3213,\n",
       " 'Arts': 1769,\n",
       " 'Pisces': 3973,\n",
       " 44: 37,\n",
       " 16: 3909,\n",
       " 'Internet': 1279,\n",
       " 'Museums-Libraries': 275,\n",
       " 'Accounting': 349,\n",
       " 39: 389,\n",
       " 35: 2994,\n",
       " 'Technology': 3960,\n",
       " 36: 1890,\n",
       " 'Law': 300,\n",
       " 46: 308,\n",
       " 'Consulting': 190,\n",
       " 'Automotive': 116,\n",
       " 42: 92,\n",
       " 'Religion': 243,\n",
       " 13: 728,\n",
       " 'Fashion': 1718,\n",
       " 38: 188,\n",
       " 43: 148,\n",
       " 'Publishing': 203,\n",
       " 40: 182,\n",
       " 'Marketing': 399,\n",
       " 'LawEnforcement-Security': 120,\n",
       " 'HumanResources': 79,\n",
       " 'Telecommunications': 12,\n",
       " 'Military': 190,\n",
       " 'Government': 556,\n",
       " 'Transportation': 180,\n",
       " 'Architecture': 61,\n",
       " 'Advertising': 262,\n",
       " 47: 199,\n",
       " 'Agriculture': 58,\n",
       " 'Biotech': 101,\n",
       " 'RealEstate': 17,\n",
       " 'Manufacturing': 436,\n",
       " 48: 158,\n",
       " 'Construction': 26,\n",
       " 'Chemicals': 71,\n",
       " 'Maritime': 54,\n",
       " 'Tourism': 63,\n",
       " 'Environment': 5}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check label counts now.\n",
    "#First convert to regular dict\n",
    "label_counts = dict(label_counts)\n",
    "\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the length of the dict keys to find the unique labels\n",
    "len(label_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are 80 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the labels \n",
    "\n",
    "As we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose, it is convenient to use MultiLabelBinarizer from sklearn a. Convert your train and test labels using MultiLabelBinarizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get binarizer from sklearn\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the MLB\n",
    "#We will feed the keys as the classes\n",
    "mlb = MultiLabelBinarizer(classes=list(label_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and transform on training\n",
    "#We will get the individual keys and enclose in array to avoid splitting by characters\n",
    "labels_train_enc = mlb.fit_transform(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform test labels\n",
    "labels_test_enc = mlb.transform(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 15, 'Student', 'Leo', 33, 'InvestmentBanking', 'Aquarius',\n",
       "       'female', 14, 'indUnk', 'Aries', 25, 'Capricorn', 17, 'Gemini', 23,\n",
       "       'Non-Profit', 'Cancer', 'Banking', 37, 'Sagittarius', 26, 24,\n",
       "       'Scorpio', 27, 'Education', 45, 'Engineering', 'Libra', 'Science',\n",
       "       34, 41, 'Communications-Media', 'BusinessServices',\n",
       "       'Sports-Recreation', 'Virgo', 'Taurus', 'Arts', 'Pisces', 44, 16,\n",
       "       'Internet', 'Museums-Libraries', 'Accounting', 39, 35,\n",
       "       'Technology', 36, 'Law', 46, 'Consulting', 'Automotive', 42,\n",
       "       'Religion', 13, 'Fashion', 38, 43, 'Publishing', 40, 'Marketing',\n",
       "       'LawEnforcement-Security', 'HumanResources', 'Telecommunications',\n",
       "       'Military', 'Government', 'Transportation', 'Architecture',\n",
       "       'Advertising', 47, 'Agriculture', 'Biotech', 'RealEstate',\n",
       "       'Manufacturing', 48, 'Construction', 'Chemicals', 'Maritime',\n",
       "       'Tourism', 'Environment'], dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the classes\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37844, 80)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of encoded train labels\n",
    "labels_train_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "labels_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9462, 80)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of encoded train labels\n",
    "labels_test_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity\n",
    "labels_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[str(x) for x in mlb.classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a classifier \n",
    "\n",
    "In this task, we suggest using the One-vs-Rest approach, which is implemented in OneVsRestClassifier class. In this approach k classifiers (= number of tags) are trained. As a basic classifier, use LogisticRegression. It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time because the number of classifiers to train is large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import needed libs\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,classification_report,f1_score,recall_score,precision_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the base classifier\n",
    "\n",
    "#We will start with Logistic Classifier\n",
    "lr = LogisticRegression(solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize our onevsrest classifier with LR as the base estimator\n",
    "clf = OneVsRestClassifier(estimator=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37844, 2426702)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37844, 80)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the classifier, make predictions and get the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train on the training features and labels\n",
    "clf.fit(features_train_dtm,labels_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the predictions\n",
    "y_pred = clf.predict(features_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlb.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 0.13253012048192772\n"
     ]
    }
   ],
   "source": [
    "#Overall Accuracy\n",
    "print(\"Accuracy Score\",clf.score(features_test_dtm,labels_test_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (macro) :  0.619\n",
      "Precision (micro) :  0.734\n",
      "Precision (weighted) :  0.720\n"
     ]
    }
   ],
   "source": [
    "#Get Precison scores - again using the tree settings for averaging\n",
    "avg = [\"macro\",\"micro\",\"weighted\"]\n",
    "for a in avg:\n",
    "    print(f'Precision ({a}) : {precision_score(labels_test_enc,y_pred,average=a,zero_division=0): 0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall (macro) :  0.186\n",
      "Recall (micro) :  0.385\n",
      "Recall (weighted) :  0.385\n"
     ]
    }
   ],
   "source": [
    "#Get Recall scores - again using the tree settings for averaging\n",
    "avg = [\"macro\",\"micro\",\"weighted\"]\n",
    "for a in avg:\n",
    "    print(f'Recall ({a}) : {recall_score(labels_test_enc,y_pred,average=a,zero_division=0): 0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (macro) :  0.269\n",
      "F1 Score (micro) :  0.505\n",
      "F1 Score (weighted) :  0.465\n"
     ]
    }
   ],
   "source": [
    "#Get F1 scores - We will compare macro, micro and weighted scores\n",
    "avg = [\"macro\",\"micro\",\"weighted\"]\n",
    "for a in avg:\n",
    "    print(f'F1 Score ({a}) : {f1_score(labels_test_enc,y_pred,average=a): 0.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets try out Hamming loss also\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss 0.037720619319382795\n"
     ]
    }
   ],
   "source": [
    "print(\"Hamming Loss\", hamming_loss(labels_test_enc,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Confusion and classification matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[3257, 1278],\n",
       "        [1119, 3808]],\n",
       "\n",
       "       [[8747,   53],\n",
       "        [ 490,  172]],\n",
       "\n",
       "       [[7179,  304],\n",
       "        [1229,  750]],\n",
       "\n",
       "       [[8712,   50],\n",
       "        [ 633,   67]],\n",
       "\n",
       "       [[9115,   22],\n",
       "        [ 292,   33]],\n",
       "\n",
       "       [[9442,    1],\n",
       "        [  12,    7]],\n",
       "\n",
       "       [[8481,   90],\n",
       "        [ 589,  302]],\n",
       "\n",
       "       [[3808, 1119],\n",
       "        [1278, 3257]],\n",
       "\n",
       "       [[9038,   27],\n",
       "        [ 320,   77]],\n",
       "\n",
       "       [[5419,  771],\n",
       "        [1728, 1544]],\n",
       "\n",
       "       [[7869,  161],\n",
       "        [ 977,  455]],\n",
       "\n",
       "       [[8865,   32],\n",
       "        [ 515,   50]],\n",
       "\n",
       "       [[8713,   39],\n",
       "        [ 530,  180]],\n",
       "\n",
       "       [[8019,  177],\n",
       "        [ 879,  387]],\n",
       "\n",
       "       [[8965,   27],\n",
       "        [ 411,   59]],\n",
       "\n",
       "       [[8351,   85],\n",
       "        [ 788,  238]],\n",
       "\n",
       "       [[9348,    3],\n",
       "        [ 109,    2]],\n",
       "\n",
       "       [[8530,   52],\n",
       "        [ 690,  190]],\n",
       "\n",
       "       [[9403,    2],\n",
       "        [  53,    4]],\n",
       "\n",
       "       [[9399,    2],\n",
       "        [  59,    2]],\n",
       "\n",
       "       [[8496,   44],\n",
       "        [ 684,  238]],\n",
       "\n",
       "       [[8863,   55],\n",
       "        [ 468,   76]],\n",
       "\n",
       "       [[8266,   99],\n",
       "        [ 760,  337]],\n",
       "\n",
       "       [[8813,   37],\n",
       "        [ 544,   68]],\n",
       "\n",
       "       [[8618,   65],\n",
       "        [ 618,  161]],\n",
       "\n",
       "       [[8882,   54],\n",
       "        [ 354,  172]],\n",
       "\n",
       "       [[9445,    1],\n",
       "        [  16,    0]],\n",
       "\n",
       "       [[9178,   13],\n",
       "        [ 206,   65]],\n",
       "\n",
       "       [[8577,   50],\n",
       "        [ 586,  249]],\n",
       "\n",
       "       [[9327,    1],\n",
       "        [ 111,   23]],\n",
       "\n",
       "       [[9081,   19],\n",
       "        [ 154,  208]],\n",
       "\n",
       "       [[9361,   13],\n",
       "        [  71,   17]],\n",
       "\n",
       "       [[9170,   14],\n",
       "        [ 263,   15]],\n",
       "\n",
       "       [[9380,   10],\n",
       "        [  53,   19]],\n",
       "\n",
       "       [[9440,    0],\n",
       "        [  22,    0]],\n",
       "\n",
       "       [[8870,   28],\n",
       "        [ 517,   47]],\n",
       "\n",
       "       [[8778,   50],\n",
       "        [ 582,   52]],\n",
       "\n",
       "       [[9067,   31],\n",
       "        [ 330,   34]],\n",
       "\n",
       "       [[8606,   44],\n",
       "        [ 640,  172]],\n",
       "\n",
       "       [[9455,    0],\n",
       "        [   7,    0]],\n",
       "\n",
       "       [[8616,   62],\n",
       "        [ 583,  201]],\n",
       "\n",
       "       [[9198,   24],\n",
       "        [ 219,   21]],\n",
       "\n",
       "       [[9394,    3],\n",
       "        [  42,   23]],\n",
       "\n",
       "       [[9393,    2],\n",
       "        [  28,   39]],\n",
       "\n",
       "       [[9373,    9],\n",
       "        [  63,   17]],\n",
       "\n",
       "       [[8774,   55],\n",
       "        [ 493,  140]],\n",
       "\n",
       "       [[8523,  104],\n",
       "        [ 662,  173]],\n",
       "\n",
       "       [[9056,   15],\n",
       "        [ 254,  137]],\n",
       "\n",
       "       [[9391,    4],\n",
       "        [  59,    8]],\n",
       "\n",
       "       [[9399,    3],\n",
       "        [  51,    9]],\n",
       "\n",
       "       [[9423,    0],\n",
       "        [  33,    6]],\n",
       "\n",
       "       [[9433,    3],\n",
       "        [  18,    8]],\n",
       "\n",
       "       [[9453,    1],\n",
       "        [   7,    1]],\n",
       "\n",
       "       [[9409,    6],\n",
       "        [  43,    4]],\n",
       "\n",
       "       [[9318,    9],\n",
       "        [ 112,   23]],\n",
       "\n",
       "       [[9075,   16],\n",
       "        [ 239,  132]],\n",
       "\n",
       "       [[9422,    1],\n",
       "        [  38,    1]],\n",
       "\n",
       "       [[9434,    1],\n",
       "        [  22,    5]],\n",
       "\n",
       "       [[9424,    3],\n",
       "        [  34,    1]],\n",
       "\n",
       "       [[9414,    3],\n",
       "        [  42,    3]],\n",
       "\n",
       "       [[9381,    0],\n",
       "        [  70,   11]],\n",
       "\n",
       "       [[9437,    1],\n",
       "        [  23,    1]],\n",
       "\n",
       "       [[9440,    1],\n",
       "        [  20,    1]],\n",
       "\n",
       "       [[9458,    0],\n",
       "        [   4,    0]],\n",
       "\n",
       "       [[9415,    2],\n",
       "        [  33,   12]],\n",
       "\n",
       "       [[9345,    3],\n",
       "        [ 109,    5]],\n",
       "\n",
       "       [[9428,    0],\n",
       "        [  34,    0]],\n",
       "\n",
       "       [[9445,    1],\n",
       "        [  13,    3]],\n",
       "\n",
       "       [[9403,    3],\n",
       "        [  40,   16]],\n",
       "\n",
       "       [[9418,    7],\n",
       "        [  34,    3]],\n",
       "\n",
       "       [[9453,    0],\n",
       "        [   9,    0]],\n",
       "\n",
       "       [[9441,    0],\n",
       "        [  19,    2]],\n",
       "\n",
       "       [[9456,    2],\n",
       "        [   4,    0]],\n",
       "\n",
       "       [[9376,    1],\n",
       "        [  67,   18]],\n",
       "\n",
       "       [[9432,    2],\n",
       "        [  26,    2]],\n",
       "\n",
       "       [[9455,    0],\n",
       "        [   7,    0]],\n",
       "\n",
       "       [[9444,    1],\n",
       "        [  13,    4]],\n",
       "\n",
       "       [[9448,    2],\n",
       "        [  11,    1]],\n",
       "\n",
       "       [[9447,    0],\n",
       "        [  14,    1]],\n",
       "\n",
       "       [[9461,    1],\n",
       "        [   0,    0]]], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "print(\"Confusion Matrix\")\n",
    "multilabel_confusion_matrix(labels_test_enc,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Difficult to interpret the confusion matrix. Lets add the class names for better visualization\n",
    "class_names = [str(x) for x in mlb.classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   male       0.75      0.77      0.76      4927\n",
      "                     15       0.76      0.26      0.39       662\n",
      "                Student       0.71      0.38      0.49      1979\n",
      "                    Leo       0.57      0.10      0.16       700\n",
      "                     33       0.60      0.10      0.17       325\n",
      "      InvestmentBanking       0.88      0.37      0.52        19\n",
      "               Aquarius       0.77      0.34      0.47       891\n",
      "                 female       0.74      0.72      0.73      4535\n",
      "                     14       0.74      0.19      0.31       397\n",
      "                 indUnk       0.67      0.47      0.55      3272\n",
      "                  Aries       0.74      0.32      0.44      1432\n",
      "                     25       0.61      0.09      0.15       565\n",
      "              Capricorn       0.82      0.25      0.39       710\n",
      "                     17       0.69      0.31      0.42      1266\n",
      "                 Gemini       0.69      0.13      0.21       470\n",
      "                     23       0.74      0.23      0.35      1026\n",
      "             Non-Profit       0.40      0.02      0.03       111\n",
      "                 Cancer       0.79      0.22      0.34       880\n",
      "                Banking       0.67      0.07      0.13        57\n",
      "                     37       0.50      0.03      0.06        61\n",
      "            Sagittarius       0.84      0.26      0.40       922\n",
      "                     26       0.58      0.14      0.23       544\n",
      "                     24       0.77      0.31      0.44      1097\n",
      "                Scorpio       0.65      0.11      0.19       612\n",
      "                     27       0.71      0.21      0.32       779\n",
      "              Education       0.76      0.33      0.46       526\n",
      "                     45       0.00      0.00      0.00        16\n",
      "            Engineering       0.83      0.24      0.37       271\n",
      "                  Libra       0.83      0.30      0.44       835\n",
      "                Science       0.96      0.17      0.29       134\n",
      "                     34       0.92      0.57      0.71       362\n",
      "                     41       0.57      0.19      0.29        88\n",
      "   Communications-Media       0.52      0.05      0.10       278\n",
      "       BusinessServices       0.66      0.26      0.38        72\n",
      "      Sports-Recreation       0.00      0.00      0.00        22\n",
      "                  Virgo       0.63      0.08      0.15       564\n",
      "                 Taurus       0.51      0.08      0.14       634\n",
      "                   Arts       0.52      0.09      0.16       364\n",
      "                 Pisces       0.80      0.21      0.33       812\n",
      "                     44       0.00      0.00      0.00         7\n",
      "                     16       0.76      0.26      0.38       784\n",
      "               Internet       0.47      0.09      0.15       240\n",
      "      Museums-Libraries       0.88      0.35      0.51        65\n",
      "             Accounting       0.95      0.58      0.72        67\n",
      "                     39       0.65      0.21      0.32        80\n",
      "                     35       0.72      0.22      0.34       633\n",
      "             Technology       0.62      0.21      0.31       835\n",
      "                     36       0.90      0.35      0.50       391\n",
      "                    Law       0.67      0.12      0.20        67\n",
      "                     46       0.75      0.15      0.25        60\n",
      "             Consulting       1.00      0.15      0.27        39\n",
      "             Automotive       0.73      0.31      0.43        26\n",
      "                     42       0.50      0.12      0.20         8\n",
      "               Religion       0.40      0.09      0.14        47\n",
      "                     13       0.72      0.17      0.28       135\n",
      "                Fashion       0.89      0.36      0.51       371\n",
      "                     38       0.50      0.03      0.05        39\n",
      "                     43       0.83      0.19      0.30        27\n",
      "             Publishing       0.25      0.03      0.05        35\n",
      "                     40       0.50      0.07      0.12        45\n",
      "              Marketing       1.00      0.14      0.24        81\n",
      "LawEnforcement-Security       0.50      0.04      0.08        24\n",
      "         HumanResources       0.50      0.05      0.09        21\n",
      "     Telecommunications       0.00      0.00      0.00         4\n",
      "               Military       0.86      0.27      0.41        45\n",
      "             Government       0.62      0.04      0.08       114\n",
      "         Transportation       0.00      0.00      0.00        34\n",
      "           Architecture       0.75      0.19      0.30        16\n",
      "            Advertising       0.84      0.29      0.43        56\n",
      "                     47       0.30      0.08      0.13        37\n",
      "            Agriculture       0.00      0.00      0.00         9\n",
      "                Biotech       1.00      0.10      0.17        21\n",
      "             RealEstate       0.00      0.00      0.00         4\n",
      "          Manufacturing       0.95      0.21      0.35        85\n",
      "                     48       0.50      0.07      0.12        28\n",
      "           Construction       0.00      0.00      0.00         7\n",
      "              Chemicals       0.80      0.24      0.36        17\n",
      "               Maritime       0.33      0.08      0.13        12\n",
      "                Tourism       1.00      0.07      0.12        15\n",
      "            Environment       0.00      0.00      0.00         0\n",
      "\n",
      "              micro avg       0.73      0.38      0.51     37848\n",
      "              macro avg       0.62      0.19      0.27     37848\n",
      "           weighted avg       0.72      0.38      0.47     37848\n",
      "            samples avg       0.71      0.38      0.46     37848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification Report\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(labels_test_enc,y_pred,zero_division=0,target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can clearly see that the imbalance in class representation is causing the recall and precision of certain classes to be extremely low\n",
    "This causes the macro averages to be pulled down but micro average and weighted averages take into consideration this.\n",
    "We can see that the precision itself is better but recall is very poor. The overall accuracy is not very useful in evaluating the model.\n",
    "We can also see from the classification metrics how underepresented classes have performed poorly. \n",
    "Gender has good representation and has decent performance overall\n",
    "\n",
    "Use of the full dataset with all the classes having sufficent examples could improve the recall and overall performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print true label and predicted label for any five examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get some samples from the test set\n",
    "#Merge and take 2 samples\n",
    "sample_df = pd.concat([features_test,labels_test],axis=1).sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47133</th>\n",
       "      <td>brilliant jo excellent news amusing picture to...</td>\n",
       "      <td>[female, 17, Student, Capricorn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3590</th>\n",
       "      <td>eva dudetell didnt think head could really spi...</td>\n",
       "      <td>[male, 35, Technology, Aries]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28700</th>\n",
       "      <td>fineally got play christian like month day som...</td>\n",
       "      <td>[male, 13, Student, Cancer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>yes ill send card dont think get time co kinda...</td>\n",
       "      <td>[male, 23, indUnk, Pisces]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17355</th>\n",
       "      <td>ive several question asked project blog bloggi...</td>\n",
       "      <td>[female, 46, indUnk, Leo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "47133  brilliant jo excellent news amusing picture to...   \n",
       "3590   eva dudetell didnt think head could really spi...   \n",
       "28700  fineally got play christian like month day som...   \n",
       "14056  yes ill send card dont think get time co kinda...   \n",
       "17355  ive several question asked project blog bloggi...   \n",
       "\n",
       "                                 labels  \n",
       "47133  [female, 17, Student, Capricorn]  \n",
       "3590      [male, 35, Technology, Aries]  \n",
       "28700       [male, 13, Student, Cancer]  \n",
       "14056        [male, 23, indUnk, Pisces]  \n",
       "17355         [female, 46, indUnk, Leo]  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47133    [female, 17, Student, Capricorn]\n",
       "3590        [male, 35, Technology, Aries]\n",
       "28700         [male, 13, Student, Cancer]\n",
       "14056          [male, 23, indUnk, Pisces]\n",
       "17355           [female, 46, indUnk, Leo]\n",
       "Name: labels, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the true labels\n",
    "sample_labels = sample_df[\"labels\"]\n",
    "sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47133    brilliant jo excellent news amusing picture to...\n",
       "3590     eva dudetell didnt think head could really spi...\n",
       "28700    fineally got play christian like month day som...\n",
       "14056    yes ill send card dont think get time co kinda...\n",
       "17355    ive several question asked project blog bloggi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the text\n",
    "sample_features = sample_df[\"text\"]\n",
    "sample_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the text using the same process\n",
    "sample_features_dtm = cvect.transform(sample_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 51687)\t1\n",
      "  (0, 72139)\t3\n",
      "  (0, 72268)\t1\n",
      "  (0, 241428)\t1\n",
      "  (0, 241759)\t1\n",
      "  (0, 256020)\t1\n",
      "  (0, 481258)\t1\n",
      "  (0, 588122)\t1\n",
      "  (0, 670566)\t1\n",
      "  (0, 670762)\t1\n",
      "  (0, 982169)\t1\n",
      "  (0, 1088044)\t1\n",
      "  (0, 1239641)\t1\n",
      "  (0, 1380586)\t1\n",
      "  (0, 1427356)\t1\n",
      "  (0, 1528095)\t1\n",
      "  (0, 1528751)\t1\n",
      "  (0, 1573564)\t1\n",
      "  (0, 1574898)\t1\n",
      "  (0, 2162210)\t1\n",
      "  (0, 2162220)\t1\n",
      "  (0, 2272345)\t2\n",
      "  (0, 2272629)\t1\n",
      "  (0, 2277054)\t1\n",
      "  (0, 2306962)\t1\n",
      "  :\t:\n",
      "  (4, 1270713)\t1\n",
      "  (4, 1271669)\t1\n",
      "  (4, 1482312)\t1\n",
      "  (4, 1486499)\t1\n",
      "  (4, 1594485)\t1\n",
      "  (4, 1594668)\t1\n",
      "  (4, 1646907)\t1\n",
      "  (4, 1651760)\t2\n",
      "  (4, 1651812)\t1\n",
      "  (4, 1652263)\t1\n",
      "  (4, 1675554)\t2\n",
      "  (4, 1675627)\t1\n",
      "  (4, 1676352)\t1\n",
      "  (4, 1705831)\t1\n",
      "  (4, 1706829)\t1\n",
      "  (4, 1863001)\t1\n",
      "  (4, 1863379)\t1\n",
      "  (4, 1909656)\t1\n",
      "  (4, 1909713)\t1\n",
      "  (4, 1942719)\t1\n",
      "  (4, 1943186)\t1\n",
      "  (4, 2044181)\t1\n",
      "  (4, 2127184)\t1\n",
      "  (4, 2128822)\t1\n",
      "  (4, 2157990)\t1\n"
     ]
    }
   ],
   "source": [
    "#Sanity\n",
    "print(sample_features_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict using the predictor\n",
    "sample_pred = clf.predict(sample_features_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check\n",
    "sample_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets do a reverse transofrm of this using mlb to get the actual values\n",
    "sample_pred_labels = mlb.inverse_transform(sample_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Student', 'female', 'Capricorn', 17),\n",
       " ('male',),\n",
       " ('female',),\n",
       " ('male',),\n",
       " ('female',)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check sanity\n",
    "sample_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF for storing true and predicted labels\n",
    "sample_results = pd.DataFrame(columns=[\"True Labels\",\"Predicted Labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true sample labels\n",
    "sample_results[\"True Labels\"] = sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted sample labels\n",
    "sample_results[\"Predicted Labels\"] = sample_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Labels</th>\n",
       "      <th>Predicted Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47133</th>\n",
       "      <td>[female, 17, Student, Capricorn]</td>\n",
       "      <td>(Student, female, Capricorn, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3590</th>\n",
       "      <td>[male, 35, Technology, Aries]</td>\n",
       "      <td>(male,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28700</th>\n",
       "      <td>[male, 13, Student, Cancer]</td>\n",
       "      <td>(female,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>[male, 23, indUnk, Pisces]</td>\n",
       "      <td>(male,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17355</th>\n",
       "      <td>[female, 46, indUnk, Leo]</td>\n",
       "      <td>(female,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            True Labels                  Predicted Labels\n",
       "47133  [female, 17, Student, Capricorn]  (Student, female, Capricorn, 17)\n",
       "3590      [male, 35, Technology, Aries]                           (male,)\n",
       "28700       [male, 13, Student, Cancer]                         (female,)\n",
       "14056        [male, 23, indUnk, Pisces]                           (male,)\n",
       "17355         [female, 46, indUnk, Leo]                         (female,)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#True and predicted for sample records\n",
    "sample_results\n",
    "\n",
    "#We can see that in this example some of the labels are correct while some are way off (and not even predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try out multiple configurations of models and vectorizer to see if any improvements can be achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Function to try out different classifiers and algorithms\n",
    "#X is cleaned and split test and train data set\n",
    "#y is encoded labels\n",
    "def test_model(X_train,X_test,y_train,y_test,\n",
    "               vect=CountVectorizer(ngram_range=(1,2)),est=LogisticRegression(),\n",
    "               sample_pred=False):\n",
    "    \n",
    "    #Fit and transform the word vectorizer on training features\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    \n",
    "    print(\"Training features shape\",X_train_dtm.shape)\n",
    "    \n",
    "    print(\"Length of vocab is\",len(vect.vocabulary_))\n",
    "    \n",
    "    #Transform test features\n",
    "    \n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    \n",
    "    print(\"Test features shape\",X_test_dtm.shape)\n",
    "    \n",
    "    #Build the onevsrest classifier\n",
    "    clf = OneVsRestClassifier(estimator=est)\n",
    "    \n",
    "    #Fit on train features\n",
    "    clf.fit(X_train_dtm,y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Predict on test\n",
    "    y_pred = clf.predict(X_test_dtm)\n",
    "    \n",
    "    print(\"Classification Metrics and Scores\")\n",
    "    print(\"Accuracy Score\",clf.score(X_test_dtm,y_test))\n",
    "    #Get Precison scores - again using the tree settings for averaging\n",
    "    avg = [\"macro\",\"micro\",\"weighted\"]\n",
    "    for a in avg:\n",
    "        print(f'Precision ({a}) : {precision_score(y_test,y_pred,average=a,zero_division=0): 0.3f}')\n",
    "        \n",
    "    for a in avg:   \n",
    "        print(f'Recall ({a}) : {recall_score(y_test,y_pred,average=a,zero_division=0): 0.3f}')\n",
    "        \n",
    "    for a in avg:\n",
    "        print(f'F1 Score ({a}) : {f1_score(y_test,y_pred,average=a): 0.3f}')\n",
    "        \n",
    "    print(\"Hamming Loss\", hamming_loss(labels_test_enc,y_pred))\n",
    "        \n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_test,y_pred,zero_division=0,target_names=class_names))\n",
    "    \n",
    "    if sample_pred:\n",
    "        sample_df = pd.concat([features_test,labels_test],axis=1).sample(5)\n",
    "        sample_labels = sample_df[\"labels\"]\n",
    "        sample_features = sample_df[\"text\"]\n",
    "        sample_features_dtm = vect.transform(sample_features)\n",
    "        sample_pred = clf.predict(sample_features_dtm)\n",
    "        sample_pred_labels = mlb.inverse_transform(sample_pred)\n",
    "        sample_results = pd.DataFrame(columns=[\"True Labels\",\"Predicted Labels\"])\n",
    "        sample_results[\"True Labels\"] = sample_labels\n",
    "        sample_results[\"Predicted Labels\"] = sample_pred_labels\n",
    "        print(\"Sample Predictions\") #only in local notebook use display to get a pretty view\n",
    "        display(sample_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 1: MultinomialNB using default CountVectorizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape (37844, 2426702)\n",
      "Length of vocab is 2426702\n",
      "Test features shape (9462, 2426702)\n",
      "Classification Metrics and Scores\n",
      "Accuracy Score 0.02557598816317903\n",
      "Precision (macro) :  0.406\n",
      "Precision (micro) :  0.734\n",
      "Precision (weighted) :  0.793\n",
      "Recall (macro) :  0.039\n",
      "Recall (micro) :  0.220\n",
      "Recall (weighted) :  0.220\n",
      "F1 Score (macro) :  0.055\n",
      "F1 Score (micro) :  0.338\n",
      "F1 Score (weighted) :  0.250\n",
      "Hamming Loss 0.04299170365673219\n",
      "Classification Report\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   male       0.83      0.56      0.66      4927\n",
      "                     15       0.96      0.04      0.08       662\n",
      "                Student       0.91      0.10      0.17      1979\n",
      "                    Leo       1.00      0.01      0.03       700\n",
      "                     33       1.00      0.01      0.02       325\n",
      "      InvestmentBanking       0.00      0.00      0.00        19\n",
      "               Aquarius       0.94      0.13      0.23       891\n",
      "                 female       0.64      0.87      0.74      4535\n",
      "                     14       1.00      0.03      0.05       397\n",
      "                 indUnk       0.82      0.19      0.31      3272\n",
      "                  Aries       0.83      0.03      0.05      1432\n",
      "                     25       0.50      0.01      0.01       565\n",
      "              Capricorn       1.00      0.01      0.02       710\n",
      "                     17       0.96      0.04      0.07      1266\n",
      "                 Gemini       0.94      0.04      0.07       470\n",
      "                     23       0.95      0.04      0.07      1026\n",
      "             Non-Profit       0.00      0.00      0.00       111\n",
      "                 Cancer       0.83      0.05      0.09       880\n",
      "                Banking       0.00      0.00      0.00        57\n",
      "                     37       0.00      0.00      0.00        61\n",
      "            Sagittarius       0.96      0.02      0.05       922\n",
      "                     26       0.53      0.02      0.03       544\n",
      "                     24       1.00      0.03      0.05      1097\n",
      "                Scorpio       0.92      0.02      0.04       612\n",
      "                     27       1.00      0.02      0.03       779\n",
      "              Education       0.92      0.18      0.30       526\n",
      "                     45       0.00      0.00      0.00        16\n",
      "            Engineering       0.00      0.00      0.00       271\n",
      "                  Libra       0.97      0.04      0.08       835\n",
      "                Science       1.00      0.01      0.03       134\n",
      "                     34       0.90      0.21      0.34       362\n",
      "                     41       0.00      0.00      0.00        88\n",
      "   Communications-Media       1.00      0.01      0.01       278\n",
      "       BusinessServices       0.00      0.00      0.00        72\n",
      "      Sports-Recreation       0.00      0.00      0.00        22\n",
      "                  Virgo       0.71      0.01      0.02       564\n",
      "                 Taurus       0.53      0.01      0.02       634\n",
      "                   Arts       0.00      0.00      0.00       364\n",
      "                 Pisces       1.00      0.04      0.08       812\n",
      "                     44       0.00      0.00      0.00         7\n",
      "                     16       0.98      0.07      0.13       784\n",
      "               Internet       0.00      0.00      0.00       240\n",
      "      Museums-Libraries       0.00      0.00      0.00        65\n",
      "             Accounting       0.00      0.00      0.00        67\n",
      "                     39       1.00      0.01      0.02        80\n",
      "                     35       1.00      0.01      0.03       633\n",
      "             Technology       0.74      0.02      0.04       835\n",
      "                     36       1.00      0.01      0.02       391\n",
      "                    Law       0.00      0.00      0.00        67\n",
      "                     46       0.00      0.00      0.00        60\n",
      "             Consulting       0.00      0.00      0.00        39\n",
      "             Automotive       0.00      0.00      0.00        26\n",
      "                     42       0.00      0.00      0.00         8\n",
      "               Religion       0.00      0.00      0.00        47\n",
      "                     13       0.00      0.00      0.00       135\n",
      "                Fashion       1.00      0.01      0.02       371\n",
      "                     38       0.00      0.00      0.00        39\n",
      "                     43       0.00      0.00      0.00        27\n",
      "             Publishing       0.00      0.00      0.00        35\n",
      "                     40       0.00      0.00      0.00        45\n",
      "              Marketing       0.00      0.00      0.00        81\n",
      "LawEnforcement-Security       0.00      0.00      0.00        24\n",
      "         HumanResources       0.00      0.00      0.00        21\n",
      "     Telecommunications       0.00      0.00      0.00         4\n",
      "               Military       0.00      0.00      0.00        45\n",
      "             Government       0.00      0.00      0.00       114\n",
      "         Transportation       0.00      0.00      0.00        34\n",
      "           Architecture       0.00      0.00      0.00        16\n",
      "            Advertising       0.64      0.12      0.21        56\n",
      "                     47       0.57      0.11      0.18        37\n",
      "            Agriculture       0.00      0.00      0.00         9\n",
      "                Biotech       0.00      0.00      0.00        21\n",
      "             RealEstate       0.00      0.00      0.00         4\n",
      "          Manufacturing       0.00      0.00      0.00        85\n",
      "                     48       0.00      0.00      0.00        28\n",
      "           Construction       0.00      0.00      0.00         7\n",
      "              Chemicals       0.00      0.00      0.00        17\n",
      "               Maritime       0.00      0.00      0.00        12\n",
      "                Tourism       0.00      0.00      0.00        15\n",
      "            Environment       0.00      0.00      0.00         0\n",
      "\n",
      "              micro avg       0.73      0.22      0.34     37848\n",
      "              macro avg       0.41      0.04      0.05     37848\n",
      "           weighted avg       0.79      0.22      0.25     37848\n",
      "            samples avg       0.71      0.22      0.32     37848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test the models with various combinations\n",
    "#Check with MultinomialNB as the base estimator\n",
    "test_model(features_train,features_test,labels_train_enc,labels_test_enc,est=MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Multinomial did not perform as well as LR with the same configuration for this sample data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 2: MultinomialNB using CountVectorizer with min_df, max_df settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape (37844, 1424)\n",
      "Length of vocab is 1424\n",
      "Test features shape (9462, 1424)\n",
      "Classification Metrics and Scores\n",
      "Accuracy Score 0.0057070386810399495\n",
      "Precision (macro) :  0.145\n",
      "Precision (micro) :  0.281\n",
      "Precision (weighted) :  0.337\n",
      "Recall (macro) :  0.299\n",
      "Recall (micro) :  0.380\n",
      "Recall (weighted) :  0.380\n",
      "F1 Score (macro) :  0.177\n",
      "F1 Score (micro) :  0.323\n",
      "F1 Score (weighted) :  0.348\n",
      "Hamming Loss 0.07969245402663284\n",
      "Classification Report\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   male       0.67      0.58      0.62      4927\n",
      "                     15       0.19      0.40      0.26       662\n",
      "                Student       0.39      0.45      0.42      1979\n",
      "                    Leo       0.13      0.12      0.12       700\n",
      "                     33       0.15      0.31      0.20       325\n",
      "      InvestmentBanking       0.10      0.74      0.17        19\n",
      "               Aquarius       0.31      0.24      0.27       891\n",
      "                 female       0.60      0.69      0.64      4535\n",
      "                     14       0.16      0.39      0.22       397\n",
      "                 indUnk       0.45      0.42      0.43      3272\n",
      "                  Aries       0.23      0.19      0.21      1432\n",
      "                     25       0.14      0.18      0.16       565\n",
      "              Capricorn       0.19      0.24      0.21       710\n",
      "                     17       0.28      0.41      0.33      1266\n",
      "                 Gemini       0.13      0.16      0.15       470\n",
      "                     23       0.26      0.21      0.23      1026\n",
      "             Non-Profit       0.06      0.15      0.08       111\n",
      "                 Cancer       0.23      0.18      0.20       880\n",
      "                Banking       0.09      0.28      0.14        57\n",
      "                     37       0.02      0.15      0.04        61\n",
      "            Sagittarius       0.20      0.19      0.19       922\n",
      "                     26       0.13      0.21      0.16       544\n",
      "                     24       0.31      0.31      0.31      1097\n",
      "                Scorpio       0.14      0.13      0.13       612\n",
      "                     27       0.21      0.30      0.25       779\n",
      "              Education       0.25      0.44      0.32       526\n",
      "                     45       0.03      0.19      0.05        16\n",
      "            Engineering       0.20      0.37      0.25       271\n",
      "                  Libra       0.24      0.41      0.30       835\n",
      "                Science       0.12      0.39      0.19       134\n",
      "                     34       0.23      0.45      0.30       362\n",
      "                     41       0.08      0.58      0.15        88\n",
      "   Communications-Media       0.08      0.17      0.11       278\n",
      "       BusinessServices       0.14      0.43      0.21        72\n",
      "      Sports-Recreation       0.19      0.50      0.27        22\n",
      "                  Virgo       0.14      0.15      0.14       564\n",
      "                 Taurus       0.18      0.14      0.16       634\n",
      "                   Arts       0.10      0.15      0.12       364\n",
      "                 Pisces       0.18      0.16      0.17       812\n",
      "                     44       0.00      0.00      0.00         7\n",
      "                     16       0.22      0.41      0.28       784\n",
      "               Internet       0.09      0.17      0.12       240\n",
      "      Museums-Libraries       0.26      0.35      0.30        65\n",
      "             Accounting       0.15      0.25      0.19        67\n",
      "                     39       0.07      0.28      0.11        80\n",
      "                     35       0.16      0.28      0.21       633\n",
      "             Technology       0.18      0.26      0.21       835\n",
      "                     36       0.17      0.25      0.20       391\n",
      "                    Law       0.11      0.30      0.16        67\n",
      "                     46       0.07      0.47      0.12        60\n",
      "             Consulting       0.05      0.28      0.09        39\n",
      "             Automotive       0.11      0.69      0.18        26\n",
      "                     42       0.00      0.00      0.00         8\n",
      "               Religion       0.07      0.34      0.12        47\n",
      "                     13       0.08      0.40      0.14       135\n",
      "                Fashion       0.20      0.26      0.22       371\n",
      "                     38       0.02      0.13      0.03        39\n",
      "                     43       0.11      0.59      0.18        27\n",
      "             Publishing       0.09      0.23      0.13        35\n",
      "                     40       0.13      0.27      0.18        45\n",
      "              Marketing       0.08      0.26      0.12        81\n",
      "LawEnforcement-Security       0.03      0.17      0.04        24\n",
      "         HumanResources       0.11      0.33      0.17        21\n",
      "     Telecommunications       0.00      0.00      0.00         4\n",
      "               Military       0.10      0.47      0.17        45\n",
      "             Government       0.04      0.12      0.06       114\n",
      "         Transportation       0.04      0.06      0.05        34\n",
      "           Architecture       0.03      0.12      0.05        16\n",
      "            Advertising       0.06      0.23      0.09        56\n",
      "                     47       0.04      0.35      0.07        37\n",
      "            Agriculture       0.00      0.00      0.00         9\n",
      "                Biotech       0.22      0.52      0.31        21\n",
      "             RealEstate       0.00      0.00      0.00         4\n",
      "          Manufacturing       0.15      0.45      0.23        85\n",
      "                     48       0.12      0.46      0.19        28\n",
      "           Construction       0.02      0.14      0.03         7\n",
      "              Chemicals       0.24      0.59      0.34        17\n",
      "               Maritime       0.03      0.83      0.06        12\n",
      "                Tourism       0.05      0.40      0.09        15\n",
      "            Environment       0.00      0.00      0.00         0\n",
      "\n",
      "              micro avg       0.28      0.38      0.32     37848\n",
      "              macro avg       0.14      0.30      0.18     37848\n",
      "           weighted avg       0.34      0.38      0.35     37848\n",
      "            samples avg       0.38      0.38      0.32     37848\n",
      "\n",
      "Sample Predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Labels</th>\n",
       "      <th>Predicted Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26208</th>\n",
       "      <td>[male, 27, Law, Gemini]</td>\n",
       "      <td>(male, 33, InvestmentBanking, Aquarius, Aries,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18785</th>\n",
       "      <td>[male, 23, Student, Gemini]</td>\n",
       "      <td>(male,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6862</th>\n",
       "      <td>[male, 36, Fashion, Aries]</td>\n",
       "      <td>(male, Aries)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27754</th>\n",
       "      <td>[male, 16, indUnk, Sagittarius]</td>\n",
       "      <td>(male,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>[male, 17, Sports-Recreation, Capricorn]</td>\n",
       "      <td>(Student, female, Capricorn, 17, 27, Sports-Re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    True Labels  \\\n",
       "26208                   [male, 27, Law, Gemini]   \n",
       "18785               [male, 23, Student, Gemini]   \n",
       "6862                 [male, 36, Fashion, Aries]   \n",
       "27754           [male, 16, indUnk, Sagittarius]   \n",
       "867    [male, 17, Sports-Recreation, Capricorn]   \n",
       "\n",
       "                                        Predicted Labels  \n",
       "26208  (male, 33, InvestmentBanking, Aquarius, Aries,...  \n",
       "18785                                            (male,)  \n",
       "6862                                       (male, Aries)  \n",
       "27754                                            (male,)  \n",
       "867    (Student, female, Capricorn, 17, 27, Sports-Re...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test the models with various combinations\n",
    "#Check with MultinomialNB as the base estimator\n",
    "\n",
    "#Lets change some parameters of CV\n",
    "cv = CountVectorizer(ngram_range=(1,2),min_df=0.01,max_df=0.95)\n",
    "test_model(features_train,features_test,labels_train_enc,labels_test_enc,vect=cv,est=MultinomialNB(),sample_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The overall accuracy came down. Precision and recall was also affected. \n",
    "#The features were reduced significantly and this could have been the cause of this drop in learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 3: Logistic Regression using CountVectorizer with min_df, max_df settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape (37844, 1246)\n",
      "Length of vocab is 1246\n",
      "Test features shape (9462, 1246)\n",
      "Classification Metrics and Scores\n",
      "Accuracy Score 0.019023462270133164\n",
      "Precision (macro) :  0.308\n",
      "Precision (micro) :  0.557\n",
      "Precision (weighted) :  0.470\n",
      "Recall (macro) :  0.115\n",
      "Recall (micro) :  0.244\n",
      "Recall (weighted) :  0.244\n",
      "F1 Score (macro) :  0.158\n",
      "F1 Score (micro) :  0.339\n",
      "F1 Score (weighted) :  0.287\n",
      "Hamming Loss 0.04751902346227013\n",
      "Classification Report\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   male       0.65      0.71      0.68      4927\n",
      "                     15       0.40      0.10      0.16       662\n",
      "                Student       0.53      0.16      0.24      1979\n",
      "                    Leo       0.32      0.03      0.06       700\n",
      "                     33       0.36      0.09      0.15       325\n",
      "      InvestmentBanking       0.60      0.16      0.25        19\n",
      "               Aquarius       0.59      0.20      0.30       891\n",
      "                 female       0.65      0.58      0.61      4535\n",
      "                     14       0.20      0.05      0.08       397\n",
      "                 indUnk       0.52      0.20      0.29      3272\n",
      "                  Aries       0.39      0.05      0.09      1432\n",
      "                     25       0.30      0.05      0.09       565\n",
      "              Capricorn       0.39      0.07      0.11       710\n",
      "                     17       0.45      0.11      0.18      1266\n",
      "                 Gemini       0.24      0.04      0.07       470\n",
      "                     23       0.49      0.08      0.14      1026\n",
      "             Non-Profit       0.13      0.05      0.08       111\n",
      "                 Cancer       0.45      0.07      0.12       880\n",
      "                Banking       0.24      0.09      0.13        57\n",
      "                     37       0.04      0.02      0.02        61\n",
      "            Sagittarius       0.44      0.06      0.11       922\n",
      "                     26       0.23      0.05      0.08       544\n",
      "                     24       0.47      0.09      0.16      1097\n",
      "                Scorpio       0.22      0.02      0.04       612\n",
      "                     27       0.43      0.09      0.14       779\n",
      "              Education       0.62      0.26      0.37       526\n",
      "                     45       0.00      0.00      0.00        16\n",
      "            Engineering       0.52      0.23      0.32       271\n",
      "                  Libra       0.52      0.14      0.22       835\n",
      "                Science       0.34      0.13      0.19       134\n",
      "                     34       0.68      0.40      0.50       362\n",
      "                     41       0.30      0.15      0.20        88\n",
      "   Communications-Media       0.14      0.03      0.05       278\n",
      "       BusinessServices       0.35      0.24      0.28        72\n",
      "      Sports-Recreation       0.14      0.05      0.07        22\n",
      "                  Virgo       0.25      0.03      0.06       564\n",
      "                 Taurus       0.25      0.03      0.05       634\n",
      "                   Arts       0.19      0.04      0.07       364\n",
      "                 Pisces       0.29      0.04      0.06       812\n",
      "                     44       0.00      0.00      0.00         7\n",
      "                     16       0.46      0.10      0.16       784\n",
      "               Internet       0.17      0.02      0.04       240\n",
      "      Museums-Libraries       0.69      0.17      0.27        65\n",
      "             Accounting       0.38      0.21      0.27        67\n",
      "                     39       0.19      0.15      0.17        80\n",
      "                     35       0.42      0.09      0.15       633\n",
      "             Technology       0.44      0.08      0.14       835\n",
      "                     36       0.35      0.08      0.13       391\n",
      "                    Law       0.29      0.13      0.18        67\n",
      "                     46       0.21      0.12      0.15        60\n",
      "             Consulting       0.35      0.18      0.24        39\n",
      "             Automotive       0.44      0.27      0.33        26\n",
      "                     42       0.00      0.00      0.00         8\n",
      "               Religion       0.25      0.17      0.20        47\n",
      "                     13       0.23      0.08      0.12       135\n",
      "                Fashion       0.30      0.08      0.13       371\n",
      "                     38       0.00      0.00      0.00        39\n",
      "                     43       0.33      0.22      0.27        27\n",
      "             Publishing       0.08      0.03      0.04        35\n",
      "                     40       0.33      0.11      0.17        45\n",
      "              Marketing       0.39      0.16      0.23        81\n",
      "LawEnforcement-Security       0.17      0.04      0.07        24\n",
      "         HumanResources       0.12      0.05      0.07        21\n",
      "     Telecommunications       0.00      0.00      0.00         4\n",
      "               Military       0.21      0.09      0.12        45\n",
      "             Government       0.13      0.04      0.06       114\n",
      "         Transportation       0.00      0.00      0.00        34\n",
      "           Architecture       0.60      0.19      0.29        16\n",
      "            Advertising       0.33      0.18      0.23        56\n",
      "                     47       0.22      0.14      0.17        37\n",
      "            Agriculture       0.00      0.00      0.00         9\n",
      "                Biotech       0.43      0.29      0.34        21\n",
      "             RealEstate       0.00      0.00      0.00         4\n",
      "          Manufacturing       0.36      0.20      0.26        85\n",
      "                     48       0.18      0.07      0.10        28\n",
      "           Construction       0.00      0.00      0.00         7\n",
      "              Chemicals       0.60      0.18      0.27        17\n",
      "               Maritime       0.29      0.17      0.21        12\n",
      "                Tourism       0.40      0.13      0.20        15\n",
      "            Environment       0.00      0.00      0.00         0\n",
      "\n",
      "              micro avg       0.56      0.24      0.34     37848\n",
      "              macro avg       0.31      0.12      0.16     37848\n",
      "           weighted avg       0.47      0.24      0.29     37848\n",
      "            samples avg       0.59      0.24      0.32     37848\n",
      "\n",
      "Sample Predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Labels</th>\n",
       "      <th>Predicted Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27851</th>\n",
       "      <td>[male, 16, indUnk, Sagittarius]</td>\n",
       "      <td>(female, indUnk)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23934</th>\n",
       "      <td>[female, 23, Student, Scorpio]</td>\n",
       "      <td>(female,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20783</th>\n",
       "      <td>[female, 16, indUnk, Scorpio]</td>\n",
       "      <td>(female, indUnk, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14144</th>\n",
       "      <td>[male, 16, indUnk, Sagittarius]</td>\n",
       "      <td>(female,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10696</th>\n",
       "      <td>[female, 23, Arts, Capricorn]</td>\n",
       "      <td>(male,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           True Labels      Predicted Labels\n",
       "27851  [male, 16, indUnk, Sagittarius]      (female, indUnk)\n",
       "23934   [female, 23, Student, Scorpio]             (female,)\n",
       "20783    [female, 16, indUnk, Scorpio]  (female, indUnk, 16)\n",
       "14144  [male, 16, indUnk, Sagittarius]             (female,)\n",
       "10696    [female, 23, Arts, Capricorn]               (male,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test the models with various combinations\n",
    "#Check with LR as the base estimator\n",
    "\n",
    "#Lets change some parameters of CV\n",
    "cv = CountVectorizer(ngram_range=(1,2),min_df=0.01,max_df=0.9,stop_words=\"english\")\n",
    "\n",
    "test_model(features_train,features_test,labels_train_enc,labels_test_enc,\n",
    "           vect=cv,est=LogisticRegression(),\n",
    "           sample_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The overall accuracy came down. Precision and recall was also affected. Similar to the Multinomial case\n",
    "#The features were reduced significantly and this could have been the cause of this drop in learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 4: Logistic Regression using CountVectorizer with max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape (37844, 2000)\n",
      "Length of vocab is 2000\n",
      "Test features shape (9462, 2000)\n",
      "Classification Metrics and Scores\n",
      "Accuracy Score 0.046079053054322555\n",
      "Precision (macro) :  0.353\n",
      "Precision (micro) :  0.559\n",
      "Precision (weighted) :  0.499\n",
      "Recall (macro) :  0.159\n",
      "Recall (micro) :  0.299\n",
      "Recall (weighted) :  0.299\n",
      "F1 Score (macro) :  0.211\n",
      "F1 Score (micro) :  0.390\n",
      "F1 Score (weighted) :  0.353\n",
      "Hamming Loss 0.04685452335658423\n",
      "Classification Report\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   male       0.67      0.73      0.70      4927\n",
      "                     15       0.40      0.16      0.23       662\n",
      "                Student       0.55      0.23      0.33      1979\n",
      "                    Leo       0.24      0.05      0.08       700\n",
      "                     33       0.30      0.12      0.17       325\n",
      "      InvestmentBanking       0.70      0.37      0.48        19\n",
      "               Aquarius       0.58      0.27      0.37       891\n",
      "                 female       0.68      0.62      0.64      4535\n",
      "                     14       0.23      0.10      0.14       397\n",
      "                 indUnk       0.55      0.29      0.38      3272\n",
      "                  Aries       0.53      0.14      0.23      1432\n",
      "                     25       0.30      0.09      0.14       565\n",
      "              Capricorn       0.37      0.11      0.17       710\n",
      "                     17       0.52      0.18      0.27      1266\n",
      "                 Gemini       0.26      0.08      0.12       470\n",
      "                     23       0.48      0.16      0.24      1026\n",
      "             Non-Profit       0.09      0.05      0.06       111\n",
      "                 Cancer       0.41      0.11      0.17       880\n",
      "                Banking       0.37      0.12      0.18        57\n",
      "                     37       0.00      0.00      0.00        61\n",
      "            Sagittarius       0.59      0.19      0.29       922\n",
      "                     26       0.26      0.10      0.15       544\n",
      "                     24       0.49      0.17      0.25      1097\n",
      "                Scorpio       0.22      0.05      0.08       612\n",
      "                     27       0.48      0.16      0.24       779\n",
      "              Education       0.55      0.29      0.38       526\n",
      "                     45       0.00      0.00      0.00        16\n",
      "            Engineering       0.49      0.25      0.33       271\n",
      "                  Libra       0.52      0.21      0.30       835\n",
      "                Science       0.32      0.15      0.20       134\n",
      "                     34       0.77      0.58      0.66       362\n",
      "                     41       0.34      0.19      0.25        88\n",
      "   Communications-Media       0.14      0.06      0.08       278\n",
      "       BusinessServices       0.37      0.32      0.34        72\n",
      "      Sports-Recreation       0.36      0.18      0.24        22\n",
      "                  Virgo       0.25      0.07      0.11       564\n",
      "                 Taurus       0.23      0.05      0.09       634\n",
      "                   Arts       0.19      0.07      0.11       364\n",
      "                 Pisces       0.41      0.11      0.18       812\n",
      "                     44       0.00      0.00      0.00         7\n",
      "                     16       0.47      0.16      0.24       784\n",
      "               Internet       0.10      0.03      0.05       240\n",
      "      Museums-Libraries       0.65      0.26      0.37        65\n",
      "             Accounting       0.55      0.25      0.35        67\n",
      "                     39       0.31      0.19      0.23        80\n",
      "                     35       0.44      0.15      0.23       633\n",
      "             Technology       0.43      0.13      0.20       835\n",
      "                     36       0.58      0.29      0.39       391\n",
      "                    Law       0.22      0.12      0.16        67\n",
      "                     46       0.27      0.17      0.21        60\n",
      "             Consulting       0.36      0.23      0.28        39\n",
      "             Automotive       0.64      0.35      0.45        26\n",
      "                     42       0.00      0.00      0.00         8\n",
      "               Religion       0.19      0.13      0.15        47\n",
      "                     13       0.24      0.12      0.16       135\n",
      "                Fashion       0.55      0.29      0.38       371\n",
      "                     38       0.00      0.00      0.00        39\n",
      "                     43       0.39      0.26      0.31        27\n",
      "             Publishing       0.12      0.06      0.08        35\n",
      "                     40       0.44      0.16      0.23        45\n",
      "              Marketing       0.41      0.15      0.22        81\n",
      "LawEnforcement-Security       0.29      0.08      0.13        24\n",
      "         HumanResources       0.12      0.05      0.07        21\n",
      "     Telecommunications       0.00      0.00      0.00         4\n",
      "               Military       0.58      0.31      0.41        45\n",
      "             Government       0.10      0.04      0.06       114\n",
      "         Transportation       0.00      0.00      0.00        34\n",
      "           Architecture       0.60      0.19      0.29        16\n",
      "            Advertising       0.54      0.25      0.34        56\n",
      "                     47       0.24      0.14      0.17        37\n",
      "            Agriculture       0.00      0.00      0.00         9\n",
      "                Biotech       0.60      0.14      0.23        21\n",
      "             RealEstate       0.00      0.00      0.00         4\n",
      "          Manufacturing       0.39      0.26      0.31        85\n",
      "                     48       0.17      0.07      0.10        28\n",
      "           Construction       0.00      0.00      0.00         7\n",
      "              Chemicals       0.71      0.29      0.42        17\n",
      "               Maritime       0.33      0.17      0.22        12\n",
      "                Tourism       1.00      0.07      0.12        15\n",
      "            Environment       0.00      0.00      0.00         0\n",
      "\n",
      "              micro avg       0.56      0.30      0.39     37848\n",
      "              macro avg       0.35      0.16      0.21     37848\n",
      "           weighted avg       0.50      0.30      0.35     37848\n",
      "            samples avg       0.59      0.30      0.36     37848\n",
      "\n",
      "Sample Predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Labels</th>\n",
       "      <th>Predicted Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10941</th>\n",
       "      <td>[female, 33, indUnk, Cancer]</td>\n",
       "      <td>(male,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27038</th>\n",
       "      <td>[female, 23, indUnk, Aries]</td>\n",
       "      <td>(male,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20812</th>\n",
       "      <td>[female, 26, indUnk, Scorpio]</td>\n",
       "      <td>(female,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8193</th>\n",
       "      <td>[male, 26, Science, Scorpio]</td>\n",
       "      <td>(male,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22408</th>\n",
       "      <td>[female, 46, indUnk, Cancer]</td>\n",
       "      <td>(male, 34)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         True Labels Predicted Labels\n",
       "10941   [female, 33, indUnk, Cancer]          (male,)\n",
       "27038    [female, 23, indUnk, Aries]          (male,)\n",
       "20812  [female, 26, indUnk, Scorpio]        (female,)\n",
       "8193    [male, 26, Science, Scorpio]          (male,)\n",
       "22408   [female, 46, indUnk, Cancer]       (male, 34)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test the models with various combinations\n",
    "#Check with LR as the base estimator\n",
    "\n",
    "#Lets change some parameters of CV\n",
    "cv = CountVectorizer(ngram_range=(1,2),max_features=2000)\n",
    "\n",
    "test_model(features_train,features_test,labels_train_enc,labels_test_enc,\n",
    "           vect=cv,est=LogisticRegression(),\n",
    "           sample_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see accuracy is still low. The no of features is probably playing a role in decreasing performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 5: Logistic Regression using CountVectorizer with ngrams(1,3) and max_features changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape (37844, 30000)\n",
      "Length of vocab is 30000\n",
      "Test features shape (9462, 30000)\n",
      "Classification Metrics and Scores\n",
      "Accuracy Score 0.1313675755654196\n",
      "Precision (macro) :  0.528\n",
      "Precision (micro) :  0.629\n",
      "Precision (weighted) :  0.602\n",
      "Recall (macro) :  0.235\n",
      "Recall (micro) :  0.427\n",
      "Recall (weighted) :  0.427\n",
      "F1 Score (macro) :  0.313\n",
      "F1 Score (micro) :  0.509\n",
      "F1 Score (weighted) :  0.486\n",
      "Hamming Loss 0.041232033396744874\n",
      "Classification Report\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   male       0.74      0.74      0.74      4927\n",
      "                     15       0.59      0.33      0.42       662\n",
      "                Student       0.60      0.44      0.51      1979\n",
      "                    Leo       0.37      0.17      0.23       700\n",
      "                     33       0.46      0.16      0.23       325\n",
      "      InvestmentBanking       0.91      0.53      0.67        19\n",
      "               Aquarius       0.63      0.40      0.49       891\n",
      "                 female       0.72      0.71      0.71      4535\n",
      "                     14       0.54      0.26      0.35       397\n",
      "                 indUnk       0.59      0.52      0.55      3272\n",
      "                  Aries       0.58      0.37      0.45      1432\n",
      "                     25       0.36      0.15      0.21       565\n",
      "              Capricorn       0.63      0.32      0.43       710\n",
      "                     17       0.60      0.39      0.47      1266\n",
      "                 Gemini       0.44      0.18      0.26       470\n",
      "                     23       0.55      0.31      0.40      1026\n",
      "             Non-Profit       0.23      0.05      0.08       111\n",
      "                 Cancer       0.50      0.27      0.35       880\n",
      "                Banking       0.67      0.14      0.23        57\n",
      "                     37       0.36      0.07      0.11        61\n",
      "            Sagittarius       0.60      0.31      0.41       922\n",
      "                     26       0.40      0.20      0.26       544\n",
      "                     24       0.59      0.36      0.45      1097\n",
      "                Scorpio       0.40      0.17      0.24       612\n",
      "                     27       0.54      0.30      0.38       779\n",
      "              Education       0.68      0.37      0.48       526\n",
      "                     45       0.33      0.06      0.11        16\n",
      "            Engineering       0.73      0.30      0.43       271\n",
      "                  Libra       0.62      0.37      0.47       835\n",
      "                Science       0.70      0.22      0.34       134\n",
      "                     34       0.86      0.62      0.72       362\n",
      "                     41       0.55      0.25      0.34        88\n",
      "   Communications-Media       0.32      0.10      0.16       278\n",
      "       BusinessServices       0.57      0.29      0.39        72\n",
      "      Sports-Recreation       1.00      0.14      0.24        22\n",
      "                  Virgo       0.41      0.16      0.23       564\n",
      "                 Taurus       0.37      0.17      0.23       634\n",
      "                   Arts       0.36      0.16      0.22       364\n",
      "                 Pisces       0.59      0.28      0.38       812\n",
      "                     44       0.00      0.00      0.00         7\n",
      "                     16       0.59      0.32      0.42       784\n",
      "               Internet       0.35      0.13      0.19       240\n",
      "      Museums-Libraries       0.88      0.45      0.59        65\n",
      "             Accounting       0.89      0.48      0.62        67\n",
      "                     39       0.56      0.23      0.32        80\n",
      "                     35       0.59      0.29      0.39       633\n",
      "             Technology       0.50      0.28      0.36       835\n",
      "                     36       0.80      0.41      0.54       391\n",
      "                    Law       0.53      0.15      0.23        67\n",
      "                     46       0.58      0.23      0.33        60\n",
      "             Consulting       0.69      0.28      0.40        39\n",
      "             Automotive       0.75      0.35      0.47        26\n",
      "                     42       0.33      0.12      0.18         8\n",
      "               Religion       0.38      0.13      0.19        47\n",
      "                     13       0.64      0.22      0.33       135\n",
      "                Fashion       0.78      0.42      0.54       371\n",
      "                     38       0.33      0.03      0.05        39\n",
      "                     43       0.75      0.22      0.34        27\n",
      "             Publishing       0.27      0.09      0.13        35\n",
      "                     40       0.55      0.13      0.21        45\n",
      "              Marketing       0.84      0.20      0.32        81\n",
      "LawEnforcement-Security       0.20      0.04      0.07        24\n",
      "         HumanResources       1.00      0.19      0.32        21\n",
      "     Telecommunications       0.00      0.00      0.00         4\n",
      "               Military       0.86      0.40      0.55        45\n",
      "             Government       0.54      0.11      0.19       114\n",
      "         Transportation       0.00      0.00      0.00        34\n",
      "           Architecture       0.75      0.19      0.30        16\n",
      "            Advertising       0.75      0.32      0.45        56\n",
      "                     47       0.40      0.11      0.17        37\n",
      "            Agriculture       0.00      0.00      0.00         9\n",
      "                Biotech       0.67      0.10      0.17        21\n",
      "             RealEstate       0.00      0.00      0.00         4\n",
      "          Manufacturing       0.65      0.26      0.37        85\n",
      "                     48       0.44      0.14      0.22        28\n",
      "           Construction       0.00      0.00      0.00         7\n",
      "              Chemicals       0.80      0.24      0.36        17\n",
      "               Maritime       0.40      0.17      0.24        12\n",
      "                Tourism       0.50      0.07      0.12        15\n",
      "            Environment       0.00      0.00      0.00         0\n",
      "\n",
      "              micro avg       0.63      0.43      0.51     37848\n",
      "              macro avg       0.53      0.24      0.31     37848\n",
      "           weighted avg       0.60      0.43      0.49     37848\n",
      "            samples avg       0.62      0.43      0.48     37848\n",
      "\n",
      "Sample Predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Labels</th>\n",
       "      <th>Predicted Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40391</th>\n",
       "      <td>[male, 16, Student, Virgo]</td>\n",
       "      <td>(male, Virgo, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29568</th>\n",
       "      <td>[female, 27, Government, Leo]</td>\n",
       "      <td>(Leo, female, 27, Government)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25599</th>\n",
       "      <td>[male, 33, indUnk, Pisces]</td>\n",
       "      <td>(male, indUnk)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35776</th>\n",
       "      <td>[female, 26, Arts, Taurus]</td>\n",
       "      <td>(female,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10256</th>\n",
       "      <td>[male, 17, Student, Cancer]</td>\n",
       "      <td>(male, Student, 17, Cancer)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         True Labels               Predicted Labels\n",
       "40391     [male, 16, Student, Virgo]              (male, Virgo, 16)\n",
       "29568  [female, 27, Government, Leo]  (Leo, female, 27, Government)\n",
       "25599     [male, 33, indUnk, Pisces]                 (male, indUnk)\n",
       "35776     [female, 26, Arts, Taurus]                      (female,)\n",
       "10256    [male, 17, Student, Cancer]    (male, Student, 17, Cancer)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test the models with various combinations\n",
    "#Check with LR as the base estimator\n",
    "\n",
    "#Lets change some parameters of CV\n",
    "cv = CountVectorizer(ngram_range=(1,3),max_features=30000)\n",
    "\n",
    "test_model(features_train,features_test,labels_train_enc,labels_test_enc,\n",
    "           vect=cv,est=LogisticRegression(),\n",
    "           sample_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was better model with comparable accuracy, precision and recall with less no of features overall. \n",
    "#No of features and ngram setting seem to be giving more options than other settings. \n",
    "#At this time this seems to be the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape (37844, 15000)\n",
      "Length of vocab is 15000\n",
      "Test features shape (9462, 15000)\n",
      "Classification Metrics and Scores\n",
      "Accuracy Score 0.11805115197632636\n",
      "Precision (macro) :  0.479\n",
      "Precision (micro) :  0.596\n",
      "Precision (weighted) :  0.568\n",
      "Recall (macro) :  0.235\n",
      "Recall (micro) :  0.421\n",
      "Recall (weighted) :  0.421\n",
      "F1 Score (macro) :  0.305\n",
      "F1 Score (micro) :  0.494\n",
      "F1 Score (weighted) :  0.474\n",
      "Hamming Loss 0.043225533713802576\n",
      "Classification Report\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   male       0.73      0.74      0.73      4927\n",
      "                     15       0.53      0.33      0.41       662\n",
      "                Student       0.57      0.41      0.48      1979\n",
      "                    Leo       0.34      0.19      0.25       700\n",
      "                     33       0.38      0.16      0.23       325\n",
      "      InvestmentBanking       0.91      0.53      0.67        19\n",
      "               Aquarius       0.57      0.41      0.48       891\n",
      "                 female       0.71      0.70      0.70      4535\n",
      "                     14       0.48      0.25      0.33       397\n",
      "                 indUnk       0.59      0.48      0.53      3272\n",
      "                  Aries       0.55      0.37      0.44      1432\n",
      "                     25       0.32      0.16      0.22       565\n",
      "              Capricorn       0.53      0.29      0.37       710\n",
      "                     17       0.56      0.38      0.45      1266\n",
      "                 Gemini       0.38      0.19      0.26       470\n",
      "                     23       0.49      0.30      0.37      1026\n",
      "             Non-Profit       0.12      0.04      0.06       111\n",
      "                 Cancer       0.45      0.27      0.34       880\n",
      "                Banking       0.67      0.14      0.23        57\n",
      "                     37       0.21      0.05      0.08        61\n",
      "            Sagittarius       0.54      0.31      0.40       922\n",
      "                     26       0.38      0.23      0.29       544\n",
      "                     24       0.54      0.37      0.44      1097\n",
      "                Scorpio       0.33      0.17      0.23       612\n",
      "                     27       0.47      0.30      0.37       779\n",
      "              Education       0.66      0.39      0.49       526\n",
      "                     45       0.25      0.06      0.10        16\n",
      "            Engineering       0.67      0.30      0.42       271\n",
      "                  Libra       0.60      0.38      0.47       835\n",
      "                Science       0.67      0.21      0.32       134\n",
      "                     34       0.84      0.62      0.71       362\n",
      "                     41       0.45      0.22      0.29        88\n",
      "   Communications-Media       0.26      0.11      0.15       278\n",
      "       BusinessServices       0.55      0.31      0.39        72\n",
      "      Sports-Recreation       1.00      0.14      0.24        22\n",
      "                  Virgo       0.38      0.18      0.25       564\n",
      "                 Taurus       0.32      0.16      0.22       634\n",
      "                   Arts       0.34      0.17      0.23       364\n",
      "                 Pisces       0.55      0.28      0.37       812\n",
      "                     44       0.00      0.00      0.00         7\n",
      "                     16       0.55      0.32      0.40       784\n",
      "               Internet       0.30      0.12      0.18       240\n",
      "      Museums-Libraries       0.84      0.42      0.56        65\n",
      "             Accounting       0.81      0.45      0.58        67\n",
      "                     39       0.54      0.25      0.34        80\n",
      "                     35       0.54      0.29      0.38       633\n",
      "             Technology       0.46      0.28      0.35       835\n",
      "                     36       0.75      0.42      0.54       391\n",
      "                    Law       0.53      0.15      0.23        67\n",
      "                     46       0.50      0.20      0.29        60\n",
      "             Consulting       0.56      0.26      0.35        39\n",
      "             Automotive       0.75      0.35      0.47        26\n",
      "                     42       0.33      0.12      0.18         8\n",
      "               Religion       0.33      0.13      0.18        47\n",
      "                     13       0.54      0.23      0.32       135\n",
      "                Fashion       0.77      0.42      0.55       371\n",
      "                     38       0.00      0.00      0.00        39\n",
      "                     43       0.60      0.22      0.32        27\n",
      "             Publishing       0.25      0.09      0.13        35\n",
      "                     40       0.58      0.16      0.25        45\n",
      "              Marketing       0.70      0.20      0.31        81\n",
      "LawEnforcement-Security       0.29      0.08      0.13        24\n",
      "         HumanResources       0.50      0.14      0.22        21\n",
      "     Telecommunications       0.00      0.00      0.00         4\n",
      "               Military       0.81      0.38      0.52        45\n",
      "             Government       0.53      0.15      0.23       114\n",
      "         Transportation       0.00      0.00      0.00        34\n",
      "           Architecture       0.60      0.19      0.29        16\n",
      "            Advertising       0.72      0.32      0.44        56\n",
      "                     47       0.36      0.11      0.17        37\n",
      "            Agriculture       0.00      0.00      0.00         9\n",
      "                Biotech       0.67      0.10      0.17        21\n",
      "             RealEstate       0.00      0.00      0.00         4\n",
      "          Manufacturing       0.62      0.27      0.38        85\n",
      "                     48       0.33      0.11      0.16        28\n",
      "           Construction       0.00      0.00      0.00         7\n",
      "              Chemicals       0.86      0.35      0.50        17\n",
      "               Maritime       0.40      0.17      0.24        12\n",
      "                Tourism       0.50      0.07      0.12        15\n",
      "            Environment       0.00      0.00      0.00         0\n",
      "\n",
      "              micro avg       0.60      0.42      0.49     37848\n",
      "              macro avg       0.48      0.24      0.31     37848\n",
      "           weighted avg       0.57      0.42      0.47     37848\n",
      "            samples avg       0.60      0.42      0.46     37848\n",
      "\n",
      "Sample Predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Labels</th>\n",
       "      <th>Predicted Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42026</th>\n",
       "      <td>[female, 24, indUnk, Libra]</td>\n",
       "      <td>(female, indUnk, Aries, 24, Libra)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19433</th>\n",
       "      <td>[male, 35, Technology, Scorpio]</td>\n",
       "      <td>(female,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42078</th>\n",
       "      <td>[female, 24, indUnk, Libra]</td>\n",
       "      <td>(female, indUnk, 24, Libra)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45377</th>\n",
       "      <td>[male, 25, Law, Taurus]</td>\n",
       "      <td>(male, Leo, Aries, 25, Law)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48538</th>\n",
       "      <td>[male, 14, indUnk, Leo]</td>\n",
       "      <td>(male, Student, Leo, indUnk)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           True Labels                    Predicted Labels\n",
       "42026      [female, 24, indUnk, Libra]  (female, indUnk, Aries, 24, Libra)\n",
       "19433  [male, 35, Technology, Scorpio]                           (female,)\n",
       "42078      [female, 24, indUnk, Libra]         (female, indUnk, 24, Libra)\n",
       "45377          [male, 25, Law, Taurus]         (male, Leo, Aries, 25, Law)\n",
       "48538          [male, 14, indUnk, Leo]        (male, Student, Leo, indUnk)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test the models with various combinations\n",
    "#Check with LR as the base estimator\n",
    "\n",
    "#Lets change some parameters of CV\n",
    "cv = CountVectorizer(ngram_range=(1,3),max_features=15000)\n",
    "\n",
    "test_model(features_train,features_test,labels_train_enc,labels_test_enc,\n",
    "           vect=cv,est=LogisticRegression(),\n",
    "           sample_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple trials of the setting of max_features and ngram were tried. ngrams of 1,3 and max-features is getting comparatively\n",
    "#more accuracy, precision and recall.\n",
    "#We can also see from the samples prediction that in some cases the model predicts more than one label of the same category\n",
    "#for example two signs. This is expected for multi label classification\n",
    "#For future work - add the probability value as a confidence measure along with the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 6: Logistic Regression using CountVectorizer settings and classweight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets try manually since the predictions resulted in many labels of same category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize CV with parameter settings\n",
    "vect = CountVectorizer(ngram_range=(1,3),max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape (37844, 10000)\n",
      "Length of vocab is 10000\n",
      "Test features shape (9462, 10000)\n"
     ]
    }
   ],
   "source": [
    "#Fit and transform the word vectorizer on training features\n",
    "X_train_dtm = vect.fit_transform(features_train)\n",
    " \n",
    "print(\"Training features shape\",X_train_dtm.shape)\n",
    "  \n",
    "print(\"Length of vocab is\",len(vect.vocabulary_))\n",
    "    \n",
    "#Transform test features\n",
    "   \n",
    "X_test_dtm = vect.transform(features_test)\n",
    "  \n",
    "print(\"Test features shape\",X_test_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup base classifier as LR with class_weight settings\n",
    "\n",
    "base_est = LogisticRegression(class_weight=\"balanced\")\n",
    "\n",
    "#Build the onevsrest classifier\n",
    "clf_lr = OneVsRestClassifier(estimator=base_est)\n",
    " \n",
    "#Fit on train features\n",
    "clf_lr.fit(X_train_dtm,labels_train_enc)\n",
    "\n",
    "#Predict on test\n",
    "#capture both predictions and probabilities\n",
    "y_pred = clf_lr.predict(X_test_dtm)\n",
    "y_pred_proba = clf_lr.predict_proba(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function which takes the probability predictions and picks the top n labels\n",
    "def custom_prob(y_pred):\n",
    "    for row in y_pred:\n",
    "        #print(sorted(range(len(row)), key=lambda i: row[i], reverse=True)[:4])\n",
    "        topn = sorted(range(len(row)), key=lambda i: row[i], reverse=True)[:6]\n",
    "        for i in range(len(row)):\n",
    "            if i in topn:\n",
    "                row[i] = 1\n",
    "            else:\n",
    "                row[i] = 0\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute to convert probabilities to predictions\n",
    "y_pred = custom_prob(y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity\n",
    "#mlb.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics and Scores\n",
      "Accuracy Score 0.07926442612555486\n",
      "Precision (macro) :  0.231\n",
      "Precision (micro) :  0.362\n",
      "Precision (weighted) :  0.404\n",
      "Recall (macro) :  0.369\n",
      "Recall (micro) :  0.544\n",
      "Recall (weighted) :  0.544\n",
      "F1 Score (macro) :  0.280\n",
      "F1 Score (micro) :  0.435\n",
      "F1 Score (weighted) :  0.457\n",
      "Hamming Loss 0.07063517226801945\n",
      "Classification Report\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   male       0.70      0.73      0.71      4927\n",
      "                     15       0.30      0.50      0.37       662\n",
      "                Student       0.42      0.59      0.49      1979\n",
      "                    Leo       0.18      0.37      0.24       700\n",
      "                     33       0.17      0.29      0.21       325\n",
      "      InvestmentBanking       0.41      0.58      0.48        19\n",
      "               Aquarius       0.33      0.53      0.41       891\n",
      "                 female       0.69      0.70      0.69      4535\n",
      "                     14       0.26      0.44      0.33       397\n",
      "                 indUnk       0.48      0.61      0.54      3272\n",
      "                  Aries       0.35      0.59      0.44      1432\n",
      "                     25       0.16      0.31      0.21       565\n",
      "              Capricorn       0.28      0.46      0.35       710\n",
      "                     17       0.36      0.57      0.44      1266\n",
      "                 Gemini       0.18      0.35      0.24       470\n",
      "                     23       0.28      0.49      0.36      1026\n",
      "             Non-Profit       0.08      0.13      0.10       111\n",
      "                 Cancer       0.24      0.45      0.31       880\n",
      "                Banking       0.21      0.26      0.23        57\n",
      "                     37       0.06      0.10      0.07        61\n",
      "            Sagittarius       0.29      0.47      0.36       922\n",
      "                     26       0.20      0.36      0.26       544\n",
      "                     24       0.34      0.54      0.42      1097\n",
      "                Scorpio       0.17      0.34      0.23       612\n",
      "                     27       0.27      0.48      0.35       779\n",
      "              Education       0.34      0.47      0.39       526\n",
      "                     45       0.07      0.12      0.09        16\n",
      "            Engineering       0.30      0.45      0.36       271\n",
      "                  Libra       0.33      0.54      0.41       835\n",
      "                Science       0.22      0.37      0.28       134\n",
      "                     34       0.58      0.70      0.63       362\n",
      "                     41       0.29      0.40      0.33        88\n",
      "   Communications-Media       0.13      0.25      0.17       278\n",
      "       BusinessServices       0.19      0.38      0.25        72\n",
      "      Sports-Recreation       0.20      0.45      0.28        22\n",
      "                  Virgo       0.16      0.31      0.21       564\n",
      "                 Taurus       0.20      0.37      0.25       634\n",
      "                   Arts       0.18      0.33      0.23       364\n",
      "                 Pisces       0.27      0.45      0.33       812\n",
      "                     44       0.00      0.00      0.00         7\n",
      "                     16       0.31      0.50      0.38       784\n",
      "               Internet       0.13      0.30      0.18       240\n",
      "      Museums-Libraries       0.44      0.58      0.50        65\n",
      "             Accounting       0.41      0.67      0.51        67\n",
      "                     39       0.21      0.34      0.26        80\n",
      "                     35       0.29      0.59      0.39       633\n",
      "             Technology       0.27      0.55      0.37       835\n",
      "                     36       0.40      0.55      0.46       391\n",
      "                    Law       0.16      0.21      0.18        67\n",
      "                     46       0.23      0.38      0.29        60\n",
      "             Consulting       0.17      0.31      0.22        39\n",
      "             Automotive       0.19      0.46      0.27        26\n",
      "                     42       0.00      0.00      0.00         8\n",
      "               Religion       0.14      0.26      0.18        47\n",
      "                     13       0.21      0.41      0.28       135\n",
      "                Fashion       0.42      0.56      0.48       371\n",
      "                     38       0.08      0.08      0.08        39\n",
      "                     43       0.18      0.33      0.23        27\n",
      "             Publishing       0.08      0.11      0.09        35\n",
      "                     40       0.26      0.31      0.29        45\n",
      "              Marketing       0.13      0.30      0.18        81\n",
      "LawEnforcement-Security       0.04      0.08      0.05        24\n",
      "         HumanResources       0.26      0.43      0.33        21\n",
      "     Telecommunications       0.00      0.00      0.00         4\n",
      "               Military       0.45      0.67      0.54        45\n",
      "             Government       0.16      0.25      0.20       114\n",
      "         Transportation       0.03      0.06      0.04        34\n",
      "           Architecture       0.11      0.19      0.14        16\n",
      "            Advertising       0.45      0.45      0.45        56\n",
      "                     47       0.17      0.30      0.22        37\n",
      "            Agriculture       0.00      0.00      0.00         9\n",
      "                Biotech       0.35      0.57      0.44        21\n",
      "             RealEstate       0.00      0.00      0.00         4\n",
      "          Manufacturing       0.24      0.39      0.30        85\n",
      "                     48       0.07      0.29      0.11        28\n",
      "           Construction       0.00      0.00      0.00         7\n",
      "              Chemicals       0.28      0.53      0.37        17\n",
      "               Maritime       0.13      0.33      0.19        12\n",
      "                Tourism       0.13      0.40      0.20        15\n",
      "            Environment       0.00      0.00      0.00         0\n",
      "\n",
      "              micro avg       0.36      0.54      0.43     37848\n",
      "              macro avg       0.23      0.37      0.28     37848\n",
      "           weighted avg       0.40      0.54      0.46     37848\n",
      "            samples avg       0.36      0.54      0.43     37848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Metrics and Scores\")\n",
    "print(\"Accuracy Score\",clf_lr.score(X_test_dtm,labels_test_enc))\n",
    "#Get Precison scores - again using the tree settings for averaging\n",
    "avg = [\"macro\",\"micro\",\"weighted\"]\n",
    "for a in avg:\n",
    "    print(f'Precision ({a}) : {precision_score(labels_test_enc,y_pred,average=a,zero_division=0): 0.3f}')\n",
    "        \n",
    "for a in avg:   \n",
    "    print(f'Recall ({a}) : {recall_score(labels_test_enc,y_pred,average=a,zero_division=0): 0.3f}')\n",
    "        \n",
    "for a in avg:\n",
    "    print(f'F1 Score ({a}) : {f1_score(labels_test_enc,y_pred,average=a): 0.3f}')\n",
    "\n",
    "print(\"Hamming Loss\", hamming_loss(labels_test_enc,y_pred))\n",
    "        \n",
    "print(\"Classification Report\")\n",
    "print(classification_report(labels_test_enc,y_pred,zero_division=0,target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>19725</th>\n",
       "      <th>34039</th>\n",
       "      <th>32655</th>\n",
       "      <th>44124</th>\n",
       "      <th>44032</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Labels</th>\n",
       "      <td>[female, 35, LawEnforcement-Security, Cancer]</td>\n",
       "      <td>[male, 36, indUnk, Virgo]</td>\n",
       "      <td>[male, 26, Military, Aquarius]</td>\n",
       "      <td>[male, 34, Education, Aquarius]</td>\n",
       "      <td>[male, 34, Education, Aquarius]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Labels</th>\n",
       "      <td>(female, 17, 23, Cancer, 26, Scorpio)</td>\n",
       "      <td>(male, Cancer, 37, 26, 35, Technology)</td>\n",
       "      <td>(male, Aries, 27, Science, 36, Fashion)</td>\n",
       "      <td>(male, Aquarius, Scorpio, Education, 34, 42)</td>\n",
       "      <td>(male, Aquarius, 37, Education, Science, 34)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          19725  \\\n",
       "True Labels       [female, 35, LawEnforcement-Security, Cancer]   \n",
       "Predicted Labels          (female, 17, 23, Cancer, 26, Scorpio)   \n",
       "\n",
       "                                                   34039  \\\n",
       "True Labels                    [male, 36, indUnk, Virgo]   \n",
       "Predicted Labels  (male, Cancer, 37, 26, 35, Technology)   \n",
       "\n",
       "                                                    32655  \\\n",
       "True Labels                [male, 26, Military, Aquarius]   \n",
       "Predicted Labels  (male, Aries, 27, Science, 36, Fashion)   \n",
       "\n",
       "                                                         44124  \\\n",
       "True Labels                    [male, 34, Education, Aquarius]   \n",
       "Predicted Labels  (male, Aquarius, Scorpio, Education, 34, 42)   \n",
       "\n",
       "                                                         44032  \n",
       "True Labels                    [male, 34, Education, Aquarius]  \n",
       "Predicted Labels  (male, Aquarius, 37, Education, Science, 34)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Print sample predictions\n",
    "\n",
    "sample_df = pd.concat([features_test,labels_test],axis=1).sample(5)\n",
    "sample_labels = sample_df[\"labels\"]\n",
    "sample_features = sample_df[\"text\"]\n",
    "sample_features_dtm = vect.transform(sample_features)\n",
    "sample_pred_proba = clf_lr.predict_proba(sample_features_dtm)\n",
    "sample_pred = custom_prob(sample_pred_proba)\n",
    "sample_pred_labels = mlb.inverse_transform(sample_pred)\n",
    "sample_results = pd.DataFrame(columns=[\"True Labels\",\"Predicted Labels\"])\n",
    "sample_results[\"True Labels\"] = sample_labels\n",
    "sample_results[\"Predicted Labels\"] = sample_pred_labels\n",
    "print(\"Sample Predictions\") #only in local notebook use display to get a pretty view\n",
    "display(sample_results.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall increased as expected because of the class weights. The accuracy is still low because this did affect the precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 7: SGD Classifier using CountVectorizer with parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape (37844, 1246)\n",
      "Length of vocab is 1246\n",
      "Test features shape (9462, 1246)\n",
      "Classification Metrics and Scores\n",
      "Accuracy Score 0.008771929824561403\n",
      "Precision (macro) :  0.257\n",
      "Precision (micro) :  0.420\n",
      "Precision (weighted) :  0.394\n",
      "Recall (macro) :  0.123\n",
      "Recall (micro) :  0.239\n",
      "Recall (weighted) :  0.239\n",
      "F1 Score (macro) :  0.153\n",
      "F1 Score (micro) :  0.305\n",
      "F1 Score (weighted) :  0.270\n",
      "Hamming Loss 0.054519393362925384\n",
      "Classification Report\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   male       0.62      0.59      0.60      4927\n",
      "                     15       0.46      0.03      0.05       662\n",
      "                Student       0.42      0.28      0.33      1979\n",
      "                    Leo       0.23      0.01      0.02       700\n",
      "                     33       0.28      0.11      0.16       325\n",
      "      InvestmentBanking       0.31      0.21      0.25        19\n",
      "               Aquarius       0.58      0.18      0.27       891\n",
      "                 female       0.62      0.58      0.60      4535\n",
      "                     14       0.24      0.05      0.09       397\n",
      "                 indUnk       0.51      0.17      0.25      3272\n",
      "                  Aries       0.29      0.01      0.03      1432\n",
      "                     25       0.20      0.07      0.11       565\n",
      "              Capricorn       0.56      0.03      0.06       710\n",
      "                     17       0.17      0.28      0.21      1266\n",
      "                 Gemini       0.16      0.08      0.10       470\n",
      "                     23       0.35      0.04      0.08      1026\n",
      "             Non-Profit       0.10      0.05      0.06       111\n",
      "                 Cancer       0.30      0.05      0.08       880\n",
      "                Banking       0.21      0.07      0.11        57\n",
      "                     37       0.02      0.02      0.02        61\n",
      "            Sagittarius       0.20      0.12      0.15       922\n",
      "                     26       0.15      0.08      0.11       544\n",
      "                     24       0.35      0.09      0.14      1097\n",
      "                Scorpio       0.12      0.04      0.06       612\n",
      "                     27       0.20      0.17      0.19       779\n",
      "              Education       0.33      0.32      0.33       526\n",
      "                     45       0.00      0.00      0.00        16\n",
      "            Engineering       0.33      0.24      0.28       271\n",
      "                  Libra       0.32      0.22      0.26       835\n",
      "                Science       0.19      0.19      0.19       134\n",
      "                     34       0.59      0.36      0.45       362\n",
      "                     41       0.23      0.15      0.18        88\n",
      "   Communications-Media       0.06      0.04      0.05       278\n",
      "       BusinessServices       0.32      0.18      0.23        72\n",
      "      Sports-Recreation       0.11      0.05      0.06        22\n",
      "                  Virgo       0.16      0.04      0.06       564\n",
      "                 Taurus       0.17      0.08      0.11       634\n",
      "                   Arts       0.15      0.05      0.07       364\n",
      "                 Pisces       0.16      0.10      0.13       812\n",
      "                     44       0.00      0.00      0.00         7\n",
      "                     16       0.26      0.13      0.17       784\n",
      "               Internet       0.14      0.04      0.06       240\n",
      "      Museums-Libraries       0.42      0.15      0.22        65\n",
      "             Accounting       0.36      0.19      0.25        67\n",
      "                     39       0.14      0.14      0.14        80\n",
      "                     35       0.45      0.03      0.06       633\n",
      "             Technology       0.33      0.05      0.08       835\n",
      "                     36       0.48      0.03      0.06       391\n",
      "                    Law       0.17      0.13      0.15        67\n",
      "                     46       0.26      0.12      0.16        60\n",
      "             Consulting       0.21      0.15      0.18        39\n",
      "             Automotive       0.37      0.38      0.38        26\n",
      "                     42       0.25      0.12      0.17         8\n",
      "               Religion       0.19      0.13      0.15        47\n",
      "                     13       0.13      0.17      0.15       135\n",
      "                Fashion       0.38      0.06      0.11       371\n",
      "                     38       0.00      0.00      0.00        39\n",
      "                     43       0.47      0.30      0.36        27\n",
      "             Publishing       0.00      0.00      0.00        35\n",
      "                     40       0.20      0.02      0.04        45\n",
      "              Marketing       0.46      0.14      0.21        81\n",
      "LawEnforcement-Security       0.20      0.08      0.12        24\n",
      "         HumanResources       0.17      0.05      0.07        21\n",
      "     Telecommunications       0.00      0.00      0.00         4\n",
      "               Military       0.36      0.09      0.14        45\n",
      "             Government       0.07      0.04      0.05       114\n",
      "         Transportation       0.05      0.06      0.05        34\n",
      "           Architecture       0.17      0.12      0.14        16\n",
      "            Advertising       0.33      0.21      0.26        56\n",
      "                     47       0.29      0.14      0.19        37\n",
      "            Agriculture       0.00      0.00      0.00         9\n",
      "                Biotech       0.38      0.29      0.32        21\n",
      "             RealEstate       0.00      0.00      0.00         4\n",
      "          Manufacturing       0.41      0.20      0.27        85\n",
      "                     48       0.30      0.11      0.16        28\n",
      "           Construction       0.00      0.00      0.00         7\n",
      "              Chemicals       0.40      0.12      0.18        17\n",
      "               Maritime       0.33      0.17      0.22        12\n",
      "                Tourism       0.67      0.27      0.38        15\n",
      "            Environment       0.00      0.00      0.00         0\n",
      "\n",
      "              micro avg       0.42      0.24      0.30     37848\n",
      "              macro avg       0.26      0.12      0.15     37848\n",
      "           weighted avg       0.39      0.24      0.27     37848\n",
      "            samples avg       0.44      0.24      0.28     37848\n",
      "\n",
      "Sample Predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Labels</th>\n",
       "      <th>Predicted Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4790</th>\n",
       "      <td>[male, 26, indUnk, Gemini]</td>\n",
       "      <td>(male,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21181</th>\n",
       "      <td>[male, 17, Student, Capricorn]</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49108</th>\n",
       "      <td>[male, 23, Accounting, Cancer]</td>\n",
       "      <td>(male, indUnk, Aries)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42231</th>\n",
       "      <td>[female, 24, indUnk, Libra]</td>\n",
       "      <td>(male,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21893</th>\n",
       "      <td>[male, 16, Student, Aquarius]</td>\n",
       "      <td>(Student, Aquarius, female, Sagittarius, Tauru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          True Labels  \\\n",
       "4790       [male, 26, indUnk, Gemini]   \n",
       "21181  [male, 17, Student, Capricorn]   \n",
       "49108  [male, 23, Accounting, Cancer]   \n",
       "42231     [female, 24, indUnk, Libra]   \n",
       "21893   [male, 16, Student, Aquarius]   \n",
       "\n",
       "                                        Predicted Labels  \n",
       "4790                                             (male,)  \n",
       "21181                                                 ()  \n",
       "49108                              (male, indUnk, Aries)  \n",
       "42231                                            (male,)  \n",
       "21893  (Student, Aquarius, female, Sagittarius, Tauru...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test the models with various combinations\n",
    "#Check with LR as the base estimator\n",
    "\n",
    "#Lets change some parameters of CV\n",
    "cv = CountVectorizer(ngram_range=(1,2),min_df=0.01,max_df=0.95,stop_words=\"english\")\n",
    "base_est = SGDClassifier()\n",
    "test_model(features_train,features_test,labels_train_enc,labels_test_enc,\n",
    "           vect=cv,est=base_est,\n",
    "           sample_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Did not see any difference in the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 8: SVC using CountVectorizer with various parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###DO NOT RUN THIS TIME. TOO MANY RECORDS. WILL TAKE FOR EVER###\n",
    "#Test the models with various combinations\n",
    "#Check with LR as the base estimator\n",
    "\n",
    "#Lets change some parameters of CV\n",
    "#cv = CountVectorizer(ngram_range=(1,2),min_df=0.01,max_df=0.95,stop_words=\"english\")\n",
    "#base_est = SVC()\n",
    "#test_model(features_train,features_test,labels_train_enc,labels_test_enc,\n",
    "#           vect=cv,est=base_est,\n",
    "#           sample_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC with these many records takes a lot of time. Previous runs did not produce better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 9: Logistic Regression using TFIDF Vectorizer with various parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try TfIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape (37844, 10000)\n",
      "Length of vocab is 10000\n",
      "Test features shape (9462, 10000)\n",
      "Classification Metrics and Scores\n",
      "Accuracy Score 0.07038681039949271\n",
      "Precision (macro) :  0.492\n",
      "Precision (micro) :  0.758\n",
      "Precision (weighted) :  0.794\n",
      "Recall (macro) :  0.073\n",
      "Recall (micro) :  0.291\n",
      "Recall (weighted) :  0.291\n",
      "F1 Score (macro) :  0.110\n",
      "F1 Score (micro) :  0.420\n",
      "F1 Score (weighted) :  0.351\n",
      "Hamming Loss 0.04010383639822448\n",
      "Classification Report\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   male       0.74      0.74      0.74      4927\n",
      "                     15       0.90      0.09      0.16       662\n",
      "                Student       0.75      0.26      0.39      1979\n",
      "                    Leo       1.00      0.02      0.04       700\n",
      "                     33       0.00      0.00      0.00       325\n",
      "      InvestmentBanking       0.00      0.00      0.00        19\n",
      "               Aquarius       0.94      0.19      0.32       891\n",
      "                 female       0.72      0.72      0.72      4535\n",
      "                     14       0.88      0.07      0.13       397\n",
      "                 indUnk       0.68      0.37      0.48      3272\n",
      "                  Aries       0.91      0.19      0.31      1432\n",
      "                     25       0.75      0.01      0.01       565\n",
      "              Capricorn       0.97      0.14      0.24       710\n",
      "                     17       0.81      0.18      0.29      1266\n",
      "                 Gemini       1.00      0.04      0.09       470\n",
      "                     23       0.94      0.09      0.16      1026\n",
      "             Non-Profit       0.00      0.00      0.00       111\n",
      "                 Cancer       0.96      0.05      0.10       880\n",
      "                Banking       0.00      0.00      0.00        57\n",
      "                     37       0.00      0.00      0.00        61\n",
      "            Sagittarius       0.97      0.13      0.23       922\n",
      "                     26       1.00      0.02      0.04       544\n",
      "                     24       0.95      0.16      0.27      1097\n",
      "                Scorpio       0.89      0.01      0.03       612\n",
      "                     27       0.93      0.08      0.15       779\n",
      "              Education       0.95      0.23      0.37       526\n",
      "                     45       0.00      0.00      0.00        16\n",
      "            Engineering       1.00      0.10      0.18       271\n",
      "                  Libra       0.99      0.18      0.30       835\n",
      "                Science       1.00      0.01      0.03       134\n",
      "                     34       0.98      0.41      0.58       362\n",
      "                     41       0.75      0.07      0.12        88\n",
      "   Communications-Media       0.00      0.00      0.00       278\n",
      "       BusinessServices       1.00      0.01      0.03        72\n",
      "      Sports-Recreation       0.00      0.00      0.00        22\n",
      "                  Virgo       0.75      0.01      0.01       564\n",
      "                 Taurus       0.88      0.01      0.02       634\n",
      "                   Arts       0.00      0.00      0.00       364\n",
      "                 Pisces       0.96      0.11      0.19       812\n",
      "                     44       0.00      0.00      0.00         7\n",
      "                     16       0.92      0.12      0.22       784\n",
      "               Internet       1.00      0.02      0.03       240\n",
      "      Museums-Libraries       0.89      0.12      0.22        65\n",
      "             Accounting       0.00      0.00      0.00        67\n",
      "                     39       1.00      0.06      0.12        80\n",
      "                     35       0.86      0.08      0.14       633\n",
      "             Technology       0.82      0.09      0.16       835\n",
      "                     36       0.98      0.21      0.35       391\n",
      "                    Law       0.00      0.00      0.00        67\n",
      "                     46       0.00      0.00      0.00        60\n",
      "             Consulting       0.00      0.00      0.00        39\n",
      "             Automotive       0.00      0.00      0.00        26\n",
      "                     42       0.00      0.00      0.00         8\n",
      "               Religion       0.00      0.00      0.00        47\n",
      "                     13       1.00      0.02      0.04       135\n",
      "                Fashion       0.96      0.22      0.36       371\n",
      "                     38       0.00      0.00      0.00        39\n",
      "                     43       0.00      0.00      0.00        27\n",
      "             Publishing       0.00      0.00      0.00        35\n",
      "                     40       0.00      0.00      0.00        45\n",
      "              Marketing       1.00      0.05      0.09        81\n",
      "LawEnforcement-Security       0.00      0.00      0.00        24\n",
      "         HumanResources       0.00      0.00      0.00        21\n",
      "     Telecommunications       0.00      0.00      0.00         4\n",
      "               Military       0.00      0.00      0.00        45\n",
      "             Government       1.00      0.01      0.02       114\n",
      "         Transportation       0.00      0.00      0.00        34\n",
      "           Architecture       0.00      0.00      0.00        16\n",
      "            Advertising       1.00      0.11      0.19        56\n",
      "                     47       0.00      0.00      0.00        37\n",
      "            Agriculture       0.00      0.00      0.00         9\n",
      "                Biotech       0.00      0.00      0.00        21\n",
      "             RealEstate       0.00      0.00      0.00         4\n",
      "          Manufacturing       1.00      0.06      0.11        85\n",
      "                     48       0.00      0.00      0.00        28\n",
      "           Construction       0.00      0.00      0.00         7\n",
      "              Chemicals       0.00      0.00      0.00        17\n",
      "               Maritime       0.00      0.00      0.00        12\n",
      "                Tourism       0.00      0.00      0.00        15\n",
      "            Environment       0.00      0.00      0.00         0\n",
      "\n",
      "              micro avg       0.76      0.29      0.42     37848\n",
      "              macro avg       0.49      0.07      0.11     37848\n",
      "           weighted avg       0.79      0.29      0.35     37848\n",
      "            samples avg       0.72      0.29      0.39     37848\n",
      "\n",
      "Sample Predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Labels</th>\n",
       "      <th>Predicted Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7564</th>\n",
       "      <td>[male, 36, Fashion, Aries]</td>\n",
       "      <td>(female,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>[female, 27, Transportation, Taurus]</td>\n",
       "      <td>(female,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20782</th>\n",
       "      <td>[female, 16, indUnk, Scorpio]</td>\n",
       "      <td>(Student, female, indUnk, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5085</th>\n",
       "      <td>[female, 17, indUnk, Scorpio]</td>\n",
       "      <td>(female,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31789</th>\n",
       "      <td>[female, 35, indUnk, Libra]</td>\n",
       "      <td>(female, indUnk)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                True Labels               Predicted Labels\n",
       "7564             [male, 36, Fashion, Aries]                      (female,)\n",
       "48840  [female, 27, Transportation, Taurus]                      (female,)\n",
       "20782         [female, 16, indUnk, Scorpio]  (Student, female, indUnk, 16)\n",
       "5085          [female, 17, indUnk, Scorpio]                      (female,)\n",
       "31789           [female, 35, indUnk, Libra]               (female, indUnk)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets change some parameters of CV\n",
    "cv = TfidfVectorizer(ngram_range=(1,3),max_features=10000)\n",
    "\n",
    "test_model(features_train,features_test,labels_train_enc,labels_test_enc,\n",
    "           vect=cv,\n",
    "           est=LogisticRegression(),\n",
    "           sample_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use of TFIFDF did not change the peformance and in some settings was worse than the model with countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 10: Neural NW model with various settings for Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the number of features\n",
    "vect = CountVectorizer(ngram_range=(1,3),max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape (37844, 10000)\n",
      "Length of vocab is 10000\n",
      "Test features shape (9462, 10000)\n"
     ]
    }
   ],
   "source": [
    "#Fit and transform the word vectorizer on training features\n",
    "X_train_dtm = vect.fit_transform(features_train)\n",
    " \n",
    "print(\"Training features shape\",X_train_dtm.shape)\n",
    "  \n",
    "print(\"Length of vocab is\",len(vect.vocabulary_))\n",
    "    \n",
    "#Transform test features\n",
    "   \n",
    "X_test_dtm = vect.transform(features_test)\n",
    "  \n",
    "print(\"Test features shape\",X_test_dtm.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple DNN\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "#Add hidden layers\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu', input_shape=(len(vect.vocabulary_),)))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "#Add Output layer with # of classes equal to no of classes\n",
    "model.add(tf.keras.layers.Dense(len(class_names), activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37844 samples, validate on 9462 samples\n",
      "Epoch 1/10\n",
      "37844/37844 [==============================] - 21s 558us/sample - loss: 0.1736 - acc: 0.9460 - val_loss: 0.1350 - val_acc: 0.9558\n",
      "Epoch 2/10\n",
      "37844/37844 [==============================] - 18s 482us/sample - loss: 0.1335 - acc: 0.9569 - val_loss: 0.1267 - val_acc: 0.9579\n",
      "Epoch 3/10\n",
      "37844/37844 [==============================] - 18s 465us/sample - loss: 0.1233 - acc: 0.9599 - val_loss: 0.1227 - val_acc: 0.9594\n",
      "Epoch 4/10\n",
      "37844/37844 [==============================] - 19s 496us/sample - loss: 0.1164 - acc: 0.9622 - val_loss: 0.1210 - val_acc: 0.9601\n",
      "Epoch 5/10\n",
      "37844/37844 [==============================] - 18s 471us/sample - loss: 0.1105 - acc: 0.9642 - val_loss: 0.1210 - val_acc: 0.9605\n",
      "Epoch 6/10\n",
      "37844/37844 [==============================] - 18s 472us/sample - loss: 0.1063 - acc: 0.9657 - val_loss: 0.1208 - val_acc: 0.9606\n",
      "Epoch 7/10\n",
      "37844/37844 [==============================] - 17s 461us/sample - loss: 0.1022 - acc: 0.9669 - val_loss: 0.1216 - val_acc: 0.9609\n",
      "Epoch 8/10\n",
      "37844/37844 [==============================] - 18s 467us/sample - loss: 0.0992 - acc: 0.9678 - val_loss: 0.1227 - val_acc: 0.9608\n",
      "Epoch 9/10\n",
      "37844/37844 [==============================] - 17s 460us/sample - loss: 0.0965 - acc: 0.9687 - val_loss: 0.1233 - val_acc: 0.9609\n",
      "Epoch 10/10\n",
      "37844/37844 [==============================] - 18s 476us/sample - loss: 0.0945 - acc: 0.9692 - val_loss: 0.1243 - val_acc: 0.9607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e6805ef208>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(X_train_dtm.todense(), labels_train_enc,\n",
    "           validation_data=(X_test_dtm.todense(), labels_test_enc), \n",
    "           epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test\n",
    "y_pred = model.predict(X_test_dtm.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 3.09944153e-06, 9.99905944e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 8.94069672e-08, 1.11311674e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.99982297e-01, 1.56459212e-03, 6.71076775e-03, ...,\n",
       "        4.96745110e-04, 1.22785568e-05, 0.00000000e+00],\n",
       "       ...,\n",
       "       [2.07062364e-02, 5.35411239e-02, 5.67748249e-02, ...,\n",
       "        1.15633011e-05, 2.33381987e-04, 8.55326653e-06],\n",
       "       [3.24921310e-01, 1.01002991e-01, 7.72858262e-01, ...,\n",
       "        1.57952309e-06, 6.33597374e-05, 7.15255737e-07],\n",
       "       [5.00887156e-01, 9.20909643e-03, 3.48896980e-02, ...,\n",
       "        1.84777379e-03, 5.43734431e-03, 2.41637230e-04]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to convert probablity to predictions\n",
    "def conv_prob_pred(y_pred,threshold=0.5):\n",
    "    for row in y_pred:\n",
    "        for i in range(len(row)):\n",
    "            if row[i] >= 0.5:\n",
    "                row[i] = 1\n",
    "            else:\n",
    "                row[i] =0\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to predictions\n",
    "y_pred = conv_prob_pred(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics and Scores\n",
      "Accuracy Score [0.12427369678714668, 0.9607194]\n",
      "Precision (macro) :  0.387\n",
      "Precision (micro) :  0.728\n",
      "Precision (weighted) :  0.691\n",
      "Recall (macro) :  0.103\n",
      "Recall (micro) :  0.342\n",
      "Recall (weighted) :  0.342\n",
      "F1 Score (macro) :  0.143\n",
      "F1 Score (micro) :  0.466\n",
      "F1 Score (weighted) :  0.403\n",
      "Classification Report\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   male       0.73      0.75      0.74      4927\n",
      "                     15       0.90      0.06      0.11       662\n",
      "                Student       0.62      0.40      0.49      1979\n",
      "                    Leo       1.00      0.00      0.01       700\n",
      "                     33       0.00      0.00      0.00       325\n",
      "      InvestmentBanking       0.00      0.00      0.00        19\n",
      "               Aquarius       0.94      0.25      0.39       891\n",
      "                 female       0.72      0.70      0.71      4535\n",
      "                     14       0.67      0.02      0.03       397\n",
      "                 indUnk       0.62      0.44      0.51      3272\n",
      "                  Aries       0.71      0.35      0.47      1432\n",
      "                     25       1.00      0.01      0.01       565\n",
      "              Capricorn       0.91      0.19      0.31       710\n",
      "                     17       0.75      0.28      0.41      1266\n",
      "                 Gemini       0.00      0.00      0.00       470\n",
      "                     23       0.87      0.15      0.25      1026\n",
      "             Non-Profit       0.00      0.00      0.00       111\n",
      "                 Cancer       0.95      0.07      0.13       880\n",
      "                Banking       0.00      0.00      0.00        57\n",
      "                     37       0.00      0.00      0.00        61\n",
      "            Sagittarius       0.90      0.20      0.33       922\n",
      "                     26       0.90      0.02      0.03       544\n",
      "                     24       0.73      0.31      0.43      1097\n",
      "                Scorpio       0.00      0.00      0.00       612\n",
      "                     27       0.91      0.12      0.22       779\n",
      "              Education       0.90      0.29      0.44       526\n",
      "                     45       0.00      0.00      0.00        16\n",
      "            Engineering       0.99      0.29      0.45       271\n",
      "                  Libra       0.91      0.29      0.44       835\n",
      "                Science       0.86      0.04      0.09       134\n",
      "                     34       0.93      0.59      0.72       362\n",
      "                     41       1.00      0.06      0.11        88\n",
      "   Communications-Media       0.00      0.00      0.00       278\n",
      "       BusinessServices       0.00      0.00      0.00        72\n",
      "      Sports-Recreation       0.00      0.00      0.00        22\n",
      "                  Virgo       0.00      0.00      0.00       564\n",
      "                 Taurus       0.67      0.00      0.01       634\n",
      "                   Arts       0.00      0.00      0.00       364\n",
      "                 Pisces       0.93      0.15      0.26       812\n",
      "                     44       0.00      0.00      0.00         7\n",
      "                     16       0.84      0.18      0.29       784\n",
      "               Internet       0.00      0.00      0.00       240\n",
      "      Museums-Libraries       1.00      0.17      0.29        65\n",
      "             Accounting       0.00      0.00      0.00        67\n",
      "                     39       0.00      0.00      0.00        80\n",
      "                     35       0.66      0.29      0.40       633\n",
      "             Technology       0.61      0.26      0.36       835\n",
      "                     36       0.91      0.43      0.58       391\n",
      "                    Law       1.00      0.01      0.03        67\n",
      "                     46       0.00      0.00      0.00        60\n",
      "             Consulting       0.00      0.00      0.00        39\n",
      "             Automotive       0.00      0.00      0.00        26\n",
      "                     42       0.00      0.00      0.00         8\n",
      "               Religion       0.00      0.00      0.00        47\n",
      "                     13       0.00      0.00      0.00       135\n",
      "                Fashion       0.91      0.43      0.59       371\n",
      "                     38       0.00      0.00      0.00        39\n",
      "                     43       0.00      0.00      0.00        27\n",
      "             Publishing       0.00      0.00      0.00        35\n",
      "                     40       0.00      0.00      0.00        45\n",
      "              Marketing       1.00      0.14      0.24        81\n",
      "LawEnforcement-Security       0.00      0.00      0.00        24\n",
      "         HumanResources       0.00      0.00      0.00        21\n",
      "     Telecommunications       0.00      0.00      0.00         4\n",
      "               Military       0.00      0.00      0.00        45\n",
      "             Government       0.00      0.00      0.00       114\n",
      "         Transportation       0.00      0.00      0.00        34\n",
      "           Architecture       0.00      0.00      0.00        16\n",
      "            Advertising       1.00      0.14      0.25        56\n",
      "                     47       0.00      0.00      0.00        37\n",
      "            Agriculture       0.00      0.00      0.00         9\n",
      "                Biotech       0.00      0.00      0.00        21\n",
      "             RealEstate       0.00      0.00      0.00         4\n",
      "          Manufacturing       1.00      0.16      0.28        85\n",
      "                     48       0.00      0.00      0.00        28\n",
      "           Construction       0.00      0.00      0.00         7\n",
      "              Chemicals       0.00      0.00      0.00        17\n",
      "               Maritime       0.00      0.00      0.00        12\n",
      "                Tourism       0.00      0.00      0.00        15\n",
      "            Environment       0.00      0.00      0.00         0\n",
      "\n",
      "              micro avg       0.73      0.34      0.47     37848\n",
      "              macro avg       0.39      0.10      0.14     37848\n",
      "           weighted avg       0.69      0.34      0.40     37848\n",
      "            samples avg       0.69      0.34      0.43     37848\n",
      "\n",
      "Sample Predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Labels</th>\n",
       "      <th>Predicted Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9328</th>\n",
       "      <td>[female, 14, Student, Cancer]</td>\n",
       "      <td>(female, indUnk)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34249</th>\n",
       "      <td>[female, 23, Student, Pisces]</td>\n",
       "      <td>(Student, female, 23, Pisces)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13506</th>\n",
       "      <td>[female, 24, Student, Cancer]</td>\n",
       "      <td>(male, Aries)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21411</th>\n",
       "      <td>[male, 13, Student, Scorpio]</td>\n",
       "      <td>(male, indUnk)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>[male, 24, Engineering, Libra]</td>\n",
       "      <td>(male,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          True Labels               Predicted Labels\n",
       "9328    [female, 14, Student, Cancer]               (female, indUnk)\n",
       "34249   [female, 23, Student, Pisces]  (Student, female, 23, Pisces)\n",
       "13506   [female, 24, Student, Cancer]                  (male, Aries)\n",
       "21411    [male, 13, Student, Scorpio]                 (male, indUnk)\n",
       "677    [male, 24, Engineering, Libra]                        (male,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Metrics\n",
    "\n",
    "print(\"Classification Metrics and Scores\")\n",
    "print(\"Accuracy Score\",model.evaluate(X_test_dtm.todense(),labels_test_enc,verbose=0))\n",
    "#Get Precison scores - again using the tree settings for averaging\n",
    "avg = [\"macro\",\"micro\",\"weighted\"]\n",
    "for a in avg:\n",
    "    print(f'Precision ({a}) : {precision_score(labels_test_enc,y_pred,average=a,zero_division=0): 0.3f}')\n",
    "        \n",
    "for a in avg:   \n",
    "    print(f'Recall ({a}) : {recall_score(labels_test_enc,y_pred,average=a,zero_division=0): 0.3f}')\n",
    "        \n",
    "for a in avg:\n",
    "    print(f'F1 Score ({a}) : {f1_score(labels_test_enc,y_pred,average=a): 0.3f}')\n",
    "        \n",
    "print(\"Classification Report\")\n",
    "print(classification_report(labels_test_enc,y_pred,zero_division=0,target_names=class_names))\n",
    "\n",
    "sample_df = pd.concat([features_test,labels_test],axis=1).sample(5)\n",
    "sample_labels = sample_df[\"labels\"]\n",
    "sample_features = sample_df[\"text\"]\n",
    "sample_features_dtm = vect.transform(sample_features)\n",
    "sample_pred = model.predict(sample_features_dtm.todense())\n",
    "sample_pred = conv_prob_pred(sample_pred)\n",
    "sample_pred_labels = mlb.inverse_transform(sample_pred)\n",
    "sample_results = pd.DataFrame(columns=[\"True Labels\",\"Predicted Labels\"])\n",
    "sample_results[\"True Labels\"] = sample_labels\n",
    "sample_results[\"Predicted Labels\"] = sample_pred_labels\n",
    "print(\"Sample Predictions\") #only in local notebook use display to get a pretty view\n",
    "display(sample_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can see that while the overall accuracy went high...the recall is very poor \n",
    "##leading to same end result as other models when predicting for labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "The model performance improvements were seen most while changing the ngrams and max_features settings as part of Vectorization. \n",
    "Logistic Regression performed better than other models (while NN did improve the overall precision and accuracy)\n",
    "More exhaustive text preprocessing (spelling, NER) could probably improve the performance (and even interpretability)\n",
    "Handling class imbalance also could improve recall values of the model\n",
    "For future work - also include the confidence levels of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
