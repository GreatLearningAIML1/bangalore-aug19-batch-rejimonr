{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FU-HwvIdH0M-"
   },
   "source": [
    "## Sentiment analysis <br> \n",
    "\n",
    "The objective of this problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n",
    "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAQDiZHRH0M_"
   },
   "source": [
    "### 1. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eXGIe-SH0NA"
   },
   "outputs": [],
   "source": [
    "#Import required standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWeWe1eJH0NF"
   },
   "outputs": [],
   "source": [
    "#Load the data from the local copy of the dataset into a pandas dataframe\n",
    "#Also drop any rows with na values as it will not be useful\n",
    "review_df = pd.read_csv(\"tweets.csv\",encoding=\"unicode-escape\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8856</th>\n",
       "      <td>If there was ever any doubt on the influence o...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>@mention @mention @mention &amp;quot;@mention Goog...</td>\n",
       "      <td>Other Google product or service</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8683</th>\n",
       "      <td>I picked up a &amp;quot;mophie&amp;quot; iPhone charge...</td>\n",
       "      <td>Other Apple product or service</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5125</th>\n",
       "      <td>RT @mention @mention Google is looking at soci...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>There's something 2 b said that here at #SXSW,...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "8856  If there was ever any doubt on the influence o...   \n",
       "1544  @mention @mention @mention &quot;@mention Goog...   \n",
       "8683  I picked up a &quot;mophie&quot; iPhone charge...   \n",
       "5125  RT @mention @mention Google is looking at soci...   \n",
       "5020  There's something 2 b said that here at #SXSW,...   \n",
       "\n",
       "      emotion_in_tweet_is_directed_at  \\\n",
       "8856                            Apple   \n",
       "1544  Other Google product or service   \n",
       "8683   Other Apple product or service   \n",
       "5125                           Google   \n",
       "5020                             iPad   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "8856                                   Positive emotion  \n",
       "1544                                   Positive emotion  \n",
       "8683                                   Positive emotion  \n",
       "5125                                   Positive emotion  \n",
       "5020                                   Positive emotion  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity\n",
    "review_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3291 entries, 0 to 9088\n",
      "Data columns (total 3 columns):\n",
      "tweet_text                                            3291 non-null object\n",
      "emotion_in_tweet_is_directed_at                       3291 non-null object\n",
      "is_there_an_emotion_directed_at_a_brand_or_product    3291 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 102.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#Check the # of records and information of the dataframe\n",
    "review_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3291, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPJvTjefH0NI"
   },
   "source": [
    "### 2. Preprocess the text and add the preprocessed text in a column with name `text` in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iec5s9gH0NI"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    try:\n",
    "        return ''.join(i if ord(i)<128 else ' ' for i in text)\n",
    "    except Exception as e:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQSmqA-vH0NT"
   },
   "outputs": [],
   "source": [
    "review_df['text'] = [preprocess(text) for text in review_df.tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text    ÛÏ@mention &quot;Google before you tweet&quot...\n",
       "text             @mention &quot;Google before you tweet&quot...\n",
       "Name: 106, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.iloc[55][[\"tweet_text\",\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGWB3P2WH0NY"
   },
   "source": [
    "### 3. Consider only rows having a Positive or Negative emotion and remove other rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdgA_8N2H0NY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'Positive emotion',\n",
       "       'No emotion toward brand or product', \"I can't tell\"], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.is_there_an_emotion_directed_at_a_brand_or_product.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Jlu-reIH0Na"
   },
   "outputs": [],
   "source": [
    "retained_emotions = [\"Negative emotion\",\"Positive emotion\"]\n",
    "\n",
    "review_df = review_df[review_df.is_there_an_emotion_directed_at_a_brand_or_product.isin(retained_emotions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'Positive emotion'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity\n",
    "review_df.is_there_an_emotion_directed_at_a_brand_or_product.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retain only the text and emotions\n",
    "re_df = review_df[[\"text\",\"is_there_an_emotion_directed_at_a_brand_or_product\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_df = re_df.rename(columns={\"is_there_an_emotion_directed_at_a_brand_or_product\" : \"emotions\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3191 entries, 0 to 9088\n",
      "Data columns (total 2 columns):\n",
      "text        3191 non-null object\n",
      "emotions    3191 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 74.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#Sanity\n",
    "re_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SotCRvkDH0Nf"
   },
   "source": [
    "### 4. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcbkY4sgH0Ng"
   },
   "outputs": [],
   "source": [
    "#Import required modules for tokenization\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyXtZGr-H0Nl"
   },
   "outputs": [],
   "source": [
    "#Initialize the CV\n",
    "cvect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit CV on text\n",
    "cvect.fit(re_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the text\n",
    "text_dtm = cvect.transform(re_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4LUM-XPH0Nn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 79)\t1\n",
      "  (0, 225)\t1\n",
      "  (0, 415)\t2\n",
      "  (0, 1287)\t1\n",
      "  (0, 2282)\t1\n",
      "  (0, 2422)\t1\n",
      "  (0, 2637)\t1\n",
      "  (0, 2659)\t1\n",
      "  (0, 3300)\t1\n",
      "  (0, 3702)\t1\n",
      "  (0, 4141)\t1\n",
      "  (0, 4616)\t1\n",
      "  (0, 4769)\t1\n",
      "  (0, 5011)\t1\n",
      "  (0, 5141)\t1\n",
      "  (0, 5227)\t1\n",
      "  (0, 5370)\t1\n",
      "  (0, 5413)\t1\n",
      "  (1, 150)\t1\n",
      "  (1, 279)\t1\n",
      "  (1, 345)\t1\n",
      "  (1, 365)\t1\n",
      "  (1, 415)\t1\n",
      "  (1, 472)\t1\n",
      "  (1, 1348)\t1\n",
      "  :\t:\n",
      "  (3189, 284)\t1\n",
      "  (3189, 300)\t2\n",
      "  (3189, 345)\t1\n",
      "  (3189, 795)\t1\n",
      "  (3189, 797)\t1\n",
      "  (3189, 1813)\t1\n",
      "  (3189, 1933)\t2\n",
      "  (3189, 2271)\t1\n",
      "  (3189, 2482)\t1\n",
      "  (3189, 2627)\t1\n",
      "  (3189, 2637)\t1\n",
      "  (3189, 2659)\t1\n",
      "  (3189, 3205)\t1\n",
      "  (3189, 3276)\t1\n",
      "  (3189, 4199)\t1\n",
      "  (3189, 4592)\t1\n",
      "  (3189, 4707)\t1\n",
      "  (3189, 4769)\t1\n",
      "  (3189, 4781)\t1\n",
      "  (3189, 5244)\t1\n",
      "  (3189, 5274)\t1\n",
      "  (3190, 1696)\t1\n",
      "  (3190, 2627)\t1\n",
      "  (3190, 2905)\t1\n",
      "  (3190, 4769)\t1\n"
     ]
    }
   ],
   "source": [
    "#Get the dtm now..it is a sparse matrix\n",
    "print(text_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIdZYxJtH0Nq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pxd5fSHH0Nt"
   },
   "source": [
    "### 5. Find number of different words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1DQ2LdNH0Nu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5610"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the vocabulary now\n",
    "len(cvect.vocabulary_)\n",
    "\n",
    "#There are 5610 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wesley83': 5413,\n",
       " 'have': 2282,\n",
       " '3g': 79,\n",
       " 'iphone': 2637,\n",
       " 'after': 225,\n",
       " 'hrs': 2422,\n",
       " 'tweeting': 5141,\n",
       " 'at': 415,\n",
       " 'rise_austin': 4141,\n",
       " 'it': 2659,\n",
       " 'was': 5370,\n",
       " 'dead': 1287,\n",
       " 'need': 3300,\n",
       " 'to': 5011,\n",
       " 'upgrade': 5227,\n",
       " 'plugin': 3702,\n",
       " 'stations': 4616,\n",
       " 'sxsw': 4769,\n",
       " 'jessedee': 2688,\n",
       " 'know': 2779,\n",
       " 'about': 150,\n",
       " 'fludapp': 1904,\n",
       " 'awesome': 472,\n",
       " 'ipad': 2627,\n",
       " 'app': 345,\n",
       " 'that': 4915,\n",
       " 'you': 5577,\n",
       " 'll': 2923,\n",
       " 'likely': 2893,\n",
       " 'appreciate': 365,\n",
       " 'for': 1933,\n",
       " 'its': 2661,\n",
       " 'design': 1348,\n",
       " 'also': 279,\n",
       " 'they': 4939,\n",
       " 're': 3956,\n",
       " 'giving': 2099,\n",
       " 'free': 1965,\n",
       " 'ts': 5110,\n",
       " 'swonderlin': 4760,\n",
       " 'can': 802,\n",
       " 'not': 3361,\n",
       " 'wait': 5342,\n",
       " 'should': 4375,\n",
       " 'sale': 4194,\n",
       " 'them': 4925,\n",
       " 'down': 1498,\n",
       " 'hope': 2395,\n",
       " 'this': 4953,\n",
       " 'year': 5559,\n",
       " 'festival': 1836,\n",
       " 'isn': 2654,\n",
       " 'as': 405,\n",
       " 'crashy': 1180,\n",
       " 'sxtxstate': 4794,\n",
       " 'great': 2175,\n",
       " 'stuff': 4681,\n",
       " 'on': 3434,\n",
       " 'fri': 1970,\n",
       " 'marissa': 3059,\n",
       " 'mayer': 3090,\n",
       " 'google': 2138,\n",
       " 'tim': 4986,\n",
       " 'reilly': 4028,\n",
       " 'tech': 4858,\n",
       " 'books': 657,\n",
       " 'conferences': 1063,\n",
       " 'amp': 297,\n",
       " 'matt': 3083,\n",
       " 'mullenweg': 3259,\n",
       " 'wordpress': 5500,\n",
       " 'is': 2651,\n",
       " 'just': 2723,\n",
       " 'starting': 4608,\n",
       " 'ctia': 1222,\n",
       " 'around': 390,\n",
       " 'the': 4916,\n",
       " 'corner': 1134,\n",
       " 'and': 303,\n",
       " 'googleio': 2146,\n",
       " 'only': 3439,\n",
       " 'hop': 2394,\n",
       " 'skip': 4436,\n",
       " 'jump': 2721,\n",
       " 'from': 1979,\n",
       " 'there': 4934,\n",
       " 'good': 2131,\n",
       " 'time': 4988,\n",
       " 'be': 549,\n",
       " 'an': 300,\n",
       " 'android': 307,\n",
       " 'fan': 1778,\n",
       " 'beautifully': 557,\n",
       " 'smart': 4464,\n",
       " 'simple': 4408,\n",
       " 'idea': 2456,\n",
       " 'rt': 4173,\n",
       " 'madebymany': 3013,\n",
       " 'thenextweb': 4930,\n",
       " 'wrote': 5533,\n",
       " 'our': 3484,\n",
       " 'hollergram': 2374,\n",
       " 'http': 2428,\n",
       " 'bit': 616,\n",
       " 'ly': 2996,\n",
       " 'ieavob': 2467,\n",
       " 'counting': 1149,\n",
       " 'days': 1285,\n",
       " 'plus': 3704,\n",
       " 'strong': 4671,\n",
       " 'canadian': 804,\n",
       " 'dollar': 1469,\n",
       " 'means': 3101,\n",
       " 'stock': 4636,\n",
       " 'up': 5220,\n",
       " 'apple': 355,\n",
       " 'gear': 2045,\n",
       " 'excited': 1710,\n",
       " 'meet': 3109,\n",
       " 'samsungmobileus': 4202,\n",
       " 'so': 4495,\n",
       " 'show': 4381,\n",
       " 'my': 3276,\n",
       " 'sprint': 4584,\n",
       " 'galaxy': 2024,\n",
       " 'still': 4634,\n",
       " 'running': 4184,\n",
       " 'fail': 1768,\n",
       " 'find': 1863,\n",
       " 'start': 4606,\n",
       " 'impromptu': 2506,\n",
       " 'parties': 3563,\n",
       " 'with': 5469,\n",
       " 'hurricaneparty': 2439,\n",
       " 'gvlrin': 2222,\n",
       " 'til': 4984,\n",
       " 'comes': 1021,\n",
       " 'out': 3486,\n",
       " 'foursquare': 1954,\n",
       " 'ups': 5231,\n",
       " 'game': 2026,\n",
       " 'in': 2515,\n",
       " 'mp': 3251,\n",
       " 'grn7pk': 2186,\n",
       " 'prefer': 3769,\n",
       " 'gowalla': 2157,\n",
       " 'by': 775,\n",
       " 'far': 1787,\n",
       " 'best': 589,\n",
       " 'looking': 2952,\n",
       " 'date': 1276,\n",
       " 'gotta': 2152,\n",
       " 'love': 2971,\n",
       " 'calendar': 785,\n",
       " 'featuring': 1816,\n",
       " 'top': 5032,\n",
       " 'cases': 831,\n",
       " 'check': 903,\n",
       " 'hamsandwich': 2244,\n",
       " 'via': 5292,\n",
       " 'ischafer': 2653,\n",
       " 'gt': 2204,\n",
       " 'axzwxb': 479,\n",
       " 'tinyurl': 4996,\n",
       " 'com': 1014,\n",
       " '4nqv92l': 95,\n",
       " 'haha': 2236,\n",
       " 'awesomely': 473,\n",
       " 'rad': 3926,\n",
       " 'htdfim': 2424,\n",
       " 'noticed': 3369,\n",
       " 'dst': 1531,\n",
       " 'coming': 1024,\n",
       " 'weekend': 5401,\n",
       " 'how': 2416,\n",
       " 'many': 3046,\n",
       " 'users': 5249,\n",
       " 'will': 5445,\n",
       " 'hour': 2412,\n",
       " 'late': 2815,\n",
       " 'come': 1018,\n",
       " 'sunday': 4715,\n",
       " 'morning': 3233,\n",
       " 'added': 192,\n",
       " 'flights': 1894,\n",
       " 'planely': 3675,\n",
       " 'matching': 3080,\n",
       " 'people': 3608,\n",
       " 'planes': 3676,\n",
       " 'airports': 254,\n",
       " 'downloaded': 1500,\n",
       " 'klm': 2773,\n",
       " 'nicely': 3337,\n",
       " 'done': 1479,\n",
       " 'must': 3272,\n",
       " 'malbonster': 3034,\n",
       " 'lovely': 2975,\n",
       " 'review': 4109,\n",
       " 'forbes': 1934,\n",
       " 'holler': 2373,\n",
       " 'gram': 2164,\n",
       " 'co': 992,\n",
       " 'g4gzypv': 2016,\n",
       " 'buy': 769,\n",
       " 'ipad2': 2629,\n",
       " 'while': 5423,\n",
       " 'austin': 441,\n",
       " 'sure': 4727,\n",
       " 'if': 2468,\n",
       " 'store': 4646,\n",
       " 'oh': 3421,\n",
       " 'god': 2114,\n",
       " 'pure': 3875,\n",
       " 'unadulterated': 5174,\n",
       " 'easier': 1564,\n",
       " 'browse': 732,\n",
       " 'events': 1684,\n",
       " 'than': 4910,\n",
       " 'website': 5396,\n",
       " 'okay': 3424,\n",
       " 'really': 3972,\n",
       " 'yay': 5555,\n",
       " 'new': 3321,\n",
       " '11': 15,\n",
       " 'kthxbai': 2786,\n",
       " 'photo': 3632,\n",
       " 'installed': 2573,\n",
       " 'which': 5422,\n",
       " 'nice': 3336,\n",
       " 'tumblr': 5117,\n",
       " 'x6t1pi6av7': 5541,\n",
       " 'enjoying': 1635,\n",
       " 'changes': 881,\n",
       " 'forward': 1950,\n",
       " 'seeing': 4277,\n",
       " 'what': 5417,\n",
       " 'else': 1599,\n",
       " 'their': 4923,\n",
       " 'sleeves': 4446,\n",
       " 'laurieshook': 2830,\n",
       " 'smcdallas': 4472,\n",
       " 'pre': 3766,\n",
       " 'party': 3568,\n",
       " 'wed': 5399,\n",
       " 'hoping': 2398,\n",
       " 'win': 5451,\n",
       " 'resulting': 4095,\n",
       " 'shameless': 4336,\n",
       " 'promotion': 3837,\n",
       " 'chevysmc': 914,\n",
       " 'michaelpiliero': 3144,\n",
       " 'someone': 4522,\n",
       " 'started': 4607,\n",
       " 'partnerhub': 3565,\n",
       " 'group': 2189,\n",
       " 'groups': 2194,\n",
       " '4sq3': 97,\n",
       " 'looks': 2954,\n",
       " 'like': 2890,\n",
       " 'going': 2124,\n",
       " 'rock': 4151,\n",
       " 'update': 5223,\n",
       " 'push': 3878,\n",
       " 'tonight': 5021,\n",
       " 'etsbzk': 1673,\n",
       " 'keepaustinweird': 2735,\n",
       " 'were': 5411,\n",
       " 'right': 4130,\n",
       " 'sweeeeet': 4749,\n",
       " 'job': 2693,\n",
       " 'team': 4851,\n",
       " 'very': 5288,\n",
       " 'a3xvwc6': 143,\n",
       " 'may': 3088,\n",
       " 'leave': 2854,\n",
       " 'vuvuzela': 5338,\n",
       " 'home': 2380,\n",
       " 'now': 3376,\n",
       " 'your': 5579,\n",
       " 'are': 378,\n",
       " 'itunes': 2664,\n",
       " 'us': 5237,\n",
       " 'id420666439': 2455,\n",
       " 'mt': 3257,\n",
       " 'mention': 3124,\n",
       " 'ha': 2227,\n",
       " 'first': 1872,\n",
       " 'line': 2901,\n",
       " 'quot': 3920,\n",
       " 'pop': 3723,\n",
       " 'event': 1681,\n",
       " 'planner': 3678,\n",
       " 'eventprofs': 1683,\n",
       " 'pcma': 3593,\n",
       " 'engage365': 1625,\n",
       " 'false': 1774,\n",
       " 'alarm': 262,\n",
       " 'circles': 944,\n",
       " 'probably': 3815,\n",
       " 'ever': 1687,\n",
       " 'link': 2905,\n",
       " 'social': 4497,\n",
       " 'weather': 5388,\n",
       " 'greet': 2179,\n",
       " 'sweater': 4748,\n",
       " 'night': 3342,\n",
       " 'putting': 3885,\n",
       " 'flash': 1885,\n",
       " 'downtown': 1505,\n",
       " 'sell': 4291,\n",
       " 'smartcover': 4465,\n",
       " 'opens': 3450,\n",
       " 'instant': 2576,\n",
       " 'access': 159,\n",
       " 'waited': 5343,\n",
       " 'get': 2075,\n",
       " 'one': 3436,\n",
       " 'hooray': 2390,\n",
       " 'opening': 3449,\n",
       " 'wooooo': 5495,\n",
       " 'open': 3444,\n",
       " 'midnight': 3151,\n",
       " 'talking': 4830,\n",
       " 'effort': 1585,\n",
       " 'allow': 271,\n",
       " 'systems': 4807,\n",
       " 'bettercloud': 596,\n",
       " '1st': 44,\n",
       " 'stop': 4642,\n",
       " 'chaos': 885,\n",
       " 'hunt': 2437,\n",
       " 'java': 2676,\n",
       " 'spy': 4585,\n",
       " 'chance': 876,\n",
       " 'omfg': 3430,\n",
       " 'heard': 2304,\n",
       " 'pics': 3644,\n",
       " 'already': 278,\n",
       " 'attending': 426,\n",
       " 'headaches': 2295,\n",
       " 'power': 3754,\n",
       " 'sxswi': 4781,\n",
       " 'do': 1453,\n",
       " 'bands': 505,\n",
       " 'food': 1925,\n",
       " 'art': 394,\n",
       " 'ice': 2451,\n",
       " 'cream': 1187,\n",
       " 'nifty': 3341,\n",
       " 'interactive': 2589,\n",
       " 'maps': 3052,\n",
       " 'holla': 2372,\n",
       " 'butt': 765,\n",
       " 'over': 3494,\n",
       " 'here': 2325,\n",
       " 'case': 830,\n",
       " 'but': 764,\n",
       " 'phone': 3630,\n",
       " 'of': 3406,\n",
       " 'post': 3746,\n",
       " 'makes': 3030,\n",
       " 'easy': 1565,\n",
       " 'connect': 1074,\n",
       " 'all': 270,\n",
       " 'networks': 3318,\n",
       " 'behaving': 573,\n",
       " 'today': 5013,\n",
       " 'crashes': 1178,\n",
       " 'yesterday': 5569,\n",
       " 'ridiculous': 4126,\n",
       " 'hey': 2328,\n",
       " 'fans': 1784,\n",
       " 'peek': 3601,\n",
       " 'space': 4546,\n",
       " 'slated': 4442,\n",
       " 'tomorrow': 5017,\n",
       " 'thing': 4943,\n",
       " 'doing': 1467,\n",
       " 'earth': 1559,\n",
       " 'face': 1758,\n",
       " 'company': 1038,\n",
       " 'her': 2324,\n",
       " 'sxwsi': 4795,\n",
       " 'thanks': 4912,\n",
       " 'speech': 4559,\n",
       " 'apps': 372,\n",
       " 'being': 576,\n",
       " 'showcased': 4383,\n",
       " 'conf': 1061,\n",
       " 'sxswh': 4780,\n",
       " 'sxsh': 4767,\n",
       " 'does': 1462,\n",
       " 'provide': 3848,\n",
       " 'chargers': 890,\n",
       " 've': 5274,\n",
       " 'changed': 879,\n",
       " 'mind': 3164,\n",
       " 'next': 3330,\n",
       " 'xmas': 5546,\n",
       " 'shiny': 4356,\n",
       " 'garyvee': 2035,\n",
       " 'book': 655,\n",
       " 'stores': 4647,\n",
       " 'christmas': 935,\n",
       " 'nerds': 3309,\n",
       " 'yai': 5552,\n",
       " 'ubersocial': 5158,\n",
       " 'includes': 2523,\n",
       " 'uberguide': 5157,\n",
       " 'sponsored': 4575,\n",
       " 'cont': 1093,\n",
       " 'fast': 1795,\n",
       " 'fun': 2001,\n",
       " 'future': 2007,\n",
       " 'presenting': 3789,\n",
       " 'search': 4262,\n",
       " 'local': 2930,\n",
       " 'mobile': 3198,\n",
       " 'headline': 2298,\n",
       " 'gadget': 2018,\n",
       " 'hmm': 2364,\n",
       " 'could': 1146,\n",
       " 'seen': 4282,\n",
       " 'launched': 2826,\n",
       " 'checkins': 906,\n",
       " 'month': 3224,\n",
       " 'ago': 236,\n",
       " 'ins': 2558,\n",
       " 'ok': 3423,\n",
       " 'outs': 3492,\n",
       " 'bizzy': 619,\n",
       " 'before': 566,\n",
       " 'tweet': 5134,\n",
       " 'think': 4946,\n",
       " 'speak': 4552,\n",
       " 'mark': 3063,\n",
       " 'belinsky': 578,\n",
       " '911tweets': 131,\n",
       " 'panel': 3537,\n",
       " 'kawasaki': 2733,\n",
       " 'lewis': 2875,\n",
       " 'level': 2873,\n",
       " 'reasoning': 3977,\n",
       " 'continued': 1100,\n",
       " 'existence': 1721,\n",
       " 'evidence': 1697,\n",
       " 'bawling': 544,\n",
       " 'pagemaker': 3526,\n",
       " 'saved': 4214,\n",
       " 'those': 4958,\n",
       " 'jwtatl': 2728,\n",
       " 'enchantment': 1616,\n",
       " 'thoughts': 4962,\n",
       " 'japan': 2673,\n",
       " 'apac': 341,\n",
       " 'regions': 4018,\n",
       " 'dealing': 1291,\n",
       " 'earthquake': 1561,\n",
       " 'tsunami': 5112,\n",
       " 'trauma': 5078,\n",
       " 'schools': 4245,\n",
       " 'marketing': 3066,\n",
       " 'experts': 1735,\n",
       " 'temporary': 4886,\n",
       " 'def': 1308,\n",
       " 'tent': 4893,\n",
       " 'powerhouse': 3756,\n",
       " 'gym': 2223,\n",
       " '6th': 115,\n",
       " 'congress': 1073,\n",
       " 'along': 275,\n",
       " '10': 4,\n",
       " '000': 0,\n",
       " 'happy': 2265,\n",
       " 'hipsters': 2351,\n",
       " 'wins': 5459,\n",
       " 'support': 4723,\n",
       " 'launch': 2825,\n",
       " 'trending': 5086,\n",
       " 'nerdy': 3311,\n",
       " 'christian': 934,\n",
       " 'devs': 1376,\n",
       " 'want': 5360,\n",
       " 'talk': 4828,\n",
       " 'or': 3460,\n",
       " 'maybe': 3089,\n",
       " 'we': 5385,\n",
       " 'wk': 5475,\n",
       " 'together': 5014,\n",
       " 'cool': 1119,\n",
       " 'me': 3096,\n",
       " 'has': 2271,\n",
       " 'taken': 4820,\n",
       " 'storm': 4649,\n",
       " 'part': 3559,\n",
       " 'haz': 2290,\n",
       " 'ifrom': 2469,\n",
       " 'gr8': 2160,\n",
       " 'stacks': 4593,\n",
       " 'waiting': 5344,\n",
       " 'bought': 676,\n",
       " 'got': 2151,\n",
       " 'mine': 3169,\n",
       " 'no': 3349,\n",
       " 'hassle': 2277,\n",
       " 'handled': 2249,\n",
       " 'perfectly': 3614,\n",
       " 'take': 4818,\n",
       " 'major': 3026,\n",
       " 'south': 4542,\n",
       " 'korean': 2784,\n",
       " 'director': 1411,\n",
       " 'gets': 2077,\n",
       " '130': 22,\n",
       " 'make': 3028,\n",
       " 'movie': 3247,\n",
       " 'entirely': 1645,\n",
       " 'his': 2355,\n",
       " 'beautiful': 556,\n",
       " 'pic': 3639,\n",
       " 'sneaky': 4494,\n",
       " 'usual': 5254,\n",
       " 'beta': 593,\n",
       " 'testing': 4901,\n",
       " 'moonbot': 3228,\n",
       " 'studios': 4678,\n",
       " 'louisiana': 2968,\n",
       " 'won': 5488,\n",
       " 'day': 1283,\n",
       " 'ton': 5018,\n",
       " 'sold': 4510,\n",
       " 'everything': 1694,\n",
       " 'except': 1707,\n",
       " '64gig': 111,\n",
       " 'wifi': 5440,\n",
       " 'white': 5425,\n",
       " 'did': 1385,\n",
       " 'manage': 3038,\n",
       " 'yours': 5580,\n",
       " 'known': 2782,\n",
       " 'jeans': 2682,\n",
       " 'configuration': 1065,\n",
       " 'offered': 3409,\n",
       " 'promo': 3835,\n",
       " 'ninjafinder': 3348,\n",
       " 'who': 5427,\n",
       " 'sucks': 4701,\n",
       " 'poursite': 3753,\n",
       " 'learning': 2849,\n",
       " 'life': 2881,\n",
       " 'changing': 882,\n",
       " 'impact': 2492,\n",
       " 'real': 3965,\n",
       " 'actual': 184,\n",
       " 'lives': 2918,\n",
       " 'bravo': 697,\n",
       " 'short': 4369,\n",
       " 'lines': 2902,\n",
       " 'left': 2857,\n",
       " 'tradeshow': 5065,\n",
       " 'demo': 1333,\n",
       " 'theatre': 4918,\n",
       " 'see': 4276,\n",
       " 'why': 5434,\n",
       " 'presenters': 3788,\n",
       " 'using': 5252,\n",
       " 'anyone': 333,\n",
       " 'quick': 3913,\n",
       " 'hundred': 2433,\n",
       " 'dollars': 1470,\n",
       " 'ad': 186,\n",
       " 'hoc': 2368,\n",
       " 'cost': 1141,\n",
       " 'monday': 3216,\n",
       " 'barry': 520,\n",
       " 'diller': 1404,\n",
       " 'york': 5576,\n",
       " 'times': 4992,\n",
       " 'lunch': 2991,\n",
       " 'hotel': 2407,\n",
       " 'six': 4424,\n",
       " 'dirty': 1413,\n",
       " 'martinis': 3070,\n",
       " 'mondays': 3217,\n",
       " 'seriously': 4305,\n",
       " 'any': 329,\n",
       " 'constant': 1086,\n",
       " 'causing': 845,\n",
       " 'lost': 2963,\n",
       " 'schedules': 4241,\n",
       " 'sync': 4800,\n",
       " 'wp7': 5525,\n",
       " 'ready': 3964,\n",
       " 'some': 4518,\n",
       " 'ur': 5234,\n",
       " 'blogging': 638,\n",
       " 'conflagration': 1068,\n",
       " 'doofusness': 1487,\n",
       " 'attention': 427,\n",
       " 'ers': 1662,\n",
       " 'rumored': 4179,\n",
       " 'needs': 3302,\n",
       " 'went': 5410,\n",
       " 'lousy': 2970,\n",
       " 'winning': 5458,\n",
       " 'picture': 3645,\n",
       " 'spent': 4565,\n",
       " 'used': 5244,\n",
       " 'couple': 1151,\n",
       " 'city': 948,\n",
       " 'blocks': 634,\n",
       " 'behind': 575,\n",
       " '100s': 6,\n",
       " 'emails': 1601,\n",
       " 'give': 2094,\n",
       " 'composed': 1052,\n",
       " 'replies': 4064,\n",
       " 'protip': 3845,\n",
       " '10pm': 13,\n",
       " 'block': 631,\n",
       " 'popup': 3730,\n",
       " 'selling': 4292,\n",
       " 'ipad2s': 2630,\n",
       " '2s': 64,\n",
       " 'wild': 5443,\n",
       " 'both': 672,\n",
       " 'say': 4221,\n",
       " 'terrible': 4898,\n",
       " 'takes': 4822,\n",
       " 'video': 5298,\n",
       " 'snapping': 4491,\n",
       " 'away': 470,\n",
       " 'keynote': 2744,\n",
       " 'slides': 4450,\n",
       " 'southwest': 4545,\n",
       " 'sweet': 4752,\n",
       " 'ballroom': 501,\n",
       " 'more': 3231,\n",
       " '35': 75,\n",
       " 'million': 3161,\n",
       " 'miles': 3157,\n",
       " 'per': 3610,\n",
       " 'driving': 1521,\n",
       " 'navigation': 3293,\n",
       " 'growing': 2196,\n",
       " 'band': 504,\n",
       " 'share': 4338,\n",
       " 'track': 5058,\n",
       " 'audience': 434,\n",
       " 'stage': 4595,\n",
       " 'use': 5243,\n",
       " 'frostwire': 1983,\n",
       " 'wi': 5435,\n",
       " 'fi': 1843,\n",
       " 'available': 457,\n",
       " 'don': 1475,\n",
       " 'car': 815,\n",
       " 'zimride': 5600,\n",
       " 'etc': 1671,\n",
       " 'rides': 4123,\n",
       " 'shareable': 4339,\n",
       " 'picked': 3641,\n",
       " 'mophie': 3230,\n",
       " 'battery': 538,\n",
       " 'prep': 3778,\n",
       " 'lugging': 2990,\n",
       " 'laptop': 2805,\n",
       " 'huge': 2431,\n",
       " 'last': 2812,\n",
       " 'rumor': 4178,\n",
       " 'monger': 3220,\n",
       " 'read': 3960,\n",
       " 'preview': 3796,\n",
       " 'socbiz': 4496,\n",
       " 'fb': 1811,\n",
       " 'hobo': 2367,\n",
       " 'shotgun': 4374,\n",
       " 'yes': 5568,\n",
       " 'please': 3695,\n",
       " 'build': 748,\n",
       " 'less': 2865,\n",
       " '24': 54,\n",
       " 'hours': 2413,\n",
       " 'biggest': 606,\n",
       " 'launches': 2827,\n",
       " 'history': 2357,\n",
       " 'groupme': 2192,\n",
       " 'sounds': 4540,\n",
       " 'incredible': 2527,\n",
       " 'phones': 3631,\n",
       " 'yet': 5570,\n",
       " 'alarms': 263,\n",
       " 'botch': 671,\n",
       " 'timechange': 4989,\n",
       " 'freak': 1963,\n",
       " 'missed': 3179,\n",
       " 'panels': 3540,\n",
       " 'bloody': 640,\n",
       " 'marys': 3072,\n",
       " 'shouts': 4380,\n",
       " 'ladies': 2795,\n",
       " 'holding': 2370,\n",
       " 'shows': 4387,\n",
       " 'photos': 3635,\n",
       " 'sipping': 4416,\n",
       " 'beer': 565,\n",
       " 'cc': 848,\n",
       " 'meant': 3102,\n",
       " 'wish': 5467,\n",
       " 'dyac': 1545,\n",
       " 'stupid': 4686,\n",
       " 'rumors': 4180,\n",
       " 'signs': 4403,\n",
       " 'point': 3710,\n",
       " '15': 27,\n",
       " 'minute': 3174,\n",
       " 'brilliant': 717,\n",
       " 'enlightening': 1637,\n",
       " 'mechanics': 3107,\n",
       " 'presentation': 3786,\n",
       " 'sales': 4195,\n",
       " 'genius': 2067,\n",
       " 'dictaphone': 1383,\n",
       " 'vid': 5297,\n",
       " 'camera': 797,\n",
       " 'wow': 5522,\n",
       " 'cerebellum': 863,\n",
       " 'charged': 888,\n",
       " 'personal': 3622,\n",
       " 'found': 1952,\n",
       " 'kyping': 2789,\n",
       " 'geolocation': 2072,\n",
       " 'releasing': 4039,\n",
       " 'when': 5419,\n",
       " 'background': 484,\n",
       " 'patch': 3579,\n",
       " 'batterykiller': 539,\n",
       " 'live': 2916,\n",
       " 'rsvp': 4172,\n",
       " 'sundayswagger': 4716,\n",
       " 'eventbrite': 1682,\n",
       " '20': 45,\n",
       " 'scoremore': 4253,\n",
       " 'course': 1153,\n",
       " 'built': 751,\n",
       " 'temp': 4883,\n",
       " 'texas': 4905,\n",
       " 'understand': 5184,\n",
       " 'concept': 1057,\n",
       " 'corralling': 1137,\n",
       " 'cattle': 842,\n",
       " 'pickmeupanipad2': 3643,\n",
       " 'didn': 1386,\n",
       " 'let': 2867,\n",
       " 'something': 4524,\n",
       " 'having': 2285,\n",
       " 'retail': 4098,\n",
       " 'near': 3297,\n",
       " 'keep': 2734,\n",
       " 'surprise': 4732,\n",
       " 'opened': 3447,\n",
       " 'town': 5056,\n",
       " 'cnet': 985,\n",
       " 'ipads': 2636,\n",
       " 'example': 1704,\n",
       " 'layed': 2836,\n",
       " 'registers': 4020,\n",
       " 'checkout': 907,\n",
       " 'gswsxsw': 2203,\n",
       " 'traffic': 5066,\n",
       " 'noon': 3357,\n",
       " 'fresh': 1969,\n",
       " 'shipment': 4357,\n",
       " 'pretty': 3794,\n",
       " 'fingers': 1869,\n",
       " 'crossed': 1206,\n",
       " 'technews': 4867,\n",
       " 'saves': 4215,\n",
       " 'set': 4318,\n",
       " 'tech_news': 4860,\n",
       " 'lt': 2985,\n",
       " 'true': 5100,\n",
       " 'loathe': 2927,\n",
       " 'blogs': 639,\n",
       " 'atx': 432,\n",
       " 'mall': 3035,\n",
       " '10x': 14,\n",
       " 'crowded': 1210,\n",
       " 'fake': 1773,\n",
       " 'fucking': 1994,\n",
       " 'dongle': 1480,\n",
       " 'makeshift': 3031,\n",
       " 'kidding': 2755,\n",
       " 'amazing': 288,\n",
       " 'fanboy': 1779,\n",
       " 'shit': 4361,\n",
       " 'temptation': 4888,\n",
       " 'control': 1104,\n",
       " '5pm': 106,\n",
       " 'view': 5303,\n",
       " 'io': 2623,\n",
       " 'office': 3412,\n",
       " 'news': 3325,\n",
       " 'during': 1542,\n",
       " '150': 28,\n",
       " 'few': 1840,\n",
       " 'too': 5023,\n",
       " 'software': 4509,\n",
       " 'development': 1372,\n",
       " 'once': 3435,\n",
       " 'release': 4036,\n",
       " 'working': 5506,\n",
       " 'overheard': 3499,\n",
       " 'relaxing': 4035,\n",
       " 'computer': 1053,\n",
       " 'agreed': 238,\n",
       " 'arg': 382,\n",
       " 'hate': 2278,\n",
       " 'blackberry': 622,\n",
       " 'back': 483,\n",
       " 'shocked': 4364,\n",
       " 'technology': 4870,\n",
       " 'because': 559,\n",
       " 'go': 2112,\n",
       " 'everbody': 1688,\n",
       " 'media': 3108,\n",
       " 'driven': 1518,\n",
       " 'revolutions': 4114,\n",
       " 'better': 595,\n",
       " 'long': 2946,\n",
       " 'work': 5502,\n",
       " 'rather': 3951,\n",
       " 'agileagency': 234,\n",
       " 'knackered': 2774,\n",
       " 'playing': 3692,\n",
       " 'updating': 5226,\n",
       " 'timeline': 4990,\n",
       " 'smashed': 4471,\n",
       " 'partytweets': 3570,\n",
       " 'bed': 562,\n",
       " 'mdw': 3095,\n",
       " 'second': 4272,\n",
       " 'halfway': 2240,\n",
       " 'through': 4968,\n",
       " 'haven': 2283,\n",
       " 'even': 1679,\n",
       " 'boarded': 651,\n",
       " 'plane': 3674,\n",
       " 'amateurhour': 287,\n",
       " 'jobs_co': 2695,\n",
       " 'crowd': 1208,\n",
       " 'tripping': 5094,\n",
       " 'each': 1549,\n",
       " 'other': 3480,\n",
       " 'aclu': 172,\n",
       " '80': 119,\n",
       " 'laughed': 2824,\n",
       " 'until': 5214,\n",
       " 'week': 5400,\n",
       " 'hauling': 2281,\n",
       " 'macbook': 3002,\n",
       " 'everywhere': 1696,\n",
       " 'shoulder': 4377,\n",
       " 'rub': 4174,\n",
       " 'attendees': 425,\n",
       " 'qr': 3891,\n",
       " 'code': 997,\n",
       " 'reader': 3961,\n",
       " 'image': 2482,\n",
       " 'badge': 490,\n",
       " 'optiscan': 3459,\n",
       " 'four': 1953,\n",
       " 'took': 5026,\n",
       " 'lego': 2859,\n",
       " 'pit': 3658,\n",
       " 'replaced': 4059,\n",
       " 'recharging': 3985,\n",
       " 'station': 4615,\n",
       " 'might': 3155,\n",
       " 'prices': 3800,\n",
       " 'crap': 1170,\n",
       " 'samsung': 4201,\n",
       " 'bad': 489,\n",
       " 'qs': 3895,\n",
       " 'process': 3817,\n",
       " 'poo': 3718,\n",
       " 'poos': 3722,\n",
       " 'ideas': 2458,\n",
       " 'leaves': 2855,\n",
       " 'early': 1554,\n",
       " 'creative': 1192,\n",
       " 'busy': 763,\n",
       " 'gamestorming': 2031,\n",
       " 'trying': 5109,\n",
       " 'balance': 497,\n",
       " 'vs': 5335,\n",
       " 'airplane': 252,\n",
       " 'mode': 3205,\n",
       " '100': 5,\n",
       " 'tweets': 5142,\n",
       " 'precommerce': 3768,\n",
       " 'watch': 5375,\n",
       " 'ipadmadness': 2635,\n",
       " 'shop': 4367,\n",
       " 'core': 1133,\n",
       " 'action': 177,\n",
       " 'celebrate': 850,\n",
       " 'beauty': 558,\n",
       " 'web': 5389,\n",
       " 'msft': 3256,\n",
       " 'ie9': 2466,\n",
       " 'play': 3685,\n",
       " 'html5': 2426,\n",
       " 'gave': 2040,\n",
       " 'money': 3219,\n",
       " 'relief': 4042,\n",
       " 'had': 2234,\n",
       " 'improve': 2507,\n",
       " 'beyond': 601,\n",
       " 'sharing': 4342,\n",
       " 'positive': 3740,\n",
       " 'actions': 178,\n",
       " 'world': 5509,\n",
       " 'download': 1499,\n",
       " 'earthhour': 1560,\n",
       " '60': 108,\n",
       " 'bing': 610,\n",
       " 'bettersearch': 597,\n",
       " 'shot': 4373,\n",
       " 'success': 4693,\n",
       " 'structured': 4672,\n",
       " 'potentially': 3751,\n",
       " 'higher': 2336,\n",
       " 'margin': 3057,\n",
       " 'cpa': 1165,\n",
       " 'model': 3206,\n",
       " 'results': 4096,\n",
       " 'seo': 4300,\n",
       " 'qagb': 3889,\n",
       " 'impulsive': 2512,\n",
       " 'friends': 1977,\n",
       " 'gen': 2061,\n",
       " '32gb': 74,\n",
       " 'buying': 771,\n",
       " 'geekdate': 2049,\n",
       " 'herself': 2327,\n",
       " 'present': 3785,\n",
       " 'rsq': 4171,\n",
       " 'exciting': 1712,\n",
       " 'certain': 865,\n",
       " 'meetings': 3111,\n",
       " 'hello': 2318,\n",
       " 'dont': 1482,\n",
       " 'miss': 3178,\n",
       " 'charles': 895,\n",
       " 'chen': 912,\n",
       " 'chromeos': 937,\n",
       " 'booth': 661,\n",
       " 'exhibit': 1718,\n",
       " 'hall': 2241,\n",
       " '1pm': 43,\n",
       " 'guess': 2208,\n",
       " 'desperate': 1357,\n",
       " 'locations': 2935,\n",
       " 'considering': 1083,\n",
       " 'leaving': 2856,\n",
       " 'pro': 3813,\n",
       " 'flying': 1908,\n",
       " 'solo': 4513,\n",
       " 'battlela': 542,\n",
       " 'secret': 4274,\n",
       " 'batphone': 535,\n",
       " 'tv': 5129,\n",
       " 'league': 2843,\n",
       " 'extraordinary': 1750,\n",
       " 'hackers': 2232,\n",
       " 'lxh': 2995,\n",
       " 'big': 604,\n",
       " 'chunky': 940,\n",
       " 'elements': 1595,\n",
       " 'generous': 2066,\n",
       " 'clarity': 952,\n",
       " 'trumps': 5103,\n",
       " 'density': 1344,\n",
       " 'tap': 4833,\n",
       " 'quality': 3900,\n",
       " 'quantity': 3901,\n",
       " 'tapworthy': 4834,\n",
       " 'designing': 1352,\n",
       " 'interfaces': 2593,\n",
       " 'schemas': 4243,\n",
       " 'thinks': 4948,\n",
       " 'content': 1095,\n",
       " 'nuts': 3387,\n",
       " 'another': 318,\n",
       " 'network': 3316,\n",
       " 'docomo': 1456,\n",
       " 'introduced': 2608,\n",
       " 'years': 5560,\n",
       " 'came': 796,\n",
       " ...}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '02',\n",
       " '03',\n",
       " '08',\n",
       " '10',\n",
       " '100',\n",
       " '100s',\n",
       " '100tc',\n",
       " '101',\n",
       " '106',\n",
       " '10am',\n",
       " '10k',\n",
       " '10mins',\n",
       " '10pm',\n",
       " '10x',\n",
       " '11',\n",
       " '11ntc',\n",
       " '11th',\n",
       " '12',\n",
       " '12b',\n",
       " '12th',\n",
       " '13',\n",
       " '130',\n",
       " '14',\n",
       " '1406',\n",
       " '1413',\n",
       " '1415',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '150m',\n",
       " '157',\n",
       " '15am',\n",
       " '15k',\n",
       " '16162',\n",
       " '16gb',\n",
       " '16mins',\n",
       " '17',\n",
       " '188',\n",
       " '1986',\n",
       " '1990style',\n",
       " '1m',\n",
       " '1of',\n",
       " '1pm',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '20s',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '250k',\n",
       " '25th',\n",
       " '2am',\n",
       " '2day',\n",
       " '2honor',\n",
       " '2moro',\n",
       " '2nd',\n",
       " '2nite',\n",
       " '2s',\n",
       " '2yrs',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30a',\n",
       " '30am',\n",
       " '30p',\n",
       " '30pm',\n",
       " '32',\n",
       " '32gb',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '3d',\n",
       " '3g',\n",
       " '3gs',\n",
       " '3k',\n",
       " '3rd',\n",
       " '3x',\n",
       " '40',\n",
       " '400',\n",
       " '40min',\n",
       " '41',\n",
       " '45',\n",
       " '45am',\n",
       " '47',\n",
       " '48',\n",
       " '4android',\n",
       " '4chan',\n",
       " '4g',\n",
       " '4nqv92l',\n",
       " '4sq',\n",
       " '4sq3',\n",
       " '4square',\n",
       " '50',\n",
       " '54',\n",
       " '55',\n",
       " '58',\n",
       " '59',\n",
       " '59p',\n",
       " '59pm',\n",
       " '5pm',\n",
       " '5th',\n",
       " '60',\n",
       " '64g',\n",
       " '64gb',\n",
       " '64gig',\n",
       " '64mb',\n",
       " '65',\n",
       " '6hours',\n",
       " '6th',\n",
       " '70',\n",
       " '75',\n",
       " '7th',\n",
       " '80',\n",
       " '800',\n",
       " '80s',\n",
       " '81',\n",
       " '82',\n",
       " '89',\n",
       " '8am',\n",
       " '8p',\n",
       " '8pm',\n",
       " '8th',\n",
       " '90',\n",
       " '900',\n",
       " '911tweets',\n",
       " '95',\n",
       " '96',\n",
       " '967',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '9th',\n",
       " '__',\n",
       " '______',\n",
       " '_______',\n",
       " '_r',\n",
       " 'a3xvwc6',\n",
       " 'aapl',\n",
       " 'abacus',\n",
       " 'abandoned',\n",
       " 'aber',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'about',\n",
       " 'abroad',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abt',\n",
       " 'abuzz',\n",
       " 'academy',\n",
       " 'acc',\n",
       " 'acceptable',\n",
       " 'access',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessories',\n",
       " 'accessory',\n",
       " 'accesssxsw',\n",
       " 'accommodate',\n",
       " 'according',\n",
       " 'accordion',\n",
       " 'account',\n",
       " 'acerbic',\n",
       " 'achieve',\n",
       " 'acknowledge',\n",
       " 'aclu',\n",
       " 'aclus',\n",
       " 'acquired',\n",
       " 'across',\n",
       " 'acrosse',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'activations',\n",
       " 'activity',\n",
       " 'actors',\n",
       " 'actsofsharing',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'adapt',\n",
       " 'adaptive',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addictedtotheinterwebs',\n",
       " 'addictive',\n",
       " 'addicts',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'adfonic',\n",
       " 'admired',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admitting',\n",
       " 'ado',\n",
       " 'adopter',\n",
       " 'adopters',\n",
       " 'adoption',\n",
       " 'adpeopleproblems',\n",
       " 'ads',\n",
       " 'advanced',\n",
       " 'advent',\n",
       " 'adventure',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advisory',\n",
       " 'aesthetic',\n",
       " 'affair',\n",
       " 'affirmative',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'africans',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agchat',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agents',\n",
       " 'agileagency',\n",
       " 'agnerd',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ahing',\n",
       " 'aicn',\n",
       " 'aiding',\n",
       " 'aim',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'airline',\n",
       " 'airlines',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'airs',\n",
       " 'ajs2011',\n",
       " 'aka',\n",
       " 'akqas',\n",
       " 'al',\n",
       " 'alamo',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alarms',\n",
       " 'alas',\n",
       " 'alcoholics',\n",
       " 'alert',\n",
       " 'alerts',\n",
       " 'alex',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allowing',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'alphagraphics',\n",
       " 'already',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'always',\n",
       " 'alwayshavingtoplugin',\n",
       " 'am',\n",
       " 'amateurhour',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'ambassador',\n",
       " 'america',\n",
       " 'amex',\n",
       " 'amigos',\n",
       " 'among',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'amused',\n",
       " 'amusing',\n",
       " 'an',\n",
       " 'analysis',\n",
       " 'analytics',\n",
       " 'and',\n",
       " 'andoid',\n",
       " 'andriod',\n",
       " 'andro',\n",
       " 'android',\n",
       " 'androidsxsw',\n",
       " 'angry',\n",
       " 'angrybirds',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'anoth',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'antigov',\n",
       " 'antique',\n",
       " 'antonio',\n",
       " 'antwoord',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anybodywanttobuymeanipad2',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyones',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'aos',\n",
       " 'ap',\n",
       " 'apac',\n",
       " 'apartment',\n",
       " 'api',\n",
       " 'apis',\n",
       " 'app',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appcircus',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appears',\n",
       " 'applauds',\n",
       " 'applause',\n",
       " 'apple',\n",
       " 'apple_store',\n",
       " 'appleaddiction',\n",
       " 'appleatxdt',\n",
       " 'applefanatic',\n",
       " 'apples',\n",
       " 'appletakingoverworld',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'appolicious',\n",
       " 'appreciate',\n",
       " 'appreciation',\n",
       " 'approach',\n",
       " 'approaches',\n",
       " 'approval',\n",
       " 'approved',\n",
       " 'approves',\n",
       " 'apps',\n",
       " 'aquent',\n",
       " 'arcade',\n",
       " 'archive',\n",
       " 'arctic',\n",
       " 'arduino',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " 'arg',\n",
       " 'argues',\n",
       " 'argument',\n",
       " 'aristotle',\n",
       " 'arm',\n",
       " 'armadillo',\n",
       " 'armed',\n",
       " 'aron',\n",
       " 'around',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'art',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'articulate',\n",
       " 'artificial',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artists',\n",
       " 'artwork',\n",
       " 'artworks',\n",
       " 'arw',\n",
       " 'as',\n",
       " 'asddieu',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'assisted',\n",
       " 'assistivetech',\n",
       " 'assume',\n",
       " 'at',\n",
       " 'atari',\n",
       " 'atl',\n",
       " 'atms',\n",
       " 'atrix',\n",
       " 'att',\n",
       " 'attached',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attended',\n",
       " 'attendees',\n",
       " 'attending',\n",
       " 'attention',\n",
       " 'attitudes',\n",
       " 'attracted',\n",
       " 'attracting',\n",
       " 'attractive',\n",
       " 'atx',\n",
       " 'atzip',\n",
       " 'audience',\n",
       " 'audio',\n",
       " 'augcomm',\n",
       " 'augmented',\n",
       " 'augmentedreality',\n",
       " 'auntie',\n",
       " 'aus',\n",
       " 'austin',\n",
       " 'austincrowd',\n",
       " 'austinites',\n",
       " 'austintx',\n",
       " 'austinwins',\n",
       " 'australian',\n",
       " 'ausxsw',\n",
       " 'auth',\n",
       " 'authenticator',\n",
       " 'authorization',\n",
       " 'autistic',\n",
       " 'auto',\n",
       " 'autocorrect',\n",
       " 'autocorrects',\n",
       " 'autonomous',\n",
       " 'avail',\n",
       " 'available',\n",
       " 'ave',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'averages',\n",
       " 'avoid',\n",
       " 'avoiding',\n",
       " 'aw',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awesomely',\n",
       " 'awesomeness',\n",
       " 'awesometiming',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awwww',\n",
       " 'axzwxb',\n",
       " 'b4',\n",
       " 'baaah',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'background',\n",
       " 'backlight',\n",
       " 'backpack',\n",
       " 'backup',\n",
       " 'backupify',\n",
       " 'bad',\n",
       " 'badge',\n",
       " 'badgeless',\n",
       " 'badges',\n",
       " 'bag',\n",
       " 'bags',\n",
       " 'bahahahaha',\n",
       " 'bajillions',\n",
       " 'balance',\n",
       " 'balckberries',\n",
       " 'balcony',\n",
       " 'ball',\n",
       " 'ballroom',\n",
       " 'ballrooms',\n",
       " 'banality',\n",
       " 'band',\n",
       " 'bands',\n",
       " 'bandwaggoners',\n",
       " 'bandwidth',\n",
       " 'bang',\n",
       " 'banged',\n",
       " 'bank',\n",
       " 'banking',\n",
       " 'bankinnovate',\n",
       " 'bankinnovation',\n",
       " 'banks',\n",
       " 'bar',\n",
       " 'barcode',\n",
       " 'barely',\n",
       " 'barring',\n",
       " 'barroom',\n",
       " 'barry',\n",
       " 'barrydiller',\n",
       " 'bars',\n",
       " 'bart',\n",
       " 'barton',\n",
       " 'based',\n",
       " 'bashing',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basics',\n",
       " 'basis',\n",
       " 'basket',\n",
       " 'bastards',\n",
       " 'bat',\n",
       " 'bathroom',\n",
       " 'batphone',\n",
       " 'batt',\n",
       " 'batteries',\n",
       " 'battery',\n",
       " 'batterykiller',\n",
       " 'battle',\n",
       " 'battledecks',\n",
       " 'battlela',\n",
       " 'bavcid',\n",
       " 'bawling',\n",
       " 'bb',\n",
       " 'bbq',\n",
       " 'bc',\n",
       " 'bday',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beans',\n",
       " 'bear',\n",
       " 'beard',\n",
       " 'beat',\n",
       " 'beats',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becoming',\n",
       " 'bed',\n",
       " 'beechwood',\n",
       " 'been',\n",
       " 'beer',\n",
       " 'before',\n",
       " 'beforetwitter',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'begins',\n",
       " 'behance',\n",
       " 'behave',\n",
       " 'behaving',\n",
       " 'behavior',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'belinsky',\n",
       " 'belong',\n",
       " 'beluga',\n",
       " 'ben',\n",
       " 'benefit',\n",
       " 'benieuwd',\n",
       " 'bereft',\n",
       " 'bergstrom',\n",
       " 'berklee',\n",
       " 'bernd',\n",
       " 'berry',\n",
       " 'best',\n",
       " 'bestappever',\n",
       " 'bestie',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'betainvites',\n",
       " 'better',\n",
       " 'bettercloud',\n",
       " 'bettersearch',\n",
       " 'betterthingstodo',\n",
       " 'between',\n",
       " 'beware',\n",
       " 'beyond',\n",
       " 'bff',\n",
       " 'bicycle',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bike',\n",
       " 'billion',\n",
       " 'bin',\n",
       " 'bing',\n",
       " 'biomimicry',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bite',\n",
       " 'biz',\n",
       " 'bizzy',\n",
       " 'bjdproductions',\n",
       " 'black',\n",
       " 'blackberry',\n",
       " 'blackbook',\n",
       " 'blacked',\n",
       " 'blame',\n",
       " 'blast',\n",
       " 'bleed',\n",
       " 'blew',\n",
       " 'blind',\n",
       " 'blinksale',\n",
       " 'block',\n",
       " 'blocked',\n",
       " 'blocking',\n",
       " 'blocks',\n",
       " 'blog',\n",
       " 'bloggable',\n",
       " 'blogger',\n",
       " 'blogging',\n",
       " 'blogs',\n",
       " 'bloody',\n",
       " 'bloomberg',\n",
       " 'blows',\n",
       " 'blue',\n",
       " 'blueray',\n",
       " 'bluetooth',\n",
       " 'bluezoom',\n",
       " 'blurs',\n",
       " 'bmm',\n",
       " 'bnet',\n",
       " 'board',\n",
       " 'boarded',\n",
       " 'body',\n",
       " 'bomb',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'bookbook',\n",
       " 'books',\n",
       " 'boom',\n",
       " 'boomers',\n",
       " 'boost',\n",
       " 'booth',\n",
       " 'boots',\n",
       " 'booyah',\n",
       " 'booze',\n",
       " 'borderstylo',\n",
       " 'bored',\n",
       " 'born',\n",
       " 'borrow',\n",
       " 'borrowing',\n",
       " 'boss',\n",
       " 'botch',\n",
       " 'both',\n",
       " 'bother',\n",
       " 'bots',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'bounced',\n",
       " 'bound',\n",
       " 'boundaries',\n",
       " 'bout',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boxee',\n",
       " 'boxes',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'boys',\n",
       " 'bpm',\n",
       " 'bracket',\n",
       " 'brah',\n",
       " 'brain',\n",
       " 'brains',\n",
       " 'brainwashed',\n",
       " 'brand',\n",
       " 'branded',\n",
       " 'brands',\n",
       " 'bravo',\n",
       " 'brawls',\n",
       " 'brazil',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breakfast',\n",
       " 'breaking',\n",
       " 'breakout',\n",
       " 'breakthrough',\n",
       " 'breathtaking',\n",
       " 'breeds',\n",
       " 'brian_lam',\n",
       " 'brick',\n",
       " 'bricklin',\n",
       " 'bridging',\n",
       " 'bright',\n",
       " 'brightens',\n",
       " 'brightness',\n",
       " 'brilliance',\n",
       " 'brilliant',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'brisk',\n",
       " 'british',\n",
       " 'brits',\n",
       " 'brk',\n",
       " 'bro',\n",
       " 'broadcast',\n",
       " 'broadcastr',\n",
       " 'broadfeed',\n",
       " 'broken',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'browse',\n",
       " 'browser',\n",
       " 'browserwars',\n",
       " 'browsing',\n",
       " 'bruises',\n",
       " 'brushstroke',\n",
       " 'bryce',\n",
       " 'bt',\n",
       " 'btw',\n",
       " 'bubble',\n",
       " 'bucket',\n",
       " 'buffalo',\n",
       " 'bug',\n",
       " 'bugger',\n",
       " 'buggy',\n",
       " 'bugs',\n",
       " 'build',\n",
       " 'building',\n",
       " 'buildings',\n",
       " 'built',\n",
       " 'bulletin',\n",
       " 'bummed',\n",
       " 'bummer',\n",
       " 'bumped',\n",
       " 'bunch',\n",
       " 'burn',\n",
       " 'bursts',\n",
       " 'bus',\n",
       " 'busdev',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'butt',\n",
       " 'button',\n",
       " 'buttons',\n",
       " 'butts',\n",
       " 'buy',\n",
       " 'buyers',\n",
       " 'buying',\n",
       " 'buys',\n",
       " 'buzz',\n",
       " 'buzzing',\n",
       " 'by',\n",
       " 'bynd',\n",
       " 'ca',\n",
       " 'cab',\n",
       " 'cabbies',\n",
       " 'cable',\n",
       " 'cables',\n",
       " 'cabs',\n",
       " 'cactus',\n",
       " 'cake',\n",
       " 'calendar',\n",
       " 'calhoun',\n",
       " 'california',\n",
       " 'call',\n",
       " 'callay',\n",
       " 'callback',\n",
       " 'called',\n",
       " 'callooh',\n",
       " 'calls',\n",
       " 'calyp',\n",
       " 'cam',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'cameras',\n",
       " 'campaigns',\n",
       " 'campbell',\n",
       " 'campus',\n",
       " 'can',\n",
       " 'canada',\n",
       " 'canadian',\n",
       " 'cancel',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'canvas',\n",
       " 'capabilities',\n",
       " 'capitol',\n",
       " 'capped',\n",
       " 'captain',\n",
       " 'capture',\n",
       " 'captured',\n",
       " 'car',\n",
       " 'caramel',\n",
       " 'carbon',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'career',\n",
       " 'caring',\n",
       " 'carroll',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'cart',\n",
       " 'cartel',\n",
       " 'cartoon',\n",
       " 'cartoonishly',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cash',\n",
       " 'cashmere',\n",
       " 'cashmore',\n",
       " 'cast',\n",
       " 'castle',\n",
       " 'casually',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'catching',\n",
       " 'catphysics',\n",
       " 'cattle',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'causing',\n",
       " 'cautiously',\n",
       " 'cbatsxsw',\n",
       " 'cc',\n",
       " 'cedar',\n",
       " 'celebrate',\n",
       " 'celebrating',\n",
       " 'celebs',\n",
       " 'cell',\n",
       " 'cellular',\n",
       " 'center',\n",
       " 'centers',\n",
       " 'central',\n",
       " 'centre',\n",
       " 'cents',\n",
       " 'ceo',\n",
       " 'ceokidschat',\n",
       " 'cera',\n",
       " 'cerebellum',\n",
       " 'cerebral',\n",
       " 'certain',\n",
       " 'certificate',\n",
       " 'ces',\n",
       " 'ch',\n",
       " 'chain',\n",
       " 'chair',\n",
       " 'chalked',\n",
       " 'challenge',\n",
       " 'challenged',\n",
       " 'challenges',\n",
       " 'champ',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changer',\n",
       " 'changes',\n",
       " 'changing',\n",
       " 'channel',\n",
       " 'channels',\n",
       " 'chaos',\n",
       " 'characters',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'charger',\n",
       " 'chargers',\n",
       " 'charges',\n",
       " 'chargin2diffphonesatonce',\n",
       " 'charging',\n",
       " 'charity',\n",
       " 'charles',\n",
       " 'charm',\n",
       " 'charts',\n",
       " 'chat',\n",
       " 'chatter',\n",
       " 'chatting',\n",
       " 'cheapen',\n",
       " 'cheaper',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'checkins',\n",
       " 'checkout',\n",
       " 'cheeky',\n",
       " 'cheer',\n",
       " 'cheers',\n",
       " 'cheese',\n",
       " 'chen',\n",
       " 'chevy',\n",
       " 'chevysmc',\n",
       " 'chevysxsw',\n",
       " 'chevytweethouse',\n",
       " 'chic',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'childhood',\n",
       " 'chill',\n",
       " 'chilltab',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chip',\n",
       " 'chk',\n",
       " 'chng',\n",
       " 'choice',\n",
       " 'chokes',\n",
       " 'choose',\n",
       " 'choplifter',\n",
       " 'choreography',\n",
       " 'chris',\n",
       " 'christian',\n",
       " 'christmas',\n",
       " 'chrome',\n",
       " 'chromeos',\n",
       " 'chronicling',\n",
       " 'chumps',\n",
       " 'chunky',\n",
       " 'cigarettes',\n",
       " 'cinema',\n",
       " 'circle',\n",
       " 'circles',\n",
       " 'circusmash',\n",
       " 'cited',\n",
       " 'cites',\n",
       " 'city',\n",
       " 'ck',\n",
       " 'cks',\n",
       " 'claims',\n",
       " 'clarity',\n",
       " 'clark',\n",
       " 'class',\n",
       " 'classics',\n",
       " 'classiest',\n",
       " 'classy',\n",
       " 'cle',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'cleveland',\n",
       " 'clever',\n",
       " 'click',\n",
       " 'clicked',\n",
       " 'client',\n",
       " 'clients',\n",
       " 'climbing',\n",
       " 'clipcon',\n",
       " 'clocks',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closely',\n",
       " 'closer',\n",
       " 'clothes',\n",
       " 'cloud',\n",
       " 'cloudapp',\n",
       " 'cloudsight',\n",
       " 'clumsily',\n",
       " 'cluster',\n",
       " 'cluttering',\n",
       " 'cm48',\n",
       " 'cmswire',\n",
       " 'cmty',\n",
       " 'cnet',\n",
       " 'cnn',\n",
       " 'cnngrill',\n",
       " 'cnnmoney',\n",
       " 'cnnmoneysxsw',\n",
       " 'cnt',\n",
       " 'cntr',\n",
       " 'co',\n",
       " 'cobra',\n",
       " 'cocaine',\n",
       " 'cocky',\n",
       " 'cocoon',\n",
       " 'code',\n",
       " 'coded',\n",
       " 'coders',\n",
       " ...]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect.get_feature_names()\n",
    "\n",
    "#We can remove the digits etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ShA6D8jKH0N5"
   },
   "source": [
    "### 6. Find out how many Positive and Negative emotions are there.\n",
    "\n",
    "Hint: Use value_counts on that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7LAl5pzH0N6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive emotion    2672\n",
       "Negative emotion     519\n",
       "Name: emotions, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the positive and negative emotions\n",
    "re_df.emotions.value_counts()\n",
    "\n",
    "#More positive emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8373550611093701"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the null model accuracy..majority class\n",
    "re_df.emotions.value_counts()[0]/re_df.emotions.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUvgj0FoH0N9"
   },
   "source": [
    "### 7. Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'Label'\n",
    "\n",
    "Hint: use map on that column and give labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YftKwFv7H0N9"
   },
   "outputs": [],
   "source": [
    "#We will just map positive as 1 and negative as 0\n",
    "re_df.emotions = re_df.emotions.map({\"Positive emotion\" : 1, \"Negative emotion\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3192</th>\n",
       "      <td>#sxsw Ze Frank I missed the Childhood Walk pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2913</th>\n",
       "      <td>What happened to the Taxi Magic iPhone app? No...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>A major #Apple iOS update the day before #SXSW...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8486</th>\n",
       "      <td>Nice! Austin Apple pop up shop in time for #SX...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>Great night of Interactive parties. And Congre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  emotions\n",
       "3192  #sxsw Ze Frank I missed the Childhood Walk pro...         1\n",
       "2913  What happened to the Taxi Magic iPhone app? No...         0\n",
       "2038  A major #Apple iOS update the day before #SXSW...         0\n",
       "8486  Nice! Austin Apple pop up shop in time for #SX...         1\n",
       "3533  Great night of Interactive parties. And Congre...         1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check sample\n",
    "re_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3191 entries, 0 to 9088\n",
      "Data columns (total 2 columns):\n",
      "text        3191 non-null object\n",
      "emotions    3191 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 74.8+ KB\n"
     ]
    }
   ],
   "source": [
    "re_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YErwYLCH0N_"
   },
   "source": [
    "### 8. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNkwrGgEH0OA"
   },
   "outputs": [],
   "source": [
    "#Lets split the features and labels\n",
    "X= re_df.text\n",
    "y = re_df.emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3191,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3191,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5nlCuaaH0OD"
   },
   "source": [
    "## 9. **Predicting the sentiment:**\n",
    "\n",
    "\n",
    "### Use Naive Bayes and Logistic Regression and print their accuracy scores for predicting the sentiment of the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AbVYssaH0OE"
   },
   "outputs": [],
   "source": [
    "#Import model libs\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets first split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2552,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(639,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  5610\n"
     ]
    }
   ],
   "source": [
    "#Use the cvect to transform the text\n",
    "X_train_dtm = cvect.transform(X_train)\n",
    "X_test_dtm = cvect.transform(X_test)\n",
    "\n",
    "print('Features: ', X_train_dtm.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktXrLhmOH0Of"
   },
   "outputs": [],
   "source": [
    "#initialize a MNB model and get the accuracy\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clv2X0kKH0Ok"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model\n",
    "mnb.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K86LRMfdH0Ou"
   },
   "outputs": [],
   "source": [
    "#predict\n",
    "y_pred = mnb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 0.8497652582159625\n",
      "Confusion Matrix\n",
      " [[ 45  68]\n",
      " [ 28 498]]\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.40      0.48       113\n",
      "           1       0.88      0.95      0.91       526\n",
      "\n",
      "    accuracy                           0.85       639\n",
      "   macro avg       0.75      0.67      0.70       639\n",
      "weighted avg       0.83      0.85      0.84       639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the accuracy and confusion matrix now\n",
    "print(\"Accuracy Score\",accuracy_score(y_test,y_pred))\n",
    "print(\"Confusion Matrix\\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"Classification Metrics\\n\",classification_report(y_test,y_pred))\n",
    "\n",
    "#As probably expected the recall of 0 is very low\n",
    "#The null model is only slightly less accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try the Logistic Regression model\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model\n",
    "lr.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "y_pred = lr.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 0.8841940532081377\n",
      "Confusion Matrix\n",
      " [[ 48  65]\n",
      " [  9 517]]\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.42      0.56       113\n",
      "           1       0.89      0.98      0.93       526\n",
      "\n",
      "    accuracy                           0.88       639\n",
      "   macro avg       0.87      0.70      0.75       639\n",
      "weighted avg       0.88      0.88      0.87       639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the accuracy and confusion matrix now\n",
    "print(\"Accuracy Score\",accuracy_score(y_test,y_pred))\n",
    "print(\"Confusion Matrix\\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"Classification Metrics\\n\",classification_report(y_test,y_pred))\n",
    "\n",
    "#As probably expected the recall of 0 is very low\n",
    "#Results are better than MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sw-0B33tH0Ox"
   },
   "source": [
    "## 10. Create a function called `tokenize_test` which can take count vectorizer object as input, create document term matrix out of x_train & x_test, build and train a model using dtm created and print the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okCTOs1TH0Oy"
   },
   "outputs": [],
   "source": [
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print('Features: ', X_train_dtm.shape[1])\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    print(\"Multinomial NB\")\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred_class))\n",
    "    print(\"Classification Metrics\\n\",classification_report(y_test, y_pred_class))\n",
    "    print(\"Logistic Regression\")\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = lr.predict(X_test_dtm)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred_class))\n",
    "    print(\"Classification Metrics\\n\",classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxZ8jfPEH0O0"
   },
   "source": [
    "### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_test function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdCyAN_IH0O0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  25481\n",
      "Multinomial NB\n",
      "Accuracy:  0.86697965571205\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.27      0.42       113\n",
      "           1       0.86      0.99      0.92       526\n",
      "\n",
      "    accuracy                           0.87       639\n",
      "   macro avg       0.89      0.63      0.67       639\n",
      "weighted avg       0.87      0.87      0.84       639\n",
      "\n",
      "Logistic Regression\n",
      "Accuracy:  0.8982785602503912\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.46      0.62       113\n",
      "           1       0.90      0.99      0.94       526\n",
      "\n",
      "    accuracy                           0.90       639\n",
      "   macro avg       0.91      0.73      0.78       639\n",
      "weighted avg       0.90      0.90      0.88       639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "tokenize_test(cv)\n",
    "\n",
    "#Features increased naturally significantly\n",
    "\n",
    "#Accuracy actualy increased but recall for negative is drastically reduced for MNB\n",
    "#LR is again better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axepytmgH0O4"
   },
   "source": [
    "### Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_test function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HToGkq7vH0O4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  4775\n",
      "Multinomial NB\n",
      "Accuracy:  0.8575899843505478\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.27      0.40       113\n",
      "           1       0.86      0.98      0.92       526\n",
      "\n",
      "    accuracy                           0.86       639\n",
      "   macro avg       0.83      0.63      0.66       639\n",
      "weighted avg       0.85      0.86      0.83       639\n",
      "\n",
      "Logistic Regression\n",
      "Accuracy:  0.8732394366197183\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.35      0.50       113\n",
      "           1       0.88      0.98      0.93       526\n",
      "\n",
      "    accuracy                           0.87       639\n",
      "   macro avg       0.85      0.67      0.71       639\n",
      "weighted avg       0.87      0.87      0.85       639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "tokenize_test(cv)\n",
    "\n",
    "#Feature size reduced\n",
    "#Accuracy actualy increased slightly from the base model but recall is again very bad for MNB\n",
    "#For LR the accuracy decreased and recall is also very bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOIlJRxoH0O7"
   },
   "source": [
    "### Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_test function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fUhff-oH0O8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  300\n",
      "Multinomial NB\n",
      "Accuracy:  0.8247261345852895\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.35      0.41       113\n",
      "           1       0.87      0.93      0.90       526\n",
      "\n",
      "    accuracy                           0.82       639\n",
      "   macro avg       0.69      0.64      0.65       639\n",
      "weighted avg       0.80      0.82      0.81       639\n",
      "\n",
      "Logistic Regression\n",
      "Accuracy:  0.8482003129890454\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.24      0.36       113\n",
      "           1       0.86      0.98      0.91       526\n",
      "\n",
      "    accuracy                           0.85       639\n",
      "   macro avg       0.78      0.61      0.64       639\n",
      "weighted avg       0.83      0.85      0.82       639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words=\"english\",max_features=300)\n",
    "\n",
    "tokenize_test(cv)\n",
    "\n",
    "#Accuracy actualy decreased but recall seems to be slightly better for MNB\n",
    "#For LR accuracy reduced and recall also reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2KZNWVkH0PA"
   },
   "source": [
    "### Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_test function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3v9XD082H0PB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  15000\n",
      "Multinomial NB\n",
      "Accuracy:  0.8685446009389671\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.32      0.46       113\n",
      "           1       0.87      0.99      0.93       526\n",
      "\n",
      "    accuracy                           0.87       639\n",
      "   macro avg       0.85      0.65      0.69       639\n",
      "weighted avg       0.86      0.87      0.84       639\n",
      "\n",
      "Logistic Regression\n",
      "Accuracy:  0.8951486697965572\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.47      0.61       113\n",
      "           1       0.90      0.99      0.94       526\n",
      "\n",
      "    accuracy                           0.90       639\n",
      "   macro avg       0.89      0.73      0.78       639\n",
      "weighted avg       0.89      0.90      0.88       639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2),max_features=15000)\n",
    "\n",
    "tokenize_test(cv)\n",
    "\n",
    "#Accuracy actualy increased slightly and recall seems to be slightly better\n",
    "#For LR model also accuracy and recall increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "We3JK_SRH0PO"
   },
   "source": [
    "### Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_test function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUHrfDCyH0PP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  8370\n",
      "Multinomial NB\n",
      "Accuracy:  0.8701095461658842\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.40      0.52       113\n",
      "           1       0.88      0.97      0.92       526\n",
      "\n",
      "    accuracy                           0.87       639\n",
      "   macro avg       0.82      0.68      0.72       639\n",
      "weighted avg       0.86      0.87      0.85       639\n",
      "\n",
      "Logistic Regression\n",
      "Accuracy:  0.8967136150234741\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.49      0.62       113\n",
      "           1       0.90      0.98      0.94       526\n",
      "\n",
      "    accuracy                           0.90       639\n",
      "   macro avg       0.89      0.74      0.78       639\n",
      "weighted avg       0.89      0.90      0.88       639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2),min_df=2)\n",
    "\n",
    "tokenize_test(cv)\n",
    "\n",
    "#Accuracy actualy increased slightly and recall seems to be slightly better\n",
    "#Even for LR model accuracy and recall is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All parameters used along ngram of (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  5000\n",
      "Multinomial NB\n",
      "Accuracy:  0.8403755868544601\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.35      0.43       113\n",
      "           1       0.87      0.95      0.91       526\n",
      "\n",
      "    accuracy                           0.84       639\n",
      "   macro avg       0.73      0.65      0.67       639\n",
      "weighted avg       0.82      0.84      0.82       639\n",
      "\n",
      "Logistic Regression\n",
      "Accuracy:  0.8779342723004695\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.39      0.53       113\n",
      "           1       0.88      0.98      0.93       526\n",
      "\n",
      "    accuracy                           0.88       639\n",
      "   macro avg       0.86      0.69      0.73       639\n",
      "weighted avg       0.87      0.88      0.86       639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2),min_df=2,max_features=5000,stop_words=\"english\",max_df=0.8)\n",
    "\n",
    "tokenize_test(cv)\n",
    "\n",
    "#Accuracy actualy increased slightly and recall seems to be slightly better\n",
    "#Even for LR model accuracy and recall is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VltP3aeMvrW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try with TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  5013\n",
      "Multinomial NB\n",
      "Accuracy:  0.8278560250391236\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.03      0.05       113\n",
      "           1       0.83      1.00      0.91       526\n",
      "\n",
      "    accuracy                           0.83       639\n",
      "   macro avg       0.91      0.51      0.48       639\n",
      "weighted avg       0.86      0.83      0.75       639\n",
      "\n",
      "Logistic Regression\n",
      "Accuracy:  0.8419405320813772\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.11      0.19       113\n",
      "           1       0.84      1.00      0.91       526\n",
      "\n",
      "    accuracy                           0.84       639\n",
      "   macro avg       0.92      0.55      0.55       639\n",
      "weighted avg       0.87      0.84      0.79       639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tvect = TfidfVectorizer()\n",
    "\n",
    "tokenize_test(tvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  3000\n",
      "Multinomial NB\n",
      "Accuracy:  0.8450704225352113\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.13      0.23       113\n",
      "           1       0.84      1.00      0.91       526\n",
      "\n",
      "    accuracy                           0.85       639\n",
      "   macro avg       0.89      0.57      0.57       639\n",
      "weighted avg       0.86      0.85      0.79       639\n",
      "\n",
      "Logistic Regression\n",
      "Accuracy:  0.837245696400626\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.15       113\n",
      "           1       0.83      1.00      0.91       526\n",
      "\n",
      "    accuracy                           0.84       639\n",
      "   macro avg       0.92      0.54      0.53       639\n",
      "weighted avg       0.86      0.84      0.78       639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tvect = TfidfVectorizer(ngram_range=(1,2),min_df=2,stop_words=\"english\",max_features=3000)\n",
    "\n",
    "tokenize_test(tvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TF-IDF model does not help in this case...the recall is terrible and the model accuracy is also not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try a deep learning model with CountVectorizer and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add hidden layers\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu', input_shape=(len(cvect.vocabulary_),)))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "#Add Output layer\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "Train on 2552 samples, validate on 639 samples\n",
      "Epoch 1/10\n",
      "2552/2552 [==============================] - 1s 333us/sample - loss: 0.4553 - acc: 0.8393 - val_loss: 0.3993 - val_acc: 0.8232\n",
      "Epoch 2/10\n",
      "2552/2552 [==============================] - 1s 246us/sample - loss: 0.3143 - acc: 0.8405 - val_loss: 0.3534 - val_acc: 0.8451\n",
      "Epoch 3/10\n",
      "2552/2552 [==============================] - 1s 245us/sample - loss: 0.2187 - acc: 0.8981 - val_loss: 0.3285 - val_acc: 0.8701\n",
      "Epoch 4/10\n",
      "2552/2552 [==============================] - 1s 233us/sample - loss: 0.1229 - acc: 0.9600 - val_loss: 0.3781 - val_acc: 0.8717\n",
      "Epoch 5/10\n",
      "2552/2552 [==============================] - 1s 229us/sample - loss: 0.0693 - acc: 0.9777 - val_loss: 0.4417 - val_acc: 0.8732\n",
      "Epoch 6/10\n",
      "2552/2552 [==============================] - 1s 247us/sample - loss: 0.0437 - acc: 0.9882 - val_loss: 0.4845 - val_acc: 0.8701\n",
      "Epoch 7/10\n",
      "2552/2552 [==============================] - 1s 240us/sample - loss: 0.0320 - acc: 0.9929 - val_loss: 0.5300 - val_acc: 0.8685\n",
      "Epoch 8/10\n",
      "2552/2552 [==============================] - 1s 231us/sample - loss: 0.0277 - acc: 0.9937 - val_loss: 0.5316 - val_acc: 0.8670\n",
      "Epoch 9/10\n",
      "2552/2552 [==============================] - 1s 253us/sample - loss: 0.0201 - acc: 0.9937 - val_loss: 0.5667 - val_acc: 0.8717\n",
      "Epoch 10/10\n",
      "2552/2552 [==============================] - 1s 228us/sample - loss: 0.0245 - acc: 0.9937 - val_loss: 0.5863 - val_acc: 0.8685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c5e2f3eda0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_dtm, y_train,\n",
    "           validation_data=(X_test_dtm, y_test), \n",
    "           epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = model.predict_classes(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 0.8685446009389671\n",
      "Confusion Matrix\n",
      " [[ 56  57]\n",
      " [ 27 499]]\n",
      "Classification Metrics\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.50      0.57       113\n",
      "           1       0.90      0.95      0.92       526\n",
      "\n",
      "    accuracy                           0.87       639\n",
      "   macro avg       0.79      0.72      0.75       639\n",
      "weighted avg       0.86      0.87      0.86       639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the accuracy and confusion matrix now\n",
    "print(\"Accuracy Score\",accuracy_score(y_test,y_pred_class))\n",
    "print(\"Confusion Matrix\\n\",confusion_matrix(y_test,y_pred_class))\n",
    "print(\"Classification Metrics\\n\",classification_report(y_test,y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sentiment_analysis_twitter_data_questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
